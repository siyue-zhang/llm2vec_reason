{"TheoremQA_jianyu_xu/pigeonhole_2.json": {"gold": {"18695": 1}, "retrieved": {"20777": {"score": 0.861350417137146, "content": {"text": "\\section{Round Peg fits in Square Hole better than Square Peg fits in Round Hole}\nTags: Circles, Squares\n\n\\begin{theorem}\nA round peg fits better in a square hole than a square peg fits in a round hole.\n:600px\n\\end{theorem}\n\n\\begin{proof}\nThe situation is modelled by considering the ratios of the areas of:\n:a square to the circle in which it is inscribed\n:a square to the circle around which it has been circumscribed.\nLet a square $S$ be inscribed in a circle $C$ of radius $r$.\nLet $A_c$ and $A_s$ be the areas of $C$ and $S$ respectively.\nFrom Area of Circle:\n:$A_c = \\pi r^2$\nThe diameter of $S$ is $2 r$.\nThus from Pythagoras's Theorem its side is of length $r \\sqrt 2$.\nFrom Area of Square:\n:$A_s = 2 r^2$\nThus:\n:$\\dfrac {A_s} {A_c} = \\dfrac {2 r^2} {\\pi r^2} = \\dfrac 2 \\pi \\approx 0.6366 \\ldots$\n{{qed|lemma}}\nLet a square $S$ be circumscribed around a circle $C$ of radius $r$.\nLet $A_c$ and $A_s$ be the areas of $C$ and $S$ respectively.\nFrom Area of Circle:\n:$A_c = \\pi r^2$\nThe side of $S$ is of length $2 r$.\nFrom Area of Square:\n:$A_s = 4 r^2$\nThus:\n:$\\dfrac {A_c} {A_s} = \\dfrac {\\pi r^2} {4 r^2} = \\dfrac \\pi 4 \\approx 0.7853 \\ldots$\n{{qed|lemma}}\nThus a round peg takes up more space ($0.7853 \\ldots$) of a square hole than a square peg takes up ($0.6366 \\ldots$) of a round hole.\n{{qed}}\n\\end{proof}\n\n"}}, "14575": {"score": 0.8629157543182373, "content": {"text": "\\section{Ellipse is Bounded in Plane}\nTags: Ellipses\n\n\\begin{theorem}\nLet $E$ be an ellipse embedded in in a Euclidean plane.\nThen $E$ is bounded.\n\\end{theorem}\n\n\\begin{proof}\nLet a Cartesian coordinate system be applied to the Euclidean plane in which $E$ is embedded.\nLet $E$ be expressed in reduced form:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {x^2} {a^2} + \\dfrac {y^2} {b^2}\n      | r = 1\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\dfrac x a\n      | r = \\sqrt {1 - \\dfrac {y^2} {b^2} }\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\dfrac y b\n      | r = \\sqrt {1 - \\dfrac {x^2} {a^2} }\n      | c = \n}}\n{{end-eqn}}\nHence:\n:there are no real values of $x$ for $\\size y > b$\n:there are no real values of $y$ for $\\size x > b$.\nHence $E$ exists entirely within the rectangle whose sides are $x = \\pm a$ and $y = \\pm b$.\nThe result follows.\n{{Qed}}\n\\end{proof}\n\n"}}, "12290": {"score": 0.8705885410308838, "content": {"text": "\\section{Inscribed Squares in Right-Angled Triangle}\nTags: Squares, Inscribed Squares in Right-Angled Triangle, Right Triangles\n\n\\begin{theorem}\nFor any right-angled triangle, two squares can be inscribed inside it.\nOne square would share a vertex with the right-angled vertex of the right-angled triangle:\n:200px\nThe other square would have a side lying on the hypotenuse of the right-angled triangle:\n::400px\n\\end{theorem}\n\n\\begin{proof}\nBy definition of inscribed polygon, all four vertices of the inscribed square lies on the sides of the right-angled triangle.\nBy Pigeonhole Principle, at least two of the vertices must lie on the same side of the right-angled triangle.\nThe case where this side is the hypotenuse would be the second case above.\nFor the case where this side is not the hypotenuse, if none of the vertices coincide with the right angle of the right-angled triangle, the angle formed by the three vertices not on the hypotenuse would be obtuse, which is not possible since a square contains no obtuse angles.\nTherefore a vertex must coincide with the right angle of the right-angled triangle, which is the first case above.\n{{qed}}\nCategory:Inscribed Squares in Right-Angled Triangle\nCategory:Squares\nCategory:Right Triangles\n\\end{proof}\n\n"}}, "4263": {"score": 0.8698002099990845, "content": {"text": "\\begin{definition}[Definition:Incircle]\nLet $P$ be a polygon.\nLet $C$ be a circle which is inscribed within $P$.\n:300px\nThen $C$ is an '''incircle''' of $P$.\n\\end{definition}"}}, "16058": {"score": 0.864215612411499, "content": {"text": "\\section{Concentric Circles do not Intersect}\nTags: Circles, Geometry\n\n\\begin{theorem}\nIf two circles are concentric, they share no points on their circumferences.\nAlternatively, this can be worded:\n:If two concentric circles share one point on their circumferences, then they share them all (that is, they are the same circle).\n\\end{theorem}\n\n\\begin{proof}\nThis follows directly from:\n: Intersecting Circles have Different Centers\n: Touching Circles have Different Centers.\n{{qed}}\nCategory:Circles\n\\end{proof}\n\n"}}, "4829": {"score": 0.8878581523895264, "content": {"text": "\\begin{definition}[Definition:Kakeya's Constant]\n'''Kakeya's constant''' is defined as the area of the smallest simple convex domain in which one can put a line segment of length $1$ which will coincide with itself when rotated $180 \\degrees$:\n:$K = \\dfrac {\\paren {5 - 2 \\sqrt 2} \\pi} {24} \\approx 0 \\cdotp 28425 \\, 82246 \\ldots$\n{{OEIS|A093823}}\n{{expand|Needs considerable work done here by someone who understands exactly what is going on here. The case of the equilateral triangle is well known; so is the case of the Perron tree; I also remember a piece by Martin Gardner on the subject which demonstrates that a star-shaped area derived from the deltoid can be made arbitrarily small; and so on. Exactly what is meant here by simple convex domain needs rigorous clarification.}}\n\\end{definition}"}}, "1563": {"score": 0.8757821321487427, "content": {"text": "\\begin{definition}[Definition:Concyclic Points]\nA set of $4$ or more points $S = \\set {P_1, P_2, \\ldots, P_n}$ is '''concyclic''' if they all lie on the circumference of a circle.\n:320px\nIn the above diagram, points $A, B, C, D$ are '''concyclic''', as they all lie on the circumference of the circle centered at $O$.\n\\end{definition}"}}, "11646": {"score": 0.9193465113639832, "content": {"text": "\\section{Jung's Theorem in the Plane}\nTags: Euclidean Space\n\n\\begin{theorem}\nLet $S \\subseteq \\R^2$ be a compact region in a Euclidean plane.\nLet $d$ be the diameter of $S$.\nThen there exists a circle $C$ with radius $r$ such that:\n:$r = d \\dfrac {\\sqrt 3} 3$\nsuch that $S \\subseteq C$.\nThe parameter $\\dfrac {\\sqrt 3} 3$ can also be presented as $\\dfrac 1 {\\sqrt 3}$, and evaluates approximately as:\n:$\\dfrac {\\sqrt 3} 3 \\approx 0 \\cdotp 57735 \\, 02691 \\ldots$\n{{OEIS|A020760}}\n\\end{theorem}\n\n\\begin{proof}\nThis is an instance of Jung's Theorem, setting $n = 2$.\n{{qed}}\n{{Namedfor|Heinrich Wilhelm Ewald Jung}}\n\\end{proof}\n\n"}}, "4443": {"score": 0.8838359117507935, "content": {"text": "\\begin{definition}[Definition:Inscribe]\nLet a geometric figure $A$ be constructed in the interior of another geometric figure $B$ such that:\n:$(1): \\quad$ $A$ and $B$ have points in common\n:$(2): \\quad$ No part of $A$ is outside $B$.\nThen $A$ is '''inscribed''' inside $B$.\n\\end{definition}"}}, "21288": {"score": 0.8854474425315857, "content": {"text": "\\section{Seven Touching Cylinders}\nTags: Recreational Mathematics, Cylinders\n\n\\begin{theorem}\nIt is possible to arrange $7$ identical cylinders so that each one touches each of the others.\nThe cylinders must be such that their heights must be at least $\\dfrac {7 \\sqrt 3} 2$ of the diameters of their bases.\n\\end{theorem}\n\n\\begin{proof}\n:600px\nIt remains to be proved that the heights of the cylinders must be at least $\\dfrac {7 \\sqrt 3} 2$ of the diameters of their bases.\n{{ProofWanted|Prove the above}}\n\\end{proof}\n\n"}}}}, "TheoremQA_xueguangma/taylors_approximation_theorem.json": {"gold": {"9280": 1, "9282": 1}, "retrieved": {"2285": {"score": 0.8976714611053467, "content": {"text": "\\begin{definition}[Definition:Derivative/Real Function/Derivative at Point/Definition 1]\nLet $I$ be an open real interval.\nLet $f: I \\to \\R$ be a real function defined on $I$.\nLet $\\xi \\in I$ be a point in $I$.\nLet $f$ be differentiable at the point $\\xi$.\nThat is, suppose the limit $\\ds \\lim_{x \\mathop \\to \\xi} \\frac {\\map f x - \\map f \\xi} {x - \\xi}$ exists.\nThen this limit is called the '''derivative of $f$ at the point $\\xi$'''.\n\\end{definition}"}}, "18870": {"score": 0.8996484875679016, "content": {"text": "\\section{Power Rule for Derivatives}\nTags: Calculus, Cal, Power Rule for Derivatives, Differential Calculus, Derivatives\n\n\\begin{theorem}\nLet $n \\in \\R$.\nLet $f: \\R \\to \\R$ be the real function defined as $\\map f x = x^n$.\nThen:\n:$\\map {f'} x = n x^{n - 1}$\neverywhere that $\\map f x = x^n$ is defined.\nWhen $x = 0$ and $n = 0$, $\\map {f'} x$ is undefined.\n\\end{theorem}\n\n\\begin{proof}\nThis can be done in sections.\n\\end{proof}\n\n"}}, "2399": {"score": 0.9069120287895203, "content": {"text": "\\begin{definition}[Definition:Differentiation]\nThe process of obtaining the derivative of a differentiable function $f$ {{WRT|Differentiation}} $x$ is known as:\n: '''differentiation (of $f$) with respect to $x$'''\nor\n: '''differentiation WRT $x$'''\n\\end{definition}"}}, "2286": {"score": 0.9031826853752136, "content": {"text": "\\begin{definition}[Definition:Derivative/Real Function/Derivative at Point/Definition 2]\nLet $I$ be an open real interval.\nLet $f: I \\to \\R$ be a real function defined on $I$.\nLet $\\xi \\in I$ be a point in $I$.\nLet $f$ be differentiable at the point $\\xi$.\nThat is, suppose the limit $\\ds \\lim_{h \\mathop \\to 0} \\frac {\\map f {\\xi + h} - \\map f \\xi} h$ exists.\nThen this limit is called the '''derivative of $f$ at the point $\\xi$'''.\n\\end{definition}"}}, "2273": {"score": 0.9007826447486877, "content": {"text": "\\begin{definition}[Definition:Derivative]\nInformally, a '''derivative''' is the rate of change of one variable with respect to another.\n\\end{definition}"}}, "15118": {"score": 0.9251996278762817, "content": {"text": "\\section{Derivative of Logarithm at One}\nTags: Differential Calculus, Derivatives, Logarithms, Derivative of Logarithm at One\n\n\\begin{theorem}\nLet $\\ln x$ be the natural logarithm of $x$ for real $x$ where $x > 0$.\nThen:\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\map \\ln {1 + x} } x = 1$\n\\end{theorem}\n\n\\begin{proof}\nL'H\u00f4pital's rule gives:\n:$\\displaystyle \\lim_{x \\to c} \\frac {f \\left({x}\\right)} {g \\left({x}\\right)} = \\lim_{x \\to c} \\frac {f^{\\prime} \\left({x}\\right)} {g^{\\prime} \\left({x}\\right)}$\n(provided the appropriate conditions are fulfilled).\nHere we have:\n* $\\ln \\left({1 + 0}\\right) = 0$\n* $D_x \\left({\\ln \\left({1 + x}\\right)}\\right) = \\dfrac 1 {1 + x}$ from the Chain Rule\n* $D_x x = 1$ from Differentiation of the Identity Function.\nThus:\n:$\\displaystyle \\lim_{x \\to 0} \\frac {\\ln \\left({1 + x}\\right)} {x} = \\lim_{x \\to 0} \\frac {\\left({1 + x}\\right)^{-1}} {1} = \\frac 1 {1 + 0} = 1$\n{{qed}}\n\\end{proof}\n\n"}}, "11318": {"score": 0.9086434245109558, "content": {"text": "\\section{Limit of (Cosine (X) - 1) over X at Zero}\nTags: Cosine Function, Limits of Real Functions, Limit of (Cosine (X) - 1) over X, Limits of Functions, Analysis, Differential Calculus, Examples of Limits of Real Functions, Limit of (Cosine (X) - 1) over X at Zero\n\n\\begin{theorem}\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\cos x - 1} x = 0$\n\\end{theorem}\n\n\\begin{proof}\nThis proof works directly from the definition of the cosine function:\n{{begin-eqn}}\n{{eqn | l=\\cos x\n      | r=\\sum_{n=0}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n      | c=By the definition of the cosine function\n}}\n{{eqn | r=(-1)^0 \\cdot \\frac{x^{2\\cdot0} }{(2\\cdot0)!}+\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n}}\n{{eqn | r=1 + \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n      | c=From the definition of $0!$ and the definition of $a^0$\n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l=\\lim_{x \\to 0} \\ \\frac{\\cos (x) - 1} x\n      | r=\\lim_{x \\to 0} \\ \\frac{1 + \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!} - 1} x\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\frac{\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!} } x\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\frac{\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n-1} } {\\left({2n-1}\\right)!} } 1\n      | c=by Power Series Differentiable on Interval of Convergence and L'H\u00f4pital's Rule\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n-1} }{\\left({2n-1}\\right)!}\n}}\n{{eqn | r=\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {0^{2n-1} }{\\left({2n-1}\\right)!}\n      | c=by Polynomial is Continuous\n}}\n{{eqn | r=0\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "15227": {"score": 0.9362316727638245, "content": {"text": "\\section{Derivative of Exponential at Zero}\nTags: Differential Calculus, Exponential Function, Derivative of Exponential at Zero, Derivatives involving Exponential Function\n\n\\begin{theorem}\nLet $\\exp x$ be the exponential of $x$ for real $x$.\nThen:\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\exp x - 1} x = 1$\n\\end{theorem}\n\n\\begin{proof}\nFor all $x \\in \\R$, we have the following:\n* $\\exp 0 - 1 = 0$ from Exponential of Zero and One\n* $D_x \\left({\\exp x - 1}\\right) = \\exp x$ from Sum Rule for Derivatives\n* $D_x x = 1$ from Differentiation of the Identity Function.\nHaving verified its prerequisites, Corollary 1 to L'H\u00f4pital's rule yields immediately:\n:$\\displaystyle \\lim_{x \\to 0} \\frac {\\exp x - 1} {x} = \\lim_{x \\to 0} \\frac {\\exp x} {1} = \\exp 0 = 1$\n{{qed}}\n\\end{proof}\n\n"}}, "18873": {"score": 0.9107273817062378, "content": {"text": "\\section{Power Rule for Derivatives/Real Number Index}\nTags: Differential Calculus, Power Rule for Derivatives\n\n\\begin{theorem}\nLet $n \\in \\R$.\nLet $f: \\R \\to \\R$ be the real function defined as $\\map f x = x^n$.\nThen:\n:$\\map {f'} x = n x^{n-1}$\neverywhere that $\\map f x = x^n$ is defined.\nWhen $x = 0$ and $n = 0$, $\\map {f'} x$ is undefined.\n{{explain|Nowhere in either proof is it explained why $\\map {f'} x$ is undefined.}}\n\\end{theorem}\n\n\\begin{proof}\nWe are going to prove that $f^{\\prime}(x) = n x^{n-1}$ holds for all real $n$.\nTo do this, we compute the limit $\\displaystyle \\lim_{h \\to 0} \\frac{\\left({x + h}\\right)^n - x^n} h$:\n{{begin-eqn}}\n{{eqn | l=\\frac{\\left({x + h}\\right)^n - x^n} h\n      | r=\\frac{x^n} h \\left({\\left({1 + \\frac h x}\\right)^n - 1}\\right)\n      | c=\n}}\n{{eqn | r=\\frac{x^n} h \\left({e^{n \\ln \\left({1 + \\frac h x}\\right)} - 1}\\right)\n      | c=\n}}\n{{eqn | r=x^n \\cdot \\frac {e^{n \\ln \\left({1 + \\frac h x}\\right)} - 1} {n \\ln \\left({1 + \\frac h x}\\right)} \\cdot \\frac {n \\ln \\left({1 + \\frac h x}\\right)} {\\frac h x} \\cdot \\frac 1 x\n      | c=\n}}\n{{end-eqn}}\nNow we use the following results:\n* $\\displaystyle \\lim_{x \\to 0} \\frac {\\exp x - 1} x = 1$ from Derivative of Exponent at Zero\n* $\\displaystyle \\lim_{x \\to 0} \\frac {\\ln \\left({1 + x}\\right)} x = 1$ from Derivative of Logarithm at One\n... to obtain:\n:$\\displaystyle \\frac {e^{n \\ln \\left({1 + \\frac h x}\\right)} - 1} {n \\ln \\left( {1 + \\frac h x}\\right)} \\cdot \\frac {n \\ln \\left({1 + \\frac h x}\\right)} {\\frac h x} \\cdot \\frac 1 x \\to n x^{n-1}$ as $h \\to 0$\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "2288": {"score": 0.915078341960907, "content": {"text": "\\begin{definition}[Definition:Derivative/Real Function/With Respect To]\nLet $f$ be a real function which is differentiable on an open interval $I$.\nLet $f$ be defined as an equation: $y = \\map f x$.\nThen the '''derivative of $y$ with respect to $x$''' is defined as:\n:$\\ds y^\\prime = \\lim_{h \\mathop \\to 0} \\frac {\\map f {x + h} - \\map f x} h = D_x \\, \\map f x$\nThis is frequently abbreviated as '''derivative of $y$''' '''WRT''' or '''w.r.t.''' '''$x$''', and often pronounced something like '''wurt'''.\nWe introduce the quantity $\\delta y = \\map f {x + \\delta x} - \\map f x$.\nThis is often referred to as '''the small change in $y$ consequent on the small change in $x$'''.\nHence the motivation behind the popular and commonly-seen notation:\n:$\\ds \\dfrac {\\d y} {\\d x} := \\lim_{\\delta x \\mathop \\to 0} \\dfrac {\\map f {x + \\delta x} - \\map f x} {\\delta x} = \\lim_{\\delta x \\mathop \\to 0} \\dfrac {\\delta y} {\\delta x}$\nHence the notation $\\map {f^\\prime} x = \\dfrac {\\d y} {\\d x}$.\nThis notation is useful and powerful, and emphasizes the concept of a derivative as being the limit of a ratio of very small changes.\nHowever, it has the disadvantage that the variable $x$ is used ambiguously: both as the point at which the derivative is calculated and as the variable with respect to which the derivation is done.\nFor practical applications, however, this is not usually a problem.\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/series_convergen2.json": {"gold": {"15648": 1, "8388": 1, "16324": 1, "11276": 1}, "retrieved": {"16367": {"score": 0.8288198709487915, "content": {"text": "\\section{Combination Theorem for Sequences/Real/Sum Rule}\nTags: Named Theorems, Limits of Sequences, Combination Theorems for Sequences\n\n\\begin{theorem}\nLet $\\sequence {x_n}$ and $\\sequence {y_n}$ be sequences in $\\R$.\nLet $\\sequence {x_n}$ and $\\sequence {y_n}$ be convergent to the following limits:\n:$\\ds \\lim_{n \\mathop \\to \\infty} x_n = l$\n:$\\ds \\lim_{n \\mathop \\to \\infty} y_n = m$\nThen:\n:$\\ds \\lim_{n \\mathop \\to \\infty} \\paren {x_n + y_n} = l + m$\n\\end{theorem}\n\n\\begin{proof}\nLet $\\epsilon > 0$ be given.\nThen $\\dfrac \\epsilon 2 > 0$.\nWe are given that:\n:$\\ds \\lim_{n \\mathop \\to \\infty} x_n = l$\n:$\\ds \\lim_{n \\mathop \\to \\infty} y_n = m$\nBy definition of the limit of a real sequence, we can find $N_1$ such that:\n:$\\forall n > N_1: \\size {x_n - l} < \\dfrac \\epsilon 2$\nwhere $\\size {x_n - l}$ denotes the absolute value of $x_n - l$\nSimilarly we can find $N_2$ such that:\n:$\\forall n > N_2: \\size {y_n - m} < \\dfrac \\epsilon 2$\nLet $N = \\max \\set {N_1, N_2}$.\nThen if $n > N$, both the above inequalities will be true:\n:$n > N_1$\n:$n > N_2$\nThus $\\forall n > N$:\n{{begin-eqn}}\n{{eqn | l = \\size {\\paren {x_n + y_n} - \\paren {l + m} }\n      | r = \\size {\\paren {x_n - l} + \\paren {y_n - m} }\n      | c = \n}}\n{{eqn | o = \\le\n      | r = \\size {x_n - l} + \\size {y_n - m}\n      | c = Triangle Inequality for Real Numbers\n}}\n{{eqn | o = <\n      | r = \\frac \\epsilon 2 + \\frac \\epsilon 2\n      | c = \n}}\n{{eqn | r = \\epsilon\n      | c = \n}}\n{{end-eqn}}\nHence the result:\n:$\\ds \\lim_{n \\mathop \\to \\infty} \\paren {x_n + y_n} = l + m$\n{{qed}}\n\\end{proof}\n\n"}}, "21010": {"score": 0.8289102911949158, "content": {"text": "\\section{Sequence of Powers of Number less than One/Sufficient Condition}\nTags: Power of Number less than One, Sequence of Powers of Number less than One, Limits of Sequences\n\n\\begin{theorem}\nLet $x \\in \\R$.\nLet $\\sequence {x_n}$ be the sequence in $\\R$ defined as $x_n = x^n$.\nLet $\\sequence {x_n}$ be a null sequence.\nThen $\\size x < 1$.\n\\end{theorem}\n\n\\begin{proof}\nBy Reciprocal of Null Sequence:\n:$\\sequence {x_n}$ converges to $0$ {{iff}} $\\sequence {\\dfrac 1 {x_n} }$ diverges to $\\infty$.\nBy the definition of divergence to $\\infty$:\n:$\\exists N \\in \\N: \\forall n \\ge N: \\size {\\dfrac 1 {x_n} } > 1$\nIn particular:\n:$\\size {\\dfrac 1 {x_N} } > 1$\nBy Ordering of Reciprocals:\n:$\\size {x_N} < 1$\nThat is:\n:$\\size {x_N} = \\size {x^N} = \\size x^N < 1$\n{{AimForCont}} $\\size x \\ge 1$.\nBy Inequality of Product of Unequal Numbers:\n:$\\size x^N \\ge 1^N = 1$\nThis is a contradiction.\nSo $\\size x < 1$ as required.\n{{qed}}\nCategory:Limits of Sequences\nCategory:Sequence of Powers of Number less than One\n\\end{proof}\n\n"}}, "11212": {"score": 0.8372161388397217, "content": {"text": "\\section{Limit of Root of Positive Real Number}\nTags: Limit of Root of Positive Real Number, Limits of Sequences\n\n\\begin{theorem}\nLet $x \\in \\R: x > 0$ be a real number.\nLet $\\sequence {x_n}$ be the sequence in $\\R$ defined as:\n:$x_n = x^{1 / n}$\nThen $x_n \\to 1$ as $n \\to \\infty$.\n\\end{theorem}\n\n\\begin{proof}\nLet us define $1 = a_1 = a_2 = \\cdots = a_{n-1}$ and $a_n = x$.\nLet $G_n$ be the geometric mean of $a_1, \\ldots, a_n$.\nLet $A_n$ be the arithmetic mean of $a_1, \\ldots, a_n$.\nFrom their definitions, $G_n = x^{1/n}$ and $A_n = \\dfrac {n - 1 + x} n = 1 + \\dfrac{x - 1} n$.\nFrom Arithmetic Mean Never Less than Geometric Mean, $x^{1/n} \\le 1 + \\dfrac{x - 1} n$.\nThat is, $x^{1/n} - 1 \\le \\dfrac{x - 1} n$.\nThere are two cases to consider: $x \\ge 1$ and $0 < x < 1$.\n* Let $x \\ge 1$.\nFrom Root of Number Greater than One, it follows that $x^{1/n} \\ge 1$.\nThus $0 \\le x^{1/n} - 1 \\le \\dfrac 1 n \\left({x - 1}\\right)$.\nBut from Power of Reciprocal, $\\dfrac 1 n \\to 0$ as $n \\to \\infty$.\nFrom the Combination Theorem for Sequences it follows that $\\dfrac 1 n \\left({x - 1}\\right) \\to 0$ as $n \\to \\infty$.\nThus by the Squeeze Theorem, $x^{1/n} - 1 \\to 0$ as $n \\to \\infty$.\nHence $x^{1/n} \\to 1$ as $n \\to \\infty$, again from the Combination Theorem for Sequences.\n* Now let $0 < x < 1$.\nThen $x = \\dfrac 1 y$ where $y > 1$.\nBut from the above, $y^{1/n} \\to 1$ as $n \\to \\infty$.\nHence by the Combination Theorem for Sequences, $x^{1/n} = \\dfrac 1 {y^{1/n}} \\to \\dfrac 1 1 = 1$ as $n \\to \\infty$.\n\\end{proof}\n\n"}}, "21009": {"score": 0.8354451656341553, "content": {"text": "\\section{Sequence of Powers of Number less than One/Rational Numbers}\nTags: Power of Number less than One, Sequence of Powers of Number less than One, Limits of Sequences\n\n\\begin{theorem}\nLet $x \\in \\Q$.\nLet $\\sequence {x_n}$ be the sequence in $\\Q$ defined as $x_n = x^n$.\nThen:\n:$\\size x < 1$ {{iff}} $\\sequence {x_n}$ is a null sequence.\n\\end{theorem}\n\n\\begin{proof}\nBy the definition of convergence of a rational sequence:\n:$\\sequence {x_n}$ is a null sequence in the rational numbers {{iff}} $\\sequence {x_n}$ is a null sequence in the real numbers\nBy Sequence of Powers of Real Number less than One:\n:$\\sequence {x_n}$ is a null sequence in the real numbers {{iff}} $\\size x < 1$\n{{qed}}\nCategory:Limits of Sequences\nCategory:Sequence of Powers of Number less than One\n\\end{proof}\n\n"}}, "22204": {"score": 0.8296098709106445, "content": {"text": "\\section{Sum of Geometric Sequence/Examples/Index to Minus 2}\nTags: Sum of Geometric Sequence, Sum of Geometric Progression\n\n\\begin{theorem}\nLet $x$ be an element of one of the standard number fields: $\\Q, \\R, \\C$ such that $x \\ne 1$.\nThen the formula for Sum of Geometric Sequence:\n:$\\ds \\sum_{j \\mathop = 0}^n x^j = \\frac {x^{n + 1} - 1} {x - 1}$\nbreaks down when $n = -2$:\n:$\\ds \\sum_{j \\mathop = 0}^{-2} x^j \\ne \\frac {x^{-1} - 1} {x - 1}$\n\\end{theorem}\n\n\\begin{proof}\nThe summation on the {{LHS}} is vacuous:\n:$\\ds \\sum_{j \\mathop = 0}^{-2} x^j = 0$\nwhile on the {{RHS}} we have:\n{{begin-eqn}}\n{{eqn | l = \\frac {x^{\\paren {-2} + 1} - 1} {x - 1}\n      | r = \\frac {x^{-1} - 1} {x - 1}\n      | c = \n}}\n{{eqn | r = \\frac {1 / x - 1} {x - 1}\n      | c = \n}}\n{{eqn | r = \\frac {\\paren {1 - x} / x} {x - 1}\n      | c = \n}}\n{{eqn | r = \\frac {1 - x} {x \\paren {x - 1} }\n      | c = \n}}\n{{eqn | r = -\\frac 1 x\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "1906": {"score": 0.8479239344596863, "content": {"text": "\\begin{definition}[Definition:Convergent Series/Number Field]\nLet $S$ be one of the standard number fields $\\Q, \\R, \\C$.\nLet $\\ds \\sum_{n \\mathop = 1}^\\infty a_n$ be a series in $S$.\nLet $\\sequence {s_N}$ be the sequence of partial sums of $\\ds \\sum_{n \\mathop = 1}^\\infty a_n$.\nIt follows that $\\sequence {s_N}$ can be treated as a sequence in the metric space $S$.\nIf $s_N \\to s$ as $N \\to \\infty$, the series '''converges to the sum $s$''', and one writes $\\ds \\sum_{n \\mathop = 1}^\\infty a_n = s$.\nA series is said to be '''convergent''' {{iff}} it converges to some $s$.\n\\end{definition}"}}, "8391": {"score": 0.8409414291381836, "content": {"text": "\\begin{definition}[Definition:Series/Number Field]\nLet $S$ be one of the standard number fields $\\R$, or $\\C$.\nLet $\\sequence {a_n}$ be a sequence in $S$.\nThe '''series''' is what results when $\\sequence {a_n}$ is summed to infinity:\n:$\\ds \\sum_{n \\mathop = 1}^\\infty a_n = a_1 + a_2 + a_3 + \\cdots$\n\\end{definition}"}}, "18053": {"score": 0.8550289869308472, "content": {"text": "\\section{One Plus Reciprocal to the Nth}\nTags: Reciprocal, Limits of Sequences, Reciprocals, Analysis\n\n\\begin{theorem}\nLet $\\sequence {x_n}$ be the sequence in $\\R$ defined as $x_n = \\paren {1 + \\dfrac 1 n}^n$.\nThen $\\sequence {x_n}$ converges to a limit as $n$ increases without bound.\n\\end{theorem}\n\n\\begin{proof}\nFirst we show that $\\sequence {x_n}$ is increasing.\nLet $a_1 = a_2 = \\cdots = a_{n - 1} = 1 + \\dfrac 1 {n - 1}$.\nLet $a_n = 1$.\nLet:\n:$A_n$ be the arithmetic mean of $a_1 \\ldots a_n$\n:$G_n$ be the geometric mean of $a_1 \\ldots a_n$\nThus:\n:$A_n = \\dfrac {\\paren {n - 1} \\paren {1 + \\dfrac 1 {n - 1} } + 1} n = \\dfrac {n + 1} n = 1 + \\dfrac 1 n$\n:$G_n = \\paren {1 + \\dfrac 1 {n - 1} }^{\\dfrac {n - 1} n}$\nBy Cauchy's Mean Theorem:\n: $G_n \\le A_n$\nThus:\n:$\\paren {1 + \\dfrac 1 {n - 1} }^{\\frac {n - 1} n} \\le 1 + \\dfrac 1 n$\nand so:\n:$x_{n - 1} = \\paren {1 + \\dfrac 1 {n - 1} }^{n - 1} \\le \\paren {1 + \\dfrac 1 n}^n = x_n$\nHence $\\sequence {x_n}$ is increasing.\nNext, we show that $\\sequence {x_n}$ is bounded above.\nUsing the Binomial Theorem:\n{{begin-eqn}}\n{{eqn | l = \\paren {1 + \\frac 1 n}^n\n      | r = 1 + n \\paren {\\frac 1 n} + \\frac {n \\paren {n - 1} } 2 \\paren {\\frac 1 n}^2 + \\cdots + \\paren  {\\frac 1 n}^n\n}}\n{{eqn | r = 1 + 1 + \\paren {1 - \\frac 1 n} \\frac 1 {2!} + \\paren {1 - \\frac 1 n} \\paren {1 - \\frac 2 n} \\frac 1 {3!} + \\cdots + \\paren {1 - \\frac 1 n} \\paren {1 - \\frac 2 n} \\cdots \\paren {1 - \\frac {n - 1} n} \\frac 1 {n!}\n}}\n{{eqn | o = \\le\n      | r = 1 + 1 + \\frac 1 {2!} + \\frac 1 {3!} + \\cdots + \\frac 1 {n!}\n}}\n{{eqn | o = \\le\n      | r = 1 + 1 + \\frac 1 2 + \\frac 1 {2^2} + \\cdots + \\frac 1 {2^n}\n      | c = (because $2^{n - 1} \\le n!$)\n}}\n{{eqn | r = 1 + \\frac {1 - \\paren {\\frac 1 2}^n} {1 - \\frac 1 2}\n}}\n{{eqn | r = 1 + 2 \\paren {1 - \\paren {\\frac 1 2}^n}\n}}\n{{eqn | o = <\n      | r = 3\n}}\n{{end-eqn}}\nSo $\\sequence {x_n}$ is bounded above by $3$.\nFrom the Monotone Convergence Theorem (Real Analysis), it follows that $\\sequence {x_n}$ converges to a limit.\n\\end{proof}\n\n"}}, "15094": {"score": 0.8414304256439209, "content": {"text": "\\section{Difference Between Adjacent Square Roots Converges}\nTags: Limits of Sequences\n\n\\begin{theorem}\nLet $\\sequence {x_n}$ be the sequence in $\\R$ defined as $x_n = \\sqrt {n + 1} - \\sqrt n$.\nThen $\\sequence {x_n}$ converges to a zero limit.\n\\end{theorem}\n\n\\begin{proof}\nWe have:\n{{begin-eqn}}\n{{eqn | l = 0\n      | o = \\le\n      | r = \\sqrt {n + 1} - \\sqrt n\n      | c = \n}}\n{{eqn | r = \\frac {\\paren {\\sqrt {n + 1} - \\sqrt n} \\paren {\\sqrt {n + 1} + \\sqrt n} } {\\sqrt {n + 1} + \\sqrt n}\n      | c = multiplying top and bottom by $\\sqrt {n + 1} - \\sqrt n$\n}}\n{{eqn | r = \\frac {n + 1 - n} {\\sqrt {n + 1} + \\sqrt n}\n      | c = Difference of Two Squares\n}}\n{{eqn | r = \\frac 1 {\\sqrt {n + 1} + \\sqrt n}\n      | c = \n}}\n{{eqn | o = <\n      | r = \\frac 1 {\\sqrt n}\n      | c = as $\\sqrt {n + 1} + \\sqrt n > \\sqrt n$\n}}\n{{end-eqn}}\nBut from Sequence of Powers of Reciprocals is Null Sequence, $\\dfrac 1 {\\sqrt n} \\to 0$ as $n \\to \\infty$.\nThe result follows by the Squeeze Theorem.\n{{Qed}}\n\\end{proof}\n\n"}}, "11331": {"score": 0.8438010811805725, "content": {"text": "\\section{Limit of Integer to Reciprocal Power}\nTags: Limits of Sequences, Reciprocals, Reciprocal, Limit of Integer to Reciprocal Power, Limit of Number to Reciprocal Power\n\n\\begin{theorem}\nLet $\\sequence {x_n}$ be the real sequence defined as $x_n = n^{1/n}$, using exponentiation.\nThen $\\sequence {x_n}$ converges with a limit of $1$.\n\\end{theorem}\n\n\\begin{proof}\nFirst we show that $\\left \\langle {n^{1/n}} \\right \\rangle$ is decreasing for $n \\ge 3$.\nWe want to show that $\\left({n + 1}\\right)^{1/\\left({n + 1}\\right)} \\le n^{1/n}$.\nThis is the same as saying that $\\left({n + 1}\\right)^n \\le n^{n + 1}$.\n{{begin-eqn}}\n{{eqn | l=\\left({n + 1}\\right)^{1/\\left({n + 1}\\right)}\n      | o=\\le\n      | r=n^{1/n}\n      | c=\n}}\n{{eqn | ll=\\iff\n      | l=\\left({n + 1}\\right)^n\n      | o=\\le\n      | r=n^{n + 1}\n      | c=raising both sides to the power of $n \\left({n+1}\\right)$\n}}\n{{eqn | ll=\\iff\n      | l=\\left({n \\left({1 + \\frac 1 n}\\right)}\\right)^n\n      | o=\\le\n      | r=n^{n + 1}\n      | c=\n}}\n{{eqn | ll=\\iff\n      | l=\\left({1 + \\frac 1 n}\\right)^n\n      | o=\\le\n      | r=\\frac {n^{n + 1} } {n^n} = n\n      | c=\n}}\n{{end-eqn}}\nBut from One Plus Reciprocal to the Nth, $\\left({1 + \\dfrac 1 n}\\right)^n < 3$.\nThus the reversible chain of implication can be invoked and we see that $\\left({n + 1}\\right)^{1/\\left({n + 1}\\right)} \\le n^{1/n}$ when $n \\ge 3$.\nSo $\\left \\langle {n^{1/n}} \\right \\rangle$ is decreasing for $n \\ge 3$.\nNow, as $n^{1/n} > 0$ for all positive $n$, it follows that $\\left \\langle {n^{1/n}} \\right \\rangle$ is bounded below (by $0$, for a start).\nThus the subsequence of $\\left \\langle {n^{1/n}} \\right \\rangle$ consisting of all the elements of $\\left \\langle {n^{1/n}} \\right \\rangle$ where $n \\ge 3$ is convergent by the Monotone Convergence Theorem.\n{{qed|lemma}}\nNow we need to demonstrate that this limit is in fact $1$.\nLet $n^{1/n} \\to l$ as $n \\to \\infty$.\nHaving established this, we can investigate the subsequence $\\left \\langle {\\left({2n}\\right)^{\\frac 1 {2n}}} \\right \\rangle$.\nBy Limit of a Subsequence, this will converge to $l$ also.\nFrom Limit of Root of Positive Real Number, we have that $2^{\\frac 1 {2n}} \\to 1$ as $n \\to \\infty$.\nSo $n^{\\frac 1 {2n}} \\to l$ as $n \\to \\infty$ by the Combination Theorem for Sequences.\nThus $n^{1/n} = n^{\\frac 1 {2n}} \\cdot n^{\\frac 1 {2n}} \\to l \\cdot l = l^2$ as $n \\to \\infty$.\nSo $l^2 = l$, and as $l \\ge 1$ the result follows.\n{{qed}}\nAlternatively, we can use the definition of the power to a real number:\n:$\\displaystyle n^{1/n} = \\exp \\left({\\frac 1 n \\ln n}\\right)$.\nFrom Powers Drown Logarithms, we have that:\n:$\\displaystyle \\lim_{n \\to \\infty} \\frac 1 n \\ln n = 0$\nHence:\n:$\\displaystyle \\lim_{n \\to \\infty} n^{1/n} = \\exp 0 = 1$\nand the result.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_maxku/basic-electronics-2-1.json": {"gold": {"15397": 1}, "retrieved": {"15395": {"score": 0.8046051263809204, "content": {"text": "\\section{Current in Electric Circuit/L, R in Series/Condition for Ohm's Law}\nTags: Electronics\n\n\\begin{theorem}\nConsider the electrical circuit $K$ consisting of:\n: a resistance $R$\n: an inductance $L$\nin series with a source of electromotive force $E$ which is a function of time $t$.\n:File:CircuitRLseries.png\nOhm's Law is satisfied by $K$ whenever the current $I$ is at a maximum or a minimum.\n\\end{theorem}\n\n\\begin{proof}\nFrom Electric Current in Electric Circuit: L, R in Series:\n:$L \\dfrac {\\d I} {\\d t} + R I = E$\ndefines the behaviour of $I$.\nLet $I$ be at a maximum or a minimum.\nThen from Derivative at Maximum or Minimum:\n:$\\dfrac {\\d I} {\\d t} = 0$\nand so:\n{{begin-eqn}}\n{{eqn | l = E\n      | r = 0 + R I\n      | c = \n}}\n{{eqn | r = R I\n      | c = \n}}\n{{end-eqn}}\nwhich is Ohm's Law.\n{{qed}}\n\\end{proof}\n\n"}}, "15397": {"score": 0.8133601546287537, "content": {"text": "\\section{Current in Electric Circuit/L, R in Series/Constant EMF at t = 0/Corollary 1}\nTags: Electronics\n\n\\begin{theorem}\nConsider the electrical circuit $K$ consisting of:\n:a resistance $R$\n:an inductance $L$\nin series with a source of electromotive force $E$ which is a function of time $t$.\n:File:CircuitRLseries.png\nLet the electric current flowing in $K$ at time $t = 0$ be $I_0$.\nLet a constant EMF $E_0$ be imposed upon $K$ at time $t = 0$.\nAfter a sufficiently long time, the electric current $I$ in $K$ is given by the equation:\n:$E_0  = R I$\n\\end{theorem}\n\n\\begin{proof}\nFrom Electric Current in Electric Circuit: L, R in Series: Constant EMF at $t = 0$:\n:$I = \\dfrac {E_0} R + \\paren {I_0 - \\dfrac {E_0} R} e^{-R t / L}$\nWe have that:\n:$\\ds \\lim_{t \\mathop \\to \\infty} e^{-R t / L} \\to 0$\nand so:\n:$\\ds \\lim_{t \\mathop \\to \\infty} I \\to \\dfrac {E_0} R$\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "15393": {"score": 0.8298922777175903, "content": {"text": "\\section{Current in Electric Circuit/L, R, C in Series}\nTags: Electronics\n\n\\begin{theorem}\nConsider the electrical circuit $K$ consisting of:\n: a resistance $R$\n: an inductance $L$\n: a capacitance $C$\nin series with a source of electromotive force $E$ which is a function of time $t$.\n:File:CircuitRLCseries.png\nThe electric current $I$ in $K$ is given by the linear second order ODE:\n:$L \\dfrac {\\d^2 I} {\\d t^2} + R \\dfrac {\\d I} {\\d t} + \\dfrac 1 C I = \\dfrac {\\d E} {\\d t}$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$E_L$ be the drop in electromotive force across $L$\n:$E_R$ be the drop in electromotive force across $R$\n:$E_C$ be the drop in electromotive force across $C$.\nFrom Kirchhoff's Voltage Law:\n:$E - E_L - E_R - E_C = 0$\nFrom Ohm's Law:\n:$E_R = R I$\nFrom Drop in EMF caused by Inductance is proportional to Rate of Change of Current:\n:$E_L = L \\dfrac {\\d I} {\\d t}$\nFrom Drop in EMF caused by Capacitance is proportional to Accumulated Charge:\n:$E_C = \\dfrac 1 C Q$\nwhere $Q$ is the electric charge $Q$ that has accumulated on $C$.\nThus:\n:$E - L \\dfrac {\\d I} {\\d t} - R I - \\dfrac 1 C Q = 0$\nwhich can be rewritten:\n:$L \\dfrac {\\d I} {\\d t} + R I + \\dfrac 1 C Q = E$\nDifferentiating {{WRT|Differentiation}} $t$ gives:\n:$L \\dfrac {\\d^2 I} {\\d t^2} + R \\dfrac {\\d I} {\\d t} + \\dfrac 1 C I = \\dfrac {\\d E} {\\d t}$\n{{qed}}\n\\end{proof}\n\n"}}, "14609": {"score": 0.8203907012939453, "content": {"text": "\\section{Electric Charge in Electric Circuit/L, R, C in Series}\nTags: Electronics\n\n\\begin{theorem}\nConsider the electrical circuit $K$ consisting of:\n: a resistance $R$\n: an inductance $L$\n: a capacitance $C$\nin series with a source of electromotive force $E$ which is a function of time $t$.\n:File:CircuitRLCseries.png\nThe electric charge $Q$ in $K$ is given by the linear second order ODE:\n:$L \\dfrac {\\d^2 Q} {\\d t^2} + R \\dfrac {\\d Q} {\\d t} + \\dfrac 1 C Q = E$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$E_L$ be the drop in electromotive force across $L$\n:$E_R$ be the drop in electromotive force across $R$\n:$E_C$ be the drop in electromotive force across $C$.\nFrom Kirchhoff's Voltage Law:\n:$E - E_L - E_R - E_C = 0$\nFrom Ohm's Law:\n:$E_R = R I$\nFrom Drop in EMF caused by Inductance is proportional to Rate of Change of Current:\n:$E_L = L \\dfrac {\\d I} {\\d t}$\nFrom Drop in EMF caused by Capacitance is proportional to Accumulated Charge:\n:$E_C = \\dfrac 1 C Q$\nwhere $Q$ is the electric charge $Q$ that has accumulated on $C$.\nThus:\n:$E - L \\dfrac {\\d I} {\\d t} - R I - \\dfrac 1 C Q = 0$\nwhich can be rewritten:\n:$L \\dfrac {\\d I} {\\d t} + R I + \\dfrac 1 C Q = E$\nBy definition of electric current:\n:$I = \\dfrac {\\d Q} {\\d t}$\nand so:\n:$L \\dfrac {\\d^2 Q} {\\d t^2} + R \\dfrac {\\d Q} {\\d t} + \\dfrac 1 C Q = E$\n{{qed}}\n\\end{proof}\n\n"}}, "15399": {"score": 0.8135876059532166, "content": {"text": "\\section{Current in Electric Circuit/L, R in Series/Constant EMF at t = 0/Corollary 3}\nTags: Electronics\n\n\\begin{theorem}\nConsider the electrical circuit $K$ consisting of:\n:a resistance $R$\n:an inductance $L$\nin series with a source of electromotive force $E$ which is a function of time $t$.\n:File:CircuitRLseries.png\nLet the electric current flowing in $K$ at time $t = 0$ be $I_0$.\nLet  EMF imposed upon $K$ be zero.\nThe electric current $I$ in $K$ is given by the equation:\n:$I = I_0 e^{-R t / L}$\n\\end{theorem}\n\n\\begin{proof}\nFrom Electric Current in Electric Circuit: L, R in Series: Constant EMF at $t = 0$:\n:$I = \\dfrac {E_0} R + \\paren {I_0 - \\dfrac {E_0} R} e^{-R t / L}$\nSetting $E_0 = 0$:\n{{begin-eqn}}\n{{eqn | l = I\n      | r = \\dfrac 0 R + \\paren {I_0 - \\dfrac 0 R} e^{-R t / L}\n      | c = \n}}\n{{eqn | r = I_0 e^{-R t / L}\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "7917": {"score": 0.8444713354110718, "content": {"text": "\\begin{definition}[Definition:Resistance/Unit]\nThe unit of measurement of '''resistance'''  is the ohm $\\Omega$:\n:$1 \\ \\Omega = 1 \\ \\mathrm{V} \\ \\mathrm{A}^{-1}$\nthat is, $1$ volt per ampere.\nCategory:Definitions/Units of Measurement\nCategory:Definitions/Resistance\n\\end{definition}"}}, "15394": {"score": 0.8314756155014038, "content": {"text": "\\section{Current in Electric Circuit/L, R in Series}\nTags: Decay Equation, Electronics\n\n\\begin{theorem}\nConsider the electrical circuit $K$ consisting of:\n:a resistance $R$\n:an inductance $L$\nin series with a source of electromotive force $E$ which is a function of time $t$.\n:File:CircuitRLseries.png\nThe electric current $I$ in $K$ is given by the linear first order ODE:\n:$L \\dfrac {\\d I} {\\d t} + R I = E$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$E_L$ be the drop in electromotive force across $L$\n:$E_R$ be the drop in electromotive force across $R$\nFrom Kirchhoff's Voltage Law:\n:$E - E_L - E_R = 0$\nFrom Ohm's Law:\n:$E_R = R I$\nFrom Drop in EMF caused by Inductance is proportional to Rate of Change of Current:\n:$E_L = L \\dfrac {\\d I} {\\d t}$\nThus:\n:$E - L \\dfrac {\\d I} {\\d t} - R I = 0$\nwhich can be rewritten:\n:$L \\dfrac {\\d I} {\\d t} + R I = E$\n{{qed}}\n\\end{proof}\n\n"}}, "7915": {"score": 0.8955760598182678, "content": {"text": "\\begin{definition}[Definition:Resistance]\n'''Resistance''' is a measure of how difficult it is to pass an electric current through a conductor.\nIt is defined as the electromotive force needed to push a unit current through that conductor.\nIt is conventionally denoted $R$.\n\\end{definition}"}}, "6318": {"score": 0.8318950533866882, "content": {"text": "\\begin{definition}[Definition:Ohm (Unit)]\nThe '''ohm''' is the SI unit of electrical resistance.\nIt is defined as being the electrical resistance between two points of an electrical conductor when a constant electric potential of $1$ volt, applied to these points, produces in the conductor an electric current of $1$ ampere, the  conductor not being the seat of any electromotive force.\n\\end{definition}"}}, "7916": {"score": 0.8444384932518005, "content": {"text": "\\begin{definition}[Definition:Resistance/Dimension]\nThe dimension of measurement of '''resistance''' is:\n:$\\mathsf {M L}^2 \\mathsf T^{\u22123} \\mathsf I^{\u22122}$\nThis derives from its definition as:\n:$R = \\dfrac {\\text {EMF} } {\\text {current} }$\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/Liouville\u2019s_theorem1.json": {"gold": {"11018": 1, "20530": 1, "14493": 1}, "retrieved": {"18773": {"score": 0.8906312584877014, "content": {"text": "\\section{Pole at Infinity implies Polynomial Function}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $f : \\C \\to \\C$ be an entire function.\nLet $f$ have a pole of order $N$ at $\\infty$. \nThen $f$ is a polynomial of degree $N$.\n\\end{theorem}\n\n\\begin{proof}\nBy Complex Function is Entire iff it has Everywhere Convergent Power Series, there exists a power series: \n:$\\ds \\map f z = \\sum_{n \\mathop = 0}^\\infty a_n z^n$ \nconvergent for all $z \\in \\C$, where $\\sequence {a_n}$ is a sequence of complex coefficients. \nThis gives: \n:$\\ds \\map f {\\frac 1 z} = \\sum_{n \\mathop = 0}^\\infty \\frac {a_n} {z^n}$ \nIt is given that $\\map f z$ has a pole of order $N$ at $\\infty$, so $\\map f {\\dfrac 1 z}$ has a pole of order $N$ at $0$. \nSo $N$ is the least positive integer such that:\n:$\\ds z^N \\map f {\\frac 1 z} = \\sum_{n \\mathop = 0}^\\infty a_n z^{N - n}$ \nis holomorphic at $0$, with $a_N \\ne 0$. \nTherefore, all exponents of $z$, with non-zero coefficients, in this series must be non-negative. \nSo $a_n = 0$ for $n > N$.  \nHence: \n:$\\ds \\map f z = \\sum_{n \\mathop = 0}^N a_n z^n$\nwith $a_N \\ne 0$. \nThat is, $f$ is a polynomial of degree $N$.  \n{{qed}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "20866": {"score": 0.8974580764770508, "content": {"text": "\\section{Schwarz's Lemma}\nTags: Schwarz's Lemma, Complex Analysis\n\n\\begin{theorem}\nLet $D$ be the unit disk centred at $0$.\nLet $f: D \\to \\C$ be a holomorphic function.\nLet $\\map f 0 = 0$ and $\\cmod {\\map f z} \\le 1$ for all $z \\in D$. \nThen $\\cmod {\\map {f'} 0} \\le 1$, and $\\cmod {\\map f z} \\le \\cmod z$ for all $z \\in D$.\n\\end{theorem}\n\n\\begin{proof}\nFirst a lemma:\n\\end{proof}\n\n"}}, "16082": {"score": 0.9051534533500671, "content": {"text": "\\section{Complex Sine Function is Entire}\nTags: Entire Functions, Sine Function, Complex Sine Function is Entire\n\n\\begin{theorem}\nLet $\\sin: \\C \\to \\C$ be the complex sine function. \nThen $\\sin$ is entire.\n\\end{theorem}\n\n\\begin{proof}\nBy the definition of the complex sine function, $\\sin$ admits a power series expansion about $0$: \n:$\\displaystyle \\sin z = \\sum_{n \\mathop = 0}^\\infty \\paren {-1}^n \\frac {z^{2 n + 1} } {\\paren {2 n + 1}!}$\nBy Complex Function is Entire iff it has Everywhere Convergent Power Series, to show that $\\sin$ is entire it suffices to show that this series is everywhere convergent.\nFrom Radius of Convergence from Limit of Sequence: Complex Case, it is sufficient to show that: \n:$\\displaystyle \\lim_{n \\mathop \\to \\infty} \\size {\\frac {\\paren {-1}^{n + 1} } {\\paren {2 n + 3}!} \\times \\frac {\\paren {2 n + 1}!} {\\paren {-1}^n} } = 0$\nWe have: \n{{begin-eqn}}\n{{eqn\t| l = \\lim_{n \\mathop \\to \\infty} \\size {\\frac {\\paren {-1}^{n + 1} } {\\paren {2 n + 3}!} \\times \\frac {\\paren {2 n + 1}!} {\\paren {-1}^n} }\n\t| r = \\size {-1} \\lim_{n \\mathop \\to \\infty} \\size {\\frac {\\paren {2 n + 1}!} {\\paren {2 n + 3} \\paren {2 n + 2} \\paren {2 n + 1}!} }\n\t| c = {{Defof|Factorial}}\n}}\n{{eqn\t| r = \\lim_{n \\mathop \\to \\infty} \\paren {\\frac 1 {\\paren {2 n + 3} \\paren {2 n + 2} } }\n}}\n{{eqn\t| r = 0\n}}\n{{end-eqn}}\nhence the result.\n{{qed}}\nCategory:Sine Function\n432468\n432463\n2019-10-27T15:44:02Z\nCaliburn\n3218\n432468\nwikitext\ntext/x-wiki\n\\end{proof}\n\n"}}, "18268": {"score": 0.9038004279136658, "content": {"text": "\\section{Order of Reciprocal of Entire Function}\nTags: Entire Functions\n\n\\begin{theorem}\nLet $f: \\C \\to \\C$ be an entire function of order $\\rho$.\nLet $f$ have no zeroes.\nThen $1/f$ has order $\\rho$.\n\\end{theorem}\n\n\\begin{proof}\nBy Zerofree Analytic Function on Simply Connected Set has Logarithm, there exists an entire function $g$ with $f = \\exp g$.\n{{ProofWanted}}\n\\end{proof}\n\n"}}, "18792": {"score": 0.8991997838020325, "content": {"text": "\\section{Polynomial has Order Zero}\nTags: Entire Functions\n\n\\begin{theorem}\nLet $P: \\C \\to \\C$ be a polynomial function.\nThen $P$ has order $0$.\n\\end{theorem}\n\n\\begin{proof}\nFollows from a slightly stronger result than Limit at Infinity of Polynomial over Complex Exponential.\n{{ProofWanted}}\n\\end{proof}\n\n"}}, "20530": {"score": 0.9385827779769897, "content": {"text": "\\section{Removable Singularity at Infinity implies Constant Function}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $f : \\C \\to \\C$ be an entire function.\nLet $f$ have an removable singularity at $\\infty$. \nThen $f$ is constant.\n\\end{theorem}\n\n\\begin{proof}\nWe are given that $f$ has a removable singularity at $\\infty$.\nBy Riemann Removable Singularities Theorem, $f$ must be bounded in a neighborhood of $\\infty$.\nThat is, there exists a real number $M > 0$ such that:\n:$\\forall z \\in \\set {z : \\cmod z > r}: \\cmod {\\map f z} \\le M$\nfor some real $r \\ge 0$.\nHowever, by Continuous Function on Compact Space is Bounded, $f$ is also bounded on $\\set {z: \\cmod z \\le r}$. \nAs $\\set {z: \\cmod z > r} \\cup \\set {z: \\cmod z \\le r} = \\C$, $f$ is therefore bounded on $\\C$.\nTherefore by Liouville's Theorem, $f$ is constant.\n{{qed}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "23471": {"score": 0.9062855243682861, "content": {"text": "\\section{Zero Derivative implies Constant Complex Function}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $D \\subseteq \\C$ be a connected domain of $\\C$.\nLet $f: D \\to \\C$ be a complex-differentiable function.\nFor all $z \\in D$, let $\\map {f'} z = 0$.\nThen $f$ is constant on $D$.\n\\end{theorem}\n\n\\begin{proof}\nLet $u, v: \\set {\\tuple {x, y} \\in \\R^2: x + i y = z \\in D} \\to \\R$ be the two real-valued functions defined in the Cauchy-Riemann Equations:\n:$\\map u {x, y} = \\map \\Re {\\map f z}$\n:$\\map v {x, y} = \\map \\Im {\\map f z}$\nBy the Cauchy-Riemann Equations:\n:$f' = \\dfrac {\\partial f} {\\partial x} = \\dfrac {\\partial u} {\\partial x} + i \\dfrac {\\partial v} {\\partial x}$\n:$f' = -i \\dfrac {\\partial f} {\\partial y} = \\dfrac {\\partial v} {\\partial y} - i \\dfrac {\\partial u} {\\partial y}$\nAs $f' = 0$ by assumption, this implies:\n:$0 = \\dfrac {\\partial u} {\\partial x} = \\dfrac {\\partial u} {\\partial y} = \\dfrac {\\partial v} {\\partial x} = \\dfrac {\\partial v} {\\partial y}$\nThen Zero Derivative implies Constant Function shows that $\\map u {x + t, y} = \\map u {x, y}$ for all $t \\in \\R$.\nSimilar results apply for the other three partial derivatives.\nLet $z, w \\in D$.\nFrom Connected Domain is Connected by Staircase Contours, it follows that there exists a staircase contour $C$ in $D$ with endpoints $z$ and $w$.\nThe contour $C$ is a concatenation of directed smooth curves that can be parameterized as line segments on one of these two forms:\n:$(1): \\quad \\map \\gamma t = z_0 + t r$\n:$(2): \\quad \\map \\gamma t = z_0 + i t r$\nfor some $z_0 \\in D$ and $r \\in \\R$ for all $t \\in \\closedint 0 1$.\nIf $z_1 \\in D$ lies on the same line segment as $z_0$, it follows that for parameterizations of type $(1)$:\n{{begin-eqn}}\n{{eqn | l = \\map f {z_1}\n      | r = \\map u {z_0 + t r} + \\map v {z_0 + t r}\n      | c = for some $t \\in \\closedint 0 1$\n}}\n{{eqn | r = \\map u {z_0} + \\map v {z_0}\n      | c = Zero Derivative implies Constant Function\n}}\n{{eqn | r = \\map f {z_0}\n}}\n{{end-eqn}}\nSimilarly, for parameterizations of type $(2)$:\n{{begin-eqn}}\n{{eqn | l = \\map f {z_1}\n      | r = \\map u {z_0 + i t r} + \\map v {z_0 + i t r}\n      | c = for some $t \\in \\closedint 0 1$\n}}\n{{eqn | r = \\map f {z_0}\n      | c = Zero Derivative implies Constant Function\n}}\n{{end-eqn}}\nAs the image of $C$ is connected by these line segments, it follows that for all $z_0$ and $z_1$ in the image of $C$:\n:$\\map f {z_0} = \\map f {z_1}$\nIn particular:\n:$\\map f z = \\map f w$\nso $f$ is constant on $D$.\n{{qed}}\n\\end{proof}\n\n"}}, "14493": {"score": 0.946445643901825, "content": {"text": "\\section{Entire Function with Bounded Real Part is Constant}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $f : \\C \\to \\C$ be an entire function. \nLet the real part of $f$ be bounded.\nThat is, there exists a positive real number $M$ such that: \n:$\\cmod {\\map \\Re {\\map f z} } < M$\nfor all $z \\in \\C$, where $\\map \\Re {\\map f z}$ denotes the real part of $\\map f z$.\nThen $f$ is constant.\n\\end{theorem}\n\n\\begin{proof}\nLet $g : \\C \\to \\C$ be a complex function with:\n:$\\ds \\map g z = e^{\\map f z}$\nBy Derivative of Complex Composite Function, $g$ is entire with derivative:\n:$\\ds \\map {g'} z = \\map {f'} z e^{\\map f z}$\nWe have:\n{{begin-eqn}}\n{{eqn\t| l = \\cmod {\\map g z}\n\t| r = e^{\\map \\Re {\\map f z} }\n\t| c = Modulus of Positive Real Number to Complex Power is Positive Real Number to Power of Real Part\n}}\n{{eqn\t| o = \\le\n\t| r = e^{\\cmod {\\map \\Re {\\map f z} } }\n\t| c = Exponential is Strictly Increasing\n}}\n{{eqn\t| o = <\n\t| r = e^M\n\t| c = Exponential is Strictly Increasing\n}}\n{{end-eqn}}\nSo $g$ is a bounded entire function.\nBy Liouville's Theorem, $g$ is therefore a constant function.\nWe therefore have, by Derivative of Constant: Complex:\n:$\\map {g'} z = 0$\nfor all $z \\in \\C$.\nThat is:\n:$\\map {f'} z e^{\\map f z} = 0$\nSince the exponential function is non-zero, we must have: \n:$\\map {f'} z = 0$\nfor all $z \\in \\C$.\nFrom Zero Derivative implies Constant Complex Function, we then have that $f$ is constant on $\\C$. \n{{qed}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "11018": {"score": 0.929892897605896, "content": {"text": "\\section{Liouville's Theorem (Complex Analysis)}\nTags: Complex Analysis, Named Theorems\n\n\\begin{theorem}\nLet $f: \\C \\to \\C$ be a bounded entire function.\nThen $f$ is constant.\n\\end{theorem}\n\n\\begin{proof}\nBy assumption, there is $M \\ge 0$ such that $\\cmod {\\map f z} \\le  M$ for all $z \\in \\C$. \nFor any $R \\in \\R: R > 0$, consider the function:\n:$\\map {f_R} z := \\map f {R z}$\nUsing the Cauchy Integral Formula, we see that:\n:$\\ds \\cmod {\\map {f_R'} z} = \\frac 1 {2 \\pi} \\cmod {\\int_{\\map {C_1} z} \\frac {\\map f w} {\\paren {w - z}^2} \\rd w} \\le \\frac 1 {2 \\pi} \\int_{\\map {C_1} z} M \\rd w = M$\nwhere $\\map {C_1} z$ denotes the circle of radius $1$ around $z$.\nHence:\n:$\\ds \\cmod {\\map {f'} z} = \\cmod {\\map {f_R'} z} / R \\le M / R$\nSince $R$ was arbitrary, it follows that $\\cmod {\\map {f'} z} = 0$ for all $z \\in \\C$.\nThus $f$ is constant.\n{{qed}}\n{{MissingLinks}}\n\\end{proof}\n\n"}}, "16083": {"score": 0.9342049956321716, "content": {"text": "\\section{Complex Sine Function is Unbounded}\nTags: Sine Function\n\n\\begin{theorem}\nLet $\\sin: \\C \\to \\C$ be the complex sine function. \nThen $\\sin$ is unbounded.\n\\end{theorem}\n\n\\begin{proof}\nBy Complex Sine Function is Entire, we have that $\\sin$ is an entire function. \n{{AimForCont}} that $\\sin$ was a bounded function.\nThen, by Liouville's Theorem, we would have that $\\sin$ is a constant function.\nHowever we have, for instance, by Sine of Zero is Zero: \n:$\\sin 0 = 0$\nand by Sine of 90 Degrees: \n:$\\sin \\dfrac \\pi 2 = 1$\nTherefore, $\\sin$ is clearly not a constant function, a contradiction.\nWe hence conclude, by Proof by Contradiction, that $\\sin$ is unbounded.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Binomial_3.json": {"gold": {"17198": 1, "17247": 1}, "retrieved": {"6877": {"score": 0.8263272047042847, "content": {"text": "\\begin{definition}[Definition:Permutation/Ordered Selection/Notation]\nThe number of $r$-permutations from a set of cardinality $n$ is denoted variously:\n:$P_{n r}$\n:${}^r P_n$\n:${}_r P_n$\n:${}_n P_r$ (extra confusingly)\nThere is little consistency in the literature).\nOn {{ProofWiki}} the notation of choice is ${}^r P_n$.\nCategory:Definitions/Permutation Theory\n\\end{definition}"}}, "5855": {"score": 0.8363975286483765, "content": {"text": "\\begin{definition}[Definition:Multinomial Coefficient]\nLet $k_1, k_2, \\ldots, k_m \\in \\Z_{\\ge 0}$ be positive integers.\nThe '''multinomial coefficient''' of $k_1, \\ldots, k_m$ is defined as:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} := \\dfrac {\\left({k_1 + k_2 + \\cdots + k_m}\\right)!} {k_1! \\, k_2! \\, \\ldots k_m!}$\n\\end{definition}"}}, "1332": {"score": 0.8550449013710022, "content": {"text": "\\begin{definition}[Definition:Combination]\nLet $S$ be a set containing $n$ elements.\nAn '''$r$-combination of $S$''' is a subset of $S$ which has $r$ elements.\n\\end{definition}"}}, "17916": {"score": 0.8423920273780823, "content": {"text": "\\section{Number of Elements in Partition}\nTags: Combinatorics\n\n\\begin{theorem}\nLet $S$ be a set.\nLet there be a partition on $S$ of $n$ subsets, each of which has $m$ elements.\nThen:\n:$\\card S = n m$\n\\end{theorem}\n\n\\begin{proof}\nLet the partition of $S$ be $S_1, S_2, \\ldots, S_n$.\nThen:\n:$\\forall k \\in \\set {1, 2, \\ldots, n}: \\card {S_k} = m$\nBy definition of multiplication:\n:$\\ds \\sum_{k \\mathop = 1}^n \\card {S_k} = n m$\nand the result follows from the Fundamental Principle of Counting.\n{{qed}}\n\\end{proof}\n\n"}}, "1337": {"score": 0.8376042246818542, "content": {"text": "\\begin{definition}[Definition:Combinatorics]\n'''Combinatorics''' is that branch of mathematics concerned with counting things.\n'''Combinatorial''' problems are so named because they are exercises in counting the number of combinations of various objects.\nIt has been stated that it is the core of the discipline of discrete mathematics.\n\\end{definition}"}}, "17192": {"score": 0.8683507442474365, "content": {"text": "\\section{Binomial Coefficient/Examples/Number of Bridge Hands}\nTags: Binomial Coefficients, Examples of Binomial Coefficients\n\n\\begin{theorem}\nThe total number $N$ of possible different hands for a game of [https://en.wikipedia.org/wiki/Contract_bridge bridge] is:\n:$N = \\dfrac {52!} {13! \\, 39!} = 635 \\ 013 \\ 559 \\ 600$\n\\end{theorem}\n\n\\begin{proof}\nThe total number of cards in a standard deck is $52$.\nThe number of cards in a single bridge hand is $13$.\nThus $N$ is equal to the number of ways $13$ things can be chosen from $52$.\nThus:\n{{begin-eqn}}\n{{eqn | l = N\n      | r = \\dbinom {52} {23}\n      | c = Cardinality of Set of Subsets\n}}\n{{eqn | r = \\frac {52!} {13! \\left({52 - 13}\\right)!}\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\frac {52!} {13! \\, 39!}\n      | c = \n}}\n{{eqn | r = 635 \\ 013 \\ 559 \\ 600\n      | c = after calculation\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "10462": {"score": 0.8620588779449463, "content": {"text": "\\section{Multinomial Coefficient expressed as Product of Binomial Coefficients}\nTags: Multinomial Coefficients, Binomial Coefficients\n\n\\begin{theorem}\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} = \\dbinom {k_1 + k_2} {k_1} \\dbinom {k_1 + k_2 + k_3} {k_1 + k_2} \\cdots \\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1 + k_2 + \\cdots + k_{m - 1} }$\nwhere:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m}$ denotes a multinomial coefficient\n:$\\dbinom {k_1 + k_2} {k_1}$ etc. denotes binomial coefficients.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction.\nFor all $m \\in \\Z_{> 1}$, let $\\map P m$ be the proposition:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} = \\dbinom {k_1 + k_2} {k_1} \\dbinom {k_1 + k_2 + k_3} {k_1 + k_2} \\cdots \\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1 + k_2 + \\cdots + k_{m - 1} }$\n\\end{proof}\n\n"}}, "19708": {"score": 0.8819866180419922, "content": {"text": "\\section{Product Rule for Counting}\nTags: Product Rule for Counting, Counting Arguments, Combinatorics, combinatorics\n\n\\begin{theorem}\nLet it be possible to choose an element $\\alpha$ from a given set $S$ in $m$ different ways.\nLet it be possible to choose an element $\\beta$ from a given set $T$ in $n$ different ways.\nThen the ordered pair $\\tuple {\\alpha, \\beta}$ can be chosen from the cartesian product $S \\times T$ in $m n$ different ways.\n\\end{theorem}\n\n\\begin{proof}\n{{handwaving}}\nThe validity of this rule follows directly from the definition of multiplication of integers.\nThe product $a b$ (for $a, b \\in \\N_{>0}$) is the number of sequences $\\sequence {A, B}$, where $A$ can be any one of $a$ items and $B$ can be any one of $b$ items.\n{{qed}}\n\\end{proof}\n\n"}}, "16939": {"score": 0.8662135004997253, "content": {"text": "\\section{Cardinality of Set of Combinations with Repetition}\nTags: Combinations with Repetition\n\n\\begin{theorem}\nLet $S$ be a finite set with $n$ elements\nThe number of $k$-combinations of $S$ with repetition is given by:\n:$N = \\dbinom {n + k - 1} k$\n\\end{theorem}\n\n\\begin{proof}\nLet the elements of $S$ be (totally) ordered in some way, by assigning an index to each element.\nThus let $S = \\left\\{ {a_1, a_2, a_3, \\ldots, a_n}\\right\\}$.\nThus each $k$-combination of $S$ with repetition can be expressed as:\n:$\\left\\{ {\\left\\{ {a_{r_1}, a_{r_1}, \\ldots, a_{r_k} }\\right\\} }\\right\\}$\nwhere:\n:$r_1, r_2, \\ldots, r_k$ are all elements of $\\left\\{ {1, 2, \\ldots, n}\\right\\}$\nand such that:\n:$r_1 \\le r_2 \\le \\cdots \\le r_k$\nHence the problem reduces to the number of integer solutions $\\left({r_1, r_2, \\ldots, r_k}\\right)$ such that $1 \\le r_1 \\le r_2 \\le \\cdots \\le r_k \\le n$.\nThis is the same as the number of solutions to:\n:$0 < r_1 < r_2 + 1 < \\cdots < r_k + k - 1 < n + k$\nwhich is the same as the number of solutions to:\n:$0 < s_1 < s_2 < \\cdots < s_k < n + k$\nwhich is the number of ways to choose $k$ elements from $n + k - 1$.\nThis is the same as the number of subsets as a set with $n + k - 1$ elements.\nFrom Cardinality of Set of Subsets:\n:$N = \\dbinom {n + k - 1} k$\n{{qed}}\n\\end{proof}\n\n"}}, "1333": {"score": 0.8675341606140137, "content": {"text": "\\begin{definition}[Definition:Combination with Repetition]\nLet $S$ be a (finite) set with $n$ elements.\nA '''$k$ combination of $S$ with repetition''' is a multiset with $k$ elements selected from $S$.\n\\end{definition}"}}}}, "TheoremQA_mingyin/Lebesgue-measure1.json": {"gold": {"20176": 1, "11426": 1, "16984": 1, "4971": 1}, "retrieved": {"12415": {"score": 0.8702417612075806, "content": {"text": "\\section{Induced Outer Measure Restricted to Semiring is Pre-Measure}\nTags: Measure Theory\n\n\\begin{theorem}\nLet $\\SS$ be a semiring over a set $X$.\nLet $\\mu: \\SS \\to \\overline \\R_{\\ge 0}$ be a pre-measure on $\\SS$, where $\\overline \\R_{\\ge 0}$ denotes the set of positive extended real numbers.\nLet $\\mu^*: \\powerset X \\to \\overline \\R_{\\ge 0}$ be the outer measure induced by $\\mu$.\nThen:\n:$\\ds \\mu^*\\restriction_\\SS \\, = \\mu$\nwhere $\\restriction$ denotes restriction.\n\\end{theorem}\n\n\\begin{proof}\nLet $S \\in \\SS$.\nIt follows immediately from the definition of the induced outer measure that $\\map {\\mu^*} S \\le \\map \\mu S$.\nTherefore, it suffices to show that if $\\ds \\sequence {A_n}_{n \\mathop = 0}^\\infty$ is a countable cover for $S$, then:\n:$\\ds \\map \\mu S \\le \\sum_{n \\mathop = 0}^\\infty \\map \\mu {A_n}$\nIf the above statement is true, then it follows directly from the definition of infimum that $\\map \\mu S \\le \\map {\\mu^*} S$, thus proving the theorem.\n{{refactor|The structure of this proof needs to be clarified. I recommend that the induction part of it be extracted into a separately-paged lemma, structured as our usual house style.|level = medium}}\nDefine, for all natural numbers $n \\in \\N$:\n:$\\ds B_n = A_n \\setminus A_{n - 1} \\setminus \\cdots \\setminus A_0$\nwhere $\\setminus$ denotes set difference.\nWe take $B_0 = A_0$.\nUsing the mathematical induction, we will prove that for all natural numbers $m < n$, $B_{n, m} = A_n \\setminus A_{n-1} \\setminus \\cdots \\setminus A_{n-m}$ is the finite union of pairwise disjoint elements of $\\SS$.\nWe take $B_{n, 0} = A_n$.\nThe base case $m = 0$ is trivial.\n{{Disambiguate|Definition:Trivial}}\nNow assume the induction hypothesis that the above statement is true for some natural number $m < n - 1$, and let $D_1, D_2, \\ldots, D_N$ be pairwise disjoint elements of $\\SS$ such that:\n:$\\ds B_{n, m} = \\bigcup_{k \\mathop = 1}^N D_k$\nThen:\n{{begin-eqn}}\n{{eqn | l = B_{n, m+1}\n      | r = B_{n, m} \\setminus A_{n - m - 1}\n}}\n{{eqn | r = \\paren {\\bigcup_{k \\mathop = 1}^N D_k} \\setminus A_{n - m - 1}\n}}\n{{eqn | r = \\bigcup_{k \\mathop = 1}^N \\paren {D_k \\setminus A_{n - m - 1} }\n      | c = by Set Difference is Right Distributive over Union\n}}\n{{end-eqn}}\nBy the definition of a semiring, for all natural numbers $k \\le N$, $D_k \\setminus A_{n - m - 1}$ is the finite union of pairwise disjoint elements of $\\SS$.\nHence $B_{n, m+1}$ is the finite union of pairwise disjoint elements of $\\SS$, completing the induction step.\nTherefore, $B_{n, n-1} = B_n$ is the finite union of pairwise disjoint elements of $\\SS$, as desired.\nUsing the above result and applying the axiom of countable choice, we can, for all $n \\in \\N$, choose a finite set $\\FF_n$ of pairwise disjoint elements of $\\SS$ for which:\n:$\\ds B_n = \\bigcup \\FF_n$\nNow, $x \\in S$ {{iff}} there exists an $n \\in \\N$ such that $x \\in S \\cap A_n$.\nTaking the smallest such $n$, which exists because $\\N$ is well-ordered, it follows that $x \\notin A_0, A_1, \\ldots, A_{n - 1}$, and so $x \\in S \\cap B_n$.\nTherefore:\n:$\\ds S = \\bigcup_{n \\mathop = 0}^\\infty \\paren {S \\cap B_n}$\nHence:\n{{begin-eqn}}\n{{eqn | l = \\map \\mu S\n      | r = \\map \\mu {\\bigcup_{n \\mathop = 0}^\\infty \\paren {S \\cap B_n} }\n}}\n{{eqn | r = \\map \\mu {\\bigcup_{n \\mathop = 0}^\\infty \\paren {S \\cap \\bigcup_{T \\mathop \\in \\FF_n} T} }\n      | c = by definition of $\\FF_n$\n}}\n{{eqn | r = \\map \\mu {\\bigcup_{n \\mathop = 0}^\\infty \\bigcup_{T \\mathop \\in \\FF_n} \\paren {S \\cap T} }\n      | c = Intersection Distributes over Union\n}}\n{{eqn | o = \\le\n      | r = \\sum_{n \\mathop = 0}^\\infty \\sum_{T \\mathop \\in \\FF_n} \\map \\mu {S \\cap T}\n      | c = countable union of finite sets is countable and {{Defof|Countably Subadditive Function}}\n}}\n{{eqn | o = \\le\n      | r = \\sum_{n \\mathop = 0}^\\infty \\sum_{T \\mathop \\in \\FF_n} \\map \\mu T\n      | c = {{Defof|Monotone (Measure Theory)|Monotone}}\n}}\n{{eqn | r = \\sum_{n \\mathop = 0}^\\infty \\map \\mu {\\bigcup \\FF_n}\n      | c = {{Defof|Additive Function (Measure Theory)|Additive Function}}\n}}\n{{eqn | r = \\sum_{n \\mathop = 0}^\\infty \\map \\mu {B_n}\n      | c = by definition of $\\FF_n$\n}}\n{{eqn | o = \\le\n      | r = \\sum_{n \\mathop = 0}^\\infty \\map \\mu {A_n}\n      | c = Set Difference is Subset and {{Defof|Monotone (Measure Theory)|Monotone}}\n}}\n{{end-eqn}}\n{{qed}}\n{{ACC}}\nCategory:Measure Theory\n\\end{proof}\n\n"}}, "15459": {"score": 0.870807409286499, "content": {"text": "\\section{Countable Set is Null Set under Lebesgue Measure}\nTags: Lebesgue Measure\n\n\\begin{theorem}\nLet $S \\subseteq \\R$ be a countable set.\nThen $\\map \\lambda S = 0$, where $\\lambda$ is Lebesgue measure.\nThat is, $S$ is a $\\lambda$-null set.\n\\end{theorem}\n\n\\begin{proof}\nBy Surjection from Natural Numbers iff Countable, there exists a surjection $f: \\N \\to S$.\nIt follows that:\n:$S = \\ds \\bigcup_{n \\mathop \\in \\N} \\set{\\map f n}$\nAs Lebesgue Measure is Diffuse, it holds that:\n:$\\forall n \\in \\N: \\map \\lambda {\\set{\\map f n}} = 0$\nThus, by Null Sets Closed under Countable Union, it follows that:\n:$\\map \\lambda S = 0$\n{{qed}}\n\\end{proof}\n\n"}}, "16986": {"score": 0.876187264919281, "content": {"text": "\\section{Cantor Set is Uncountable}\nTags: Uncountable Sets, Cantor Set is Uncountable, Cantor Set, Cantor Space\n\n\\begin{theorem}\nThe Cantor set $\\CC$ is uncountable.\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition as a ternary representation, $\\mathcal C$ consists of all the elements of $\\left[{0 \\,.\\,.\\, 1}\\right]$ which can be written without using a $1$.\nSo let $x \\in \\mathcal C$. Then in base $3$ notation, we have (as $0 \\le x \\le 1$):\n:$\\displaystyle x = \\sum_{i=1}^\\infty r_j 3^{-j}$\nFrom the definition of the Cantor set, we have $\\forall j: r_j \\in \\{0,2\\}$.\nFurthermore, from Representation of Ternary Expansions, the $r_j$ are unique.\nNow define the following function:\n:$\\displaystyle f: \\mathcal C \\to \\left[{0 \\,.\\,.\\, 1}\\right],\\quad f \\left({\\sum_{i=1}^\\infty r_j 3^{-j}}\\right) = \\sum_{i=1}^\\infty \\frac {r_j} 2 2^{-j}$\nObserve that $\\forall j: \\dfrac {r_j} 2 \\in \\{0, 1\\}$.\nThat the right-hand expression is in fact an element of $\\left[{0 \\,.\\,.\\, 1}\\right]$ now follows from binary notation.\nFurthermore by Existence of Base-N Representation, any element $y$ of $\\left[{0 \\,.\\,.\\, 1}\\right]$ may be written in the following form (where $\\forall j: b_j \\in \\{0,1\\}$):\n:$\\displaystyle y = \\sum_{i=1}^\\infty b_j 2^{-j}$\nObviously, $y = f \\left({x}\\right)$, where $x \\in \\mathcal C$ is defined as follows:\n:$\\displaystyle x = \\sum_{i=1}^\\infty 2b_j 3^{-j}$\nIt follows that $f$ is surjective.\nFrom Closed Interval in Reals is Uncountable, the closed interval $\\left[{0 \\,.\\,.\\, 1}\\right]$ is uncountable.\nThe result follows.\n{{explain|it is obvious, but most likely there is some result that could be referred to}}\n{{qed}}\n\\end{proof}\n\n"}}, "2029": {"score": 0.8748587369918823, "content": {"text": "\\begin{definition}[Definition:Countably Additive Function]\nLet $\\Sigma$ be a $\\sigma$-algebra.\nLet $f: \\Sigma \\to \\overline \\R$ be a function, where $\\overline \\R$ denotes the set of extended real numbers.\nThen $f$ is defined as '''countably additive''' {{iff}}:\n:$\\ds \\map f {\\bigcup_{n \\mathop \\in \\N} E_n} = \\sum_{n \\mathop \\in \\N} \\map f {E_n}$\nwhere $\\sequence {E_n}$ is any sequence of pairwise disjoint elements of $\\Sigma$.\nThat is, for any countably infinite set of pairwise disjoint elements of $\\Sigma$, $f$ of their union equals the sum of $f$ of the individual elements.\n\\end{definition}"}}, "16984": {"score": 0.8728998899459839, "content": {"text": "\\section{Cantor Set has Zero Lebesgue Measure}\nTags: Cantor Set, Measure Theory, Lebesgue Measure, Cantor Space\n\n\\begin{theorem}\nLet $\\CC$ be the Cantor set.\nLet $\\lambda$ denote the Lebesgue measure on the Borel $\\sigma$-algebra $\\map \\BB \\R$ on $\\R$.\nThen $\\CC$ is $\\map \\BB \\R$-measurable, and $\\map \\lambda \\CC = 0$.\nThat is, $\\CC$ is a $\\lambda$-null set.\n\\end{theorem}\n\n\\begin{proof}\nConsider the definition of $\\CC$ as a limit of a decreasing sequence.\nIn the notation as introduced there, we see that each $S_n$ is a collection of disjoint closed intervals.\nFrom Closed Set Measurable in Borel Sigma-Algebra, these are measurable sets.\nFurthermore, each $S_n$ is finite.\nHence by Sigma-Algebra Closed under Union, it follows that $C_n := \\ds \\bigcup S_n$ is measurable as well.\nThen, as we have:\n:$\\CC = \\ds \\bigcap_{n \\mathop \\in \\N} C_n$\nit follows from Sigma-Algebra Closed under Countable Intersection that $\\CC$ is measurable.\nThe $C_n$ also form a decreasing sequence of sets with limit $\\CC$.\nThus, from Characterization of Measures: $(3')$, it follows that:\n:$\\map \\lambda \\CC = \\ds \\lim_{n \\mathop \\to \\infty} \\map \\lambda {C_n}$\nIt is not too hard to show that, for all $n \\in \\N$:\n:$\\map \\lambda {C_n} = \\paren {\\dfrac 2 3}^n$\n{{finish|yes, I know}}\nNow we have by Sequence of Powers of Number less than One that:\n:$\\ds \\lim_{n \\mathop \\to \\infty} \\paren {\\frac 2 3}^n = 0$\nand the result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "13942": {"score": 0.8918932676315308, "content": {"text": "\\section{Existence of Probability Space and Discrete Random Variable}\nTags: Probability Theory\n\n\\begin{theorem}\nLet $I$ be an arbitrary countable indexing set.\nLet $S = \\set {s_i: i \\in I} \\subset \\R$ be a countable set of real numbers.\nLet $\\set {\\pi_i: i \\in I} \\subset \\R$ be a countable set of real numbers which satisfies:\n:$\\ds \\forall i \\in I: \\pi_i \\ge 0, \\sum_{i \\mathop \\in I} \\pi_i = 1$\nThen there exists:\n:a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$\nand:\n:a discrete random variable $X$ on $\\struct {\\Omega, \\Sigma, \\Pr}$\nsuch that the probability mass function $p_X$ of $X$ is given by:\n{{begin-eqn}}\n{{eqn | l = \\map {p_X} {s_i}\n      | r = \\pi_i\n      | c = if $i \\in I$\n}}\n{{eqn | l = \\map {p_X} s\n      | r = 0\n      | c = if $s \\notin S$\n}}\n{{end-eqn}}\n\\end{theorem}\n\n\\begin{proof}\nTake $\\Omega = S$ and $\\Sigma = \\powerset S$ (the power set of $S$).\nThen let:\n:$\\ds \\map \\Pr A = \\sum_{i: s_i \\mathop \\in A} \\pi_i$\nfor all $A \\in \\Sigma$.\nThen we can define $X: \\Omega \\to \\R$ by:\n:$\\forall \\omega \\in \\Omega: \\map X \\omega = \\omega$\nThis suits the conditions of the assertion well enough.\n{{qed}}\n\\end{proof}\n\n"}}, "19682": {"score": 0.8769993185997009, "content": {"text": "\\section{Probability of Continuous Random Variable Lying in Singleton Set is Zero/Corollary}\nTags: Probability of Continuous Random Variable Lying in Singleton Set is Zero\n\n\\begin{theorem}\nLet $\\struct {\\Omega, \\Sigma, \\Pr}$ be a probability density function. \nLet $X$ be a continuous real variable on $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $C$ be a countable subset of $\\R$. \nThen: \n:$\\map \\Pr {X \\in C} = 0$\n\\end{theorem}\n\n\\begin{proof}\nSince $C$ is countable, there exists a sequence $\\sequence {x_n}_{n \\mathop \\in \\N}$ of distinct real numbers such that: \n:$C = \\set {x_n : n \\mathop \\in \\N}$\nThat is: \n:$\\ds C = \\bigcup_{n \\mathop = 1}^\\infty \\set {x_n}$\nwhere $\\set {\\set {x_1}, \\set {x_2}, \\ldots}$ is pairwise disjoint.\nWe then have: \n{{begin-eqn}}\n{{eqn\t| l = \\map \\Pr {X \\in C}\n\t\t| r = \\map {P_X} C\n}}\n{{eqn\t| r = \\map {P_X} {\\bigcup_{n \\mathop = 1}^\\infty \\set {x_n} }\n}}\n{{eqn\t| r = \\sum_{n \\mathop = 1}^\\infty \\map {P_X} {\\set {x_n} }\n\t\t| c = using the countable additivity of $P_X$\n}}\n{{eqn\t| r = \\sum_{n \\mathop = 1}^\\infty \\map \\Pr {X \\in \\set {x_n} }\n\t\t| c = {{Defof|Probability Distribution}}\n}}\n{{eqn\t| r = \\sum_{n \\mathop = 1}^\\infty \\map \\Pr {X = x_n}\n}}\n{{eqn\t| r = 0\n\t\t| c = Probability of Continuous Random Variable Lying in Singleton Set is Zero\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Probability of Continuous Random Variable Lying in Singleton Set is Zero\n\\end{proof}\n\n"}}, "2511": {"score": 0.8926159739494324, "content": {"text": "\\begin{definition}[Definition:Discrete Probability Measure]\nLet $\\Omega$ be a countable set.\nLet $\\powerset \\Omega$ be its power set, regarded as a $\\sigma$-algebra.\nLet $\\paren {p_\\omega}_{\\omega \\mathop \\in \\Omega} \\subseteq \\closedint 0 1$ be a subset of the closed unit interval in $\\R$, indexed by $\\Omega$.\nSuppose that $\\ds \\sum_{\\omega \\mathop \\in \\Omega} p_\\omega = 1$.\n{{explain|link to definition of sum over countable set}}\nThe '''discrete probability measure on $\\Omega$''', denoted $P$, is the mapping defined by:\n:$\\ds P: \\powerset \\Omega \\to \\overline \\R, \\map P S = \\sum_{\\omega \\mathop \\in \\Omega} p_\\omega \\map {\\delta_\\omega} S$\nwhere $\\overline \\R$ denotes the extended real numbers, and $\\delta_\\omega$ is the Dirac measure at $\\omega$.\nFrom this definition, it is seen that the name '''discrete probability measure''' is compatible with the notion of discrete measure, as $\\Omega$ is countable.\n{{MissingLinks|to eg. Definition:Discrete Sample Space and connected stuff}}\n\\end{definition}"}}, "13930": {"score": 0.8790878057479858, "content": {"text": "\\section{Existence of Non-Measurable Subset of Real Numbers}\nTags: Measure Theory, Axiom of Choice, Analysis\n\n\\begin{theorem}\nThere exists a subset of the real numbers which is not measurable.\n\\end{theorem}\n\n\\begin{proof}\nWe construct such a set.\nFor $x, y \\in \\hointr 0 1$, define the sum modulo 1:\n:$x +_1 y = \\begin {cases} x + y & : x + y < 1 \\\\ x + y - 1 & : x + y \\ge 1 \\end {cases}$\nLet $E \\subset \\hointr 0 1$ be a measurable set.\nLet $E_1 = E \\cap \\hointr 0 {1 - x}$ and $E_2 = E \\cap \\hointr {1 - x} 1$.\nBy Measure of Interval is Length, these disjoint intervals are measurable.\nBy Measurable Sets form Algebra of Sets, so are these intersections $E_1$ and $E_2$.\nSo:\n:$\\map m {E_1} + \\map m {E_2} = \\map m E$\nWe have:\n:$E_1 +_1 x = E_1 + x$\nBy Lebesgue Measure is Translation-Invariant:\n:$\\map m {E_1 +_1 x} = \\map m {E_1}$\nAlso:\n:$E_2 +_1 x = E_2 + x - 1$\nand so:\n:$\\map m {E_2 +_1 x} = \\map m {E_2}$\nThen we have:\n:$\\map m {E +_1 x} = \\map m {E_1 +_1 x} + \\map m {E_2 +_1 x} = \\map m {E_1} + \\map m {E_2} = \\map m E$\nSo, for each $x \\in \\hointr 0 1$, the set $E +_1 x$ is measurable and:\n:$\\map m {E + x} = \\map m E$\nTaking, as before, $x, y \\in \\hointr 0 1$, define the relation:\n:$x \\sim y \\iff x - y \\in \\Q$\nwhere $\\Q$ is the set of rational numbers.\nBy Difference is Rational is Equivalence Relation, $\\sim$ is an equivalence relation.\nAs this is an equivalence relation we can invoke the fundamental theorem on equivalence relations.\nHence $\\sim$ partitions $\\hointr 0 1$ into equivalence classes.\nBy the axiom of choice, there is a set $P$ which contains exactly one element from each equivalence class.\n{{AimForCont}} $P$ is measurable.\nLet $\\set {r_i}_{i \\mathop = 0}^\\infty$ be an enumeration of the rational numbers in $\\hointr 0 1$ with $r_0 = 0$.\nLet $P_i := P +_1 r_i$.\nThen $P_0 = P$.\nLet $x \\in P_i \\cap P_j$.\nThen:\n:$x = p_i + r_i = p_j + r_j$\nwhere $p_i, p_j$ are elements of $P$.\nBut then $p_i - p_j$ is a rational number.\nSince $P$ has only one element from each equivalence class:\n:$i = j$\nThe $P_i$ are pairwise disjoint.\nEach real number $x \\in \\hointr 0 1$ is in ''some'' equivalence class and hence is equivalent to an element of $P$.\nBut if $x$ differs from an element in $P$ by the rational number $r_i$, then $x \\in P_i$ and so:\n:$\\ds \\bigcup P_i = \\hointr 0 1$\nSince each $P_i$ is a translation modulo $1$ of $P$, each $P_i$ will be measurable if $P$ is, with measure $\\map m {P_i} = \\map m P$.\nBut if this were the case, then:\n:$\\ds m \\hointr 0 1 = \\sum_{i \\mathop = 1}^\\infty m \\paren {P_i} = \\sum_{i \\mathop = 1}^\\infty \\map m P$\nTherefore:\n:$\\map m P = 0$ implies $m \\hointr 0 1 = 0$\nand:\n:$\\map m P \\ne 0$ implies $m \\hointr 0 1 = \\infty$\nThis contradicts Measure of Interval is Length.\nSo the set $P$ is not measurable.\n{{Qed}}\n{{AoC||4}}\n{{BPI}}\n{{explain|While BPI has been invoked as being necessary for this theorem, no explanation has been added as to why, or how, or where it would be applied.}}\n\\end{proof}\n\n"}}, "15461": {"score": 0.8905619978904724, "content": {"text": "\\section{Countable Sets Have Measure Zero}\nTags: Analysis\n\n\\begin{theorem}\nLet $S$ be a countable set.\n{{explain|Is it assumed that $S \\subseteq \\R$?}}\nThen the measure of $S$ is $\\map m S = 0$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\ds \\set {x_i}_{i \\mathop = 1}^\\infty$ be an enumeration of the elements of $S$.\nFor any (strictly) positive real number $\\epsilon$, define:\n:$A_i = \\paren {x_i - 2^{-i} \\epsilon, x_i + 2^{-i} \\epsilon}$\n{{explain|The notation of the above seems to be of an open real interval. This needs to be clarified.}}\nThen:\n:$\\ds S \\subseteq \\bigcup_{i \\mathop = 1}^\\infty A_i$\nand:\n:$\\ds \\map m {\\bigcup A_i} \\le \\sum_{i \\mathop = 1}^\\infty 2^{1 - i} \\epsilon = 2 \\epsilon$\nSince our choice of $\\epsilon$ was arbitrary, for any positive real $z$ we can construct a set $T$ such that $S \\subseteq T$ and $\\map m T \\le z$.\nHence $X$ has zero measure.\n{{qed}}\nCategory:Analysis\n\\end{proof}\n\n"}}}}, "TheoremQA_tonyxia/maxplanar1.json": {"gold": {"14139": 1, "11239": 1}, "retrieved": {"18389": {"score": 0.8813706040382385, "content": {"text": "\\section{Ore Graph is Connected}\nTags: Graph Theory, Ore Graphs\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be an Ore graph.\nThen $G$ is connected.\n\\end{theorem}\n\n\\begin{proof}\nLet $G$ be an Ore graph.\n{{AimForCont}} $G$ is not connected.\nThen it has at least two components.\nCall these components $C_1$ and $C_2$.\nThus, there exist non-adjacent vertices $u$ and $v$ such that $u$ is in $C_1$ and $v$ is in $C_2$.\nLet $m_1$ and $m_2$ be the number of vertices in $C_1$ and $C_2$ respectively.\nIt is clear that:\n:$m_1 + m_2 \\le n$\nBy definition of Ore graph, $G$ is simple.\nThus it follows that:\n:$\\map {\\deg_G} u \\le m_1 - 1$ and $\\map {\\deg_G} v \\le m_2 - 1$\nThus:\n{{begin-eqn}}\n{{eqn | l = \\map {\\deg_G} u + \\map {\\deg_G} v\n      | o = \\le\n      | r = m_1 - 1 + m_2 - 1\n      | c = \n}}\n{{eqn | r = m_1 + m_2 - 2\n      | c = \n}}\n{{eqn | o = <\n      | r = m_1 + m_2\n      | c = \n}}\n{{eqn | o = \\le\n      | r = n\n      | c = \n}}\n{{end-eqn}}\nThat is:\n:$\\map {\\deg_G} u + \\map {\\deg_G} v < n$\nBut by definition of Ore graph:\n:$\\map {\\deg_G} u + \\map {\\deg_G} v \\ge n$\nBy Proof by Contradiction, it follows that our assumption that $G$ is not connected was false.\nHence the result.\n{{Qed}}\n\\end{proof}\n\n"}}, "18604": {"score": 0.8843204379081726, "content": {"text": "\\section{Path Graph from Cycle Graph}\nTags: Graph Theory\n\n\\begin{theorem}\nRemoving one edge from a cycle graph leaves a path graph.\n\\end{theorem}\n\n\\begin{proof}\nLet $G_n$ be the graph obtained by removing any edge from the cycle graph $C_n$.\nAs each of those edges lies on a cycle, by Condition for Edge to be Bridge, none of them is a bridge.\nSo removing any edge from $C_n$ leaves the resulting subgraph of $C_n$ connected.\nFrom Size of Cycle Graph equals Order, the cycle graph $C_n$ has $n$ edges.\nSo $G_n$ has $n$ nodes and $n - 1$ edges, and is connected.\nFrom Size of Tree is One Less than Order, $G_n$ is a tree.\nBut $G_n$ has two nodes of degree $1$, and all the others are of degree $2$.\nIt follows that $G_n$ is traversable, and hence by definition is a path graph.\n{{qed}}\nCategory:Graph Theory\n\\end{proof}\n\n"}}, "18606": {"score": 0.8906819224357605, "content": {"text": "\\section{Path in Tree is Unique/Necessary Condition}\nTags: Tree Theory, Graph Theory, Trees, Proofs by Contraposition\n\n\\begin{theorem}\nLet $T$ be a tree.\nThen there is exactly one path between any two vertices.\n\\end{theorem}\n\n\\begin{proof}\nLet $T$ be a tree.\n{{AimForCont}} there exists a pair of vertices $u$ and $v$ in $T$ such that there is not exactly one path between them.\nIf there is no path between $u$ and $v$, $T$ is not connected.\nIn this case, $T$ is certainly not a tree.\nSo, in keeping with our supposition, there is more than one path between $u$ and $v$.\nLet two of these paths be:\n:$P_1 = \\tuple {u, u_1, \\ldots, u_i, r_1, r_2, \\ldots, r_{j - 1}, r_j, u_{i + 1}, \\ldots, v}$\n:$P_2 = \\tuple {u, u_1, \\ldots, u_i, s_1, s_2, \\ldots, s_{k - 1}, s_k, u_{i + 1}, \\ldots, v}$\nNow consider the path:\n:$P_3 = \\tuple {u_i, r_1, r_2, \\ldots, r_{j - 1}, r_j, u_{i + 1}, s_k, s_{k - 1}, \\ldots, s_2, s_1, u_i}$\nIt can be seen that $P_3$ is a circuit.\nThus by definition $T$ can not be a tree.\nFrom Proof by Contradiction it follows that there is exactly one path between any pair of vertices.\n{{qed}}\n\\end{proof}\n\n"}}, "6804": {"score": 0.8892278075218201, "content": {"text": "\\begin{definition}[Definition:Path Graph]\nA '''path graph''' is a tree which has a path which passes through all its vertices.\nThe path graph with $n$ vertices is denoted $P_n$.\n\\end{definition}"}}, "18607": {"score": 0.8864737153053284, "content": {"text": "\\section{Path in Tree is Unique/Sufficient Condition}\nTags: Tree Theory, Graph Theory, Trees\n\n\\begin{theorem}\nLet $T$ be a graph.\nLet $T$ be such that between any two vertices there is exactly one path.\nThen $T$ is a tree.\n\\end{theorem}\n\n\\begin{proof}\nLet $T$ be such that between any two vertices there is exactly one path.\nThen for a start $T$ is by definition connected.\nSuppose $T$ had a circuit, say $\\left({u, u_1, u_2, \\ldots, u_n, v, u}\\right)$.\nThen there are two paths from $u$ to $v$:\n: $\\left({u, u_1, u_2, \\ldots, u_n, v}\\right)$\nand\n: $\\left({u, v}\\right)$.\nHence, by Modus Tollendo Tollens, $T$ can have no circuits.\nThat is, by definition, $T$ is a tree.\n{{qed}}\n\\end{proof}\n\n"}}, "14217": {"score": 0.9060463905334473, "content": {"text": "\\section{Equivalent Definitions for Finite Tree}\nTags: Tree Theory, Graph Theory, Trees\n\n\\begin{theorem}\nLet $T$ be a finite tree of order $n$.\nThe following statements are equivalent:\n: $(1): \\quad T$ is connected and has no circuits.\n: $(2): \\quad T$ has $n-1$ edges and has no circuits.\n: $(3): \\quad T$ is connected and has $n-1$ edges.\n: $(4): \\quad T$ is connected, and the removal of any one edge renders $T$ disconnected.\n: $(5): \\quad$ Any two vertices of $T$ are connected by exactly one path.\n: $(6): \\quad T$ has no circuits, but adding one edge creates a cycle.\n\\end{theorem}\n\n\\begin{proof}\nStatement $1$ is the usual definition of a tree.\n\\end{proof}\n\n"}}, "21471": {"score": 0.8907087445259094, "content": {"text": "\\section{Size of Tree is One Less than Order}\nTags: Size of Tree is One Less than Order, Tree Theory, Graph Theory, Trees\n\n\\begin{theorem}\nLet $T$ be a connected simple graph of order $n$.\nThen $T$ is a tree {{Iff}} the size of $T$ is $n-1$.\n\\end{theorem}\n\n\\begin{proof}\nBy definition:\n:the order of a tree is how many nodes it has\nand:\n:its size is how many edges it has.\n\\end{proof}\n\n"}}, "21475": {"score": 0.921976625919342, "content": {"text": "\\section{Size of Tree is One Less than Order/Sufficient Condition}\nTags: Size of Tree is One Less than Order, Tree Theory, Trees\n\n\\begin{theorem}\nLet $T$ be a connected simple graph of order $n$.\nLet the size of $T$ be $n-1$.\nThen $T$ is a tree.\n\\end{theorem}\n\n\\begin{proof}\nBy definition, the order of a tree is how many nodes it has, and its size is how many edges it has.\nSuppose $T$ is a connected simple graph of order $n$ with $n - 1$ edges.\nWe need to show that $T$ is a tree.\n{{AimForCont}} $T$ is not a tree.\nThen it contains a circuit.\nIt follows from Condition for Edge to be Bridge that there is at least one edge in $T$ which is not a bridge.\nSo we can remove this edge and obtain a graph $T'$ which is connected and has $n$ nodes and $n - 2$ edges.\nLet us try and construct a connected graph with $n$ nodes and $n - 2$ edges.\nWe start with the edgeless graph $N_n$, and add edges till the graph is connected.\nWe pick any two vertices of $N_n$, label them $u_1$ and $u_2$ for convenience, and use one edge to connect them, labelling that edge $e_1$.\nWe pick any other vertex, label it $u_3$, and use one edge to connect it to either $u_1$ or $u_2$, labelling that edge $e_2$.\nWe pick any other vertex, label it $u_4$, and use one edge to connect it to either $u_1, u_2$ or $u_3$, labelling that edge $e_3$.\nWe continue in this way, until we pick a vertex, label it $u_{n - 1}$, and use one edge to connect it to either $u_1, u_2, \\ldots, u_{n - 2}$, labelling that edge $e_{n - 2}$.\nThat was the last of our edges, and the last vertex still has not been connected.\nTherefore a graph with $n$ vertices and $n-2$ edges that such a graph ''cannot'' be connected.\nTherefore we cannot remove any edge from $T$ without leaving it disconnected.\nTherefore all the edges in $T$ are bridges.\nHence $T$ can contain no circuits.\nHence, by Proof by Contradiction, $T$ must be a tree.\n{{qed}}\n\\end{proof}\n\n"}}, "8694": {"score": 0.8930138349533081, "content": {"text": "\\begin{definition}[Definition:Spanning Tree/Building-Up Method]\nStart with the edgeless graph $N$ whose vertices correspond with those of $G$.\nSelect edges of $G$ one by one, such that no cycles are created, and add them to $N$.\nContinue till all vertices are included.\n\\end{definition}"}}, "8693": {"score": 0.8942859768867493, "content": {"text": "\\begin{definition}[Definition:Spanning Tree]\nLet $G$ be a connected graph.\nA '''spanning tree for $G$''' is a spanning subgraph of $G$ which is also a tree.\n{{refactor|Move the below into its own page}}\nClearly a tree is its own spanning tree:\nAs a tree $T$ of order $n$ has $n - 1$ edges, its spanning tree must also contain $n-1$ edges, and those must be the same ones as in $T$.\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/kepler's_law3.json": {"gold": {"11654": 1, "10831": 1}, "retrieved": {"385": {"score": 0.7857967615127563, "content": {"text": "\\begin{definition}[Definition:Astronomical Distance Units/Astronomical Unit]\nThe '''astronomical unit''' is a derived unit of length.\nIt is defined as being $149 \\, 597 \\, 870 \\, 700$ metres.\nThe '''astronomical unit''' is the standard unit of measurement used by astronomers when discussing distances within the solar system.\nIt is derived as the approximate mean distance from the Earth to the Sun.\n\\end{definition}"}}, "5607": {"score": 0.7973839044570923, "content": {"text": "\\begin{definition}[Definition:Mercury (Planet)]\n'''Mercury''' is the innermost planet of the solar system.\nIts orbit lies within that of Venus.\nCategory:Definitions/Solar System\n\\end{definition}"}}, "388": {"score": 0.808764636516571, "content": {"text": "\\begin{definition}[Definition:Astronomical Unit/Approximate Values]\nThe '''astronomical unit''' is approximately $150 \\, 000 \\, 000 \\, \\mathrm {k m}$, or $93$ million (international) miles.\n\\end{definition}"}}, "6413": {"score": 0.8040614724159241, "content": {"text": "\\begin{definition}[Definition:Orbit (Physics)]\nThe '''orbit''' of body $A$ around body $B$ is the path taken by $A$ as it travels around $B$ under the influence of the force acting between $A$ and $B$.\n\\end{definition}"}}, "2783": {"score": 0.7976035475730896, "content": {"text": "\\begin{definition}[Definition:Ellipse/Eccentricity]\nLet $K$ be an ellipse specified in terms of:\n:a given straight line $D$\n:a given point $F$\n:a given constant $e$ such that $0 < e < 1$\nwhere $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:\n:$q = e p$\nThe constant $e$ is known as the '''eccentricity''' of the ellipse.\n\\end{definition}"}}, "6414": {"score": 0.8944673538208008, "content": {"text": "\\begin{definition}[Definition:Orbit (Physics)/Period]\nThe '''period''' of an orbit of body $A$ around body $B$ is the length of time it takes for $A$ to travel once around $B$ and return to its original position.\n\\end{definition}"}}, "14700": {"score": 0.8221784830093384, "content": {"text": "\\section{Eccentricity of Orbit indicates its Total Energy}\nTags: Celestial Mechanics\n\n\\begin{theorem}\nConsider a planet $p$ of mass $m$ orbiting a star $S$ of mass $M$ under the influence of the gravitational field which the two bodies give rise to.\nThen the total energy of the system determines the eccentricity of the orbit of $p$ around $S$.\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$\\mathbf u_r$ be the unit vector in the direction of the radial coordinate of $p$\n:$\\mathbf u_\\theta$ be the unit vector in the direction of the angular coordinate of $p$.\nBy Kinetic Energy of Motion, the kinetic energy of $p$ is:\n:$K = \\dfrac {m v^2} 2$\nwhere $v$ is the magnitude of the velocity of $p$.\nThus:\n{{begin-eqn}}\n{{eqn | l = K\n      | r = \\dfrac {m \\mathbf v \\cdot \\mathbf v} 2\n      | c = Dot Product of Vector with Itself\n}}\n{{eqn | r = \\dfrac 1 2 m \\paren {r \\dfrac {\\d \\theta} {\\d t} \\mathbf u_\\theta + \\dfrac {\\d r} {\\d t} \\mathbf u_r} \\cdot \\paren {r \\dfrac {\\d \\theta} {\\d t} \\mathbf u_\\theta + \\dfrac {\\d r} {\\d t} \\mathbf u_r}\n      | c = Velocity Vector in Polar Coordinates\n}}\n{{eqn | n = 1\n      | r = \\dfrac 1 2 m \\paren {r^2 \\paren {\\dfrac {\\d \\theta} {\\d t} }^2 + \\paren {\\dfrac {\\d r} {\\d t} }^2}\n      | c = {{Defof|Dot Product}}\n}}\n{{end-eqn}}\nThe potential energy $P$ of the system is the negative of the work required to move $p$ to infinity:\n{{MissingLinks|Definition of Work, the above definition of P.E.}}\n{{begin-eqn}}\n{{eqn | l = P\n      | r = -\\int_r^\\infty \\dfrac {G M m} {r^2} \\rd r\n      | c = Newton's Law of Universal Gravitation\n}}\n{{eqn | r = \\intlimits {\\dfrac {G M m} r} r \\infty\n      | c = Primitive of Power\n}}\n{{eqn | r = -\\dfrac {G M m} r\n      | c = \n}}\n{{eqn | n = 2\n      | r = -\\dfrac {k m} r\n      | c = defining $k = G M$: constant for this system\n}}\n{{end-eqn}}\nBy the Principle of Conservation of Energy, the total energy in the system remains constant: $E$, say.\nSo:\n:$E = \\dfrac 1 2 m \\paren {r^2 \\paren {\\dfrac {\\d \\theta} {\\d t} }^2 + \\paren {\\dfrac {\\d r} {\\d t} }^2} - \\dfrac {k m} r$\n{{WLOG}}, let us arrange the polar axis so as to make $r$ a minimum when $\\theta = 0$.\nBy Kepler's First Law of Planetary Motion, the position of $p$ in polar coordinates is:\n:$(3): \\quad r = \\dfrac {h^2 / k} {1 + e \\cos \\theta}$\nAt the instant when $\\theta = 0$, we therefore have:\n:$r = \\dfrac {h^2 / k} {1 + e}$\nAt this point, $r$ is at a local minimum.\nHence:\n:$\\dfrac {m r^2} 2 \\dfrac {h^2} {r^4} - \\dfrac {k m} r = E$\nEliminating $r$ from these gives:\n:$e = \\sqrt {1 + E \\paren {\\dfrac {2 h^2} {m k^2} } }$\nThus equation $(3)$ for the orbit of $p$ can be written as:\n:$r = \\dfrac {h^2 / k} {1 + \\sqrt {1 + E \\paren {2 h^2 / m k^2} \\cos \\theta} }$\nThus from Equation of Conic Section in Polar Form, it can be seen that the orbit is:\n:an ellipse when $E < 0$\n:a parabola when $E = 0$\n:a hyperbola when $E > 0$.\n{{qed}}\n\\end{proof}\n\n"}}, "11654": {"score": 0.9152004718780518, "content": {"text": "\\section{Kepler's Laws of Planetary Motion/Third Law/Examples}\nTags: Kepler's Laws of Planetary Motion\n\n\\begin{theorem}\nLet $P$ be a planet orbiting the sun $S$\nLet $P$ be:\n:$\\text{(a)}: \\quad$ Twice as far away from $S$ as the Earth;\n:$\\text{(b)}: \\quad$ $3$ times as far away from $S$ as the Earth;\n:$\\text{(c)}: \\quad$ $25$ times as far away from $S$ as the Earth.\nThen the orbital period of $P$ is:\n:$\\text{(a)}: \\quad$ approximately $2.8$ years;\n:$\\text{(b)}: \\quad$ approximately $5.2$ years;\n:$\\text{(c)}: \\quad$ $125$ years.\n\\end{theorem}\n\n\\begin{proof}\nLet the orbital period of Earth be $T'$ years.\nLet the mean distance of Earth from $S$ be $A$.\nLet the orbital period of $P$ be $T$ years.\nLet the mean distance of $P$ from $S$ be $a$.\nBy Kepler's Third Law of Planetary Motion:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {T'^2} {A^3}\n      | r = \\dfrac {T^2} {a^3}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = T^2\n      | r = \\dfrac {a^3} {A^3}\n      | c = as $T'$ is $1$ year\n}}\n{{eqn | ll= \\leadsto\n      | l = T\n      | r = \\left({\\dfrac a A}\\right)^{3/2}\n      | c = \n}}\n{{end-eqn}}\nThus the required orbital periods are:\n:$\\text{(a)}: \\quad 2^{3/2} = 2 \\sqrt 2 \\approx 2.8$ years\n:$\\text{(b)}: \\quad 3^{3/2} = 3 \\sqrt 3 \\approx 5.2$ years\n:$\\text{(c)}: \\quad 25^{3/2} = 125$ years.\n{{qed}}\n\\end{proof}\n\n"}}, "4841": {"score": 0.8425211906433105, "content": {"text": "\\begin{definition}[Definition:Kepler's Equation]\nConsider a system consisting of two bodies $B_1$ and $B_2$ in elliptical orbit around each other.\nLet $e$ denote the eccentricity of that orbit.\n'''Kepler's equation''' is the equation that describes the relation between the mean anomaly and eccentric anomaly of $B_1$ with respect to $B_2$:\n:$M = E - e \\sin E$\nwhere:\n:$M$ is the mean anomaly\n:$E$ is the eccentric anomaly.\n{{NamedforDef|Johannes Kepler|cat = Kepler}}\n\\end{definition}"}}, "10831": {"score": 0.8707086443901062, "content": {"text": "\\section{Mass of Sun from Gravitational Constant}\nTags: Celestial Mechanics\n\n\\begin{theorem}\nLet the gravitational constant be known.\nLet the mean distance from the Earth to the sun be known.\nThen it is possible to calculate the mass of the sun.\n\\end{theorem}\n\n\\begin{proof}\nFrom Kepler's Third Law of Planetary Motion:\n:$T^2 = \\left({\\dfrac {4 \\pi^2} {G M} }\\right) a^3$\nwhere:\n:$T$ is the orbital period of the planet in question (in this case, the Earth)\n:$a$ is the distance from the planet (in this case, the Earth) to the sun\n:$M$ is the mass of the sun\n:$G$ is the gravitational constant\nIn MKS units:\n:$T = 60 \\times 60 \\times 24 \\times 365.24219 \\, \\mathrm s$ by definition of year\n:$a = 149 \\, 597 \\, 870 \\, 700 \\, \\mathrm m$ by definition of astronomical unit\n:$G = 6.674 \\times 10^{-11} \\, \\mathrm N \\, \\mathrm m^2 \\, \\mathrm{kg}^{-2}$ by measurement, as by hypothesis.\n:$M$ will be the result in kilograms.\nThus:\n{{begin-eqn}}\n{{eqn | l = M\n      | r = \\dfrac {4 \\pi^2 a^3} {T^2 G}\n      | c = \n}}\n{{eqn | r = \\frac {4 \\times \\left({3.14159}\\right)^2 \\times \\left({149.60 \\times 10^9}\\right)^3} {\\left({31.557 \\times 10^6}\\right) \\times 6.674 \\times 10^{-11} }\n      | c = \n}}\n{{end-eqn}}\nThe calculation has been left as an exercise for anyone who has the patience.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Binomial_2.json": {"gold": {"10463": 1}, "retrieved": {"6877": {"score": 0.7999072670936584, "content": {"text": "\\begin{definition}[Definition:Permutation/Ordered Selection/Notation]\nThe number of $r$-permutations from a set of cardinality $n$ is denoted variously:\n:$P_{n r}$\n:${}^r P_n$\n:${}_r P_n$\n:${}_n P_r$ (extra confusingly)\nThere is little consistency in the literature).\nOn {{ProofWiki}} the notation of choice is ${}^r P_n$.\nCategory:Definitions/Permutation Theory\n\\end{definition}"}}, "17941": {"score": 0.8044091463088989, "content": {"text": "\\section{Number of Permutations}\nTags: Permutations, Permutation Theory, Number of Permutations, Combinatorics\n\n\\begin{theorem}\nLet $S$ be a set of $n$ elements.\nLet $r \\in \\N: r \\le n$.\nThen the number of $r$-permutations of $S$ is:\n:${}^r P_n = \\dfrac {n!} {\\paren {n - r}!}$\nWhen $r = n$, this becomes:\n:${}^n P_n = \\dfrac {n!} {\\paren {n - n}!} = n!$\nUsing the falling factorial symbol, this can also be expressed:\n:${}^r P_n = n^{\\underline r}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition, an <math>r</math>-permutations of <math>S</math> is an ordered selection of <math>r</math> elements of <math>S</math>.\nIt can be seen that an <math>r</math>-permutation is an injection from a subset of <math>S</math> into <math>S</math>.\nFrom Cardinality of Set of Injections, we see that the number of <math>r</math>-permutations <math>{}^r P_n</math> on a set of <math>n</math> elements is given by:\n:<math>{}^r P_n = \\frac {n!} {\\left({n-r}\\right)!}</math>\nFrom this definition, it can be seen that a bijection <math>f: S \\to S</math> (as defined above) is an '''<math>n</math>-permutation'''.\nHence the number of <math>r</math>-permutations on a set of <math>n</math> elements is <math>{}^n P_n = \\frac {n!} {\\left({n-n}\\right)!} = n!</math>.\n{{Qed}}\nCategory:Combinatorics\n24405\n24403\n2010-01-14T06:55:12Z\nPrime.mover\n59\n24405\nwikitext\ntext/x-wiki\n\\end{proof}\n\n"}}, "1332": {"score": 0.8069112300872803, "content": {"text": "\\begin{definition}[Definition:Combination]\nLet $S$ be a set containing $n$ elements.\nAn '''$r$-combination of $S$''' is a subset of $S$ which has $r$ elements.\n\\end{definition}"}}, "15526": {"score": 0.8063927292823792, "content": {"text": "\\section{Count of All Permutations on n Objects}\nTags: Permutation Theory, Count of All Permutations on n Objects\n\n\\begin{theorem}\nLet $S$ be a set of $n$ objects.\nLet $N$ be the number of permutations of $r$ objects from $S$, where $1 \\le r \\le N$.\nThen:\n:$\\ds N = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}$\n\\end{theorem}\n\n\\begin{proof}\nThe number of permutations on $k$ objects, from $n$ is denoted ${}^k P_{10}$.\nFrom Number of Permutations:\n:${}^k P_n = \\dfrac {n!} {\\paren {n - k}!}$\nHence:\n{{begin-eqn}}\n{{eqn | q = \n      | l = N\n      | r = \\sum_{k \\mathop = 1}^n {}^k P_n\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {\\paren {n - k}!}\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {k!}\n      | c = \n}}\n{{eqn | r = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Permutation Theory\nCategory:Count of All Permutations on n Objects\n\\end{proof}\n\n"}}, "3959": {"score": 0.8053659200668335, "content": {"text": "\\begin{definition}[Definition:Hat-Check Triangle]\nThe Hat-Check Triangle is an array formed by the product of binomial coefficients with the subfactorial which produces the hat-check distribution for any value of $n$:\n:$\\begin{array}{r|rrrrrrrrrr}\nn & !0 \\binom n 0 & !1 \\binom n 1 & !2 \\binom n 2 & !3 \\binom n 3 & !4 \\binom n 4 & !5 \\binom n 5 & !6 \\binom n 6 & !7 \\binom n 7 & !8 \\binom n 8 & !9 \\binom n 9 \\\\\n\\hline\n0  & 1 &  0 &  0 &  0  &  0   &   0   &   0   &   0    &   0    &  0 \\\\\n1  & 1 &  0 &  0 &  0  &  0   &   0   &   0   &   0    &   0    &  0 \\\\\n2  & 1 &  0 &  1 &  0  &  0   &   0   &   0   &   0    &   0    &  0 \\\\\n3  & 1 &  0 &  3 &  2  &  0   &   0   &   0   &   0    &   0    &  0 \\\\\n4  & 1 &  0 &  6 &  8  &  9   &   0   &   0   &   0    &   0    &  0 \\\\\n5  & 1 &  0 & 10 & 20  & 45   &  44   &   0   &   0    &   0    &  0 \\\\\n6  & 1 &  0 & 15 & 40  & 135  &  264  & 265   &   0    &   0    &  0 \\\\\n7  & 1 &  0 & 21 & 70  & 315  &  924  & 1855  & 1854   &   0    &  0 \\\\\n8  & 1 &  0 & 28 & 112 & 630  & 2464  & 7420  & 14832  & 14833  &  0 \\\\\n9  & 1 &  0 & 36 & 168 & 1134 & 5544  & 22260 & 66744  & 133497 &  133496 \\\\\n\\end{array}$\n{{OEIS|A098825}}\nThus the entry in row $n$ and column $k$ contains the product of the binomial coefficient $\\dbinom n k$ with the subfactorial $!k$.\n\\end{definition}"}}, "16939": {"score": 0.8373447060585022, "content": {"text": "\\section{Cardinality of Set of Combinations with Repetition}\nTags: Combinations with Repetition\n\n\\begin{theorem}\nLet $S$ be a finite set with $n$ elements\nThe number of $k$-combinations of $S$ with repetition is given by:\n:$N = \\dbinom {n + k - 1} k$\n\\end{theorem}\n\n\\begin{proof}\nLet the elements of $S$ be (totally) ordered in some way, by assigning an index to each element.\nThus let $S = \\left\\{ {a_1, a_2, a_3, \\ldots, a_n}\\right\\}$.\nThus each $k$-combination of $S$ with repetition can be expressed as:\n:$\\left\\{ {\\left\\{ {a_{r_1}, a_{r_1}, \\ldots, a_{r_k} }\\right\\} }\\right\\}$\nwhere:\n:$r_1, r_2, \\ldots, r_k$ are all elements of $\\left\\{ {1, 2, \\ldots, n}\\right\\}$\nand such that:\n:$r_1 \\le r_2 \\le \\cdots \\le r_k$\nHence the problem reduces to the number of integer solutions $\\left({r_1, r_2, \\ldots, r_k}\\right)$ such that $1 \\le r_1 \\le r_2 \\le \\cdots \\le r_k \\le n$.\nThis is the same as the number of solutions to:\n:$0 < r_1 < r_2 + 1 < \\cdots < r_k + k - 1 < n + k$\nwhich is the same as the number of solutions to:\n:$0 < s_1 < s_2 < \\cdots < s_k < n + k$\nwhich is the number of ways to choose $k$ elements from $n + k - 1$.\nThis is the same as the number of subsets as a set with $n + k - 1$ elements.\nFrom Cardinality of Set of Subsets:\n:$N = \\dbinom {n + k - 1} k$\n{{qed}}\n\\end{proof}\n\n"}}, "12461": {"score": 0.811694324016571, "content": {"text": "\\section{Incidence of Double Letters in Fibonacci String}\nTags: Fibonacci Strings\n\n\\begin{theorem}\nLet $S_n$ denote the $n$th Fibonacci string.\nThen:\n:$(1):\\quad$ There are no instances of $2$ $\\text a$'s together\n:$(2):\\quad$ There are no instances of $3$ $\\text b$'s together\nin $S_n$.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by strong induction.\nFor all $n \\in \\Z_{\\ge 1}$, let $\\map P n$ be the proposition:\n:There are no instances of $2$ $\\text a$'s or $3$ $\\text b$'s together in $S_n$.\n$\\map P 1$ is the case:\n:$S_1 = \\text a$\n\\end{proof}\n\n"}}, "17942": {"score": 0.8506721258163452, "content": {"text": "\\section{Number of Permutations with Repetition}\nTags: Number of Permutations with Repetition, Combinatorics\n\n\\begin{theorem}\nSet $S$ be a set of $n$ elements.\nLet $\\sequence T_m$ be a sequence of $m$ terms of $S$.\nThen there are $n^m$ different instances of $\\sequence T_m$.\n\\end{theorem}\n\n\\begin{proof}\nLet $N_m$ denote the set $\\set {1, 2, \\ldots, m}$.\nLet $f: N_m \\to S$ be the mapping defined as:\n:$\\forall k \\in N_m: \\map f t = t_m$\nBy definition, $f$ corresponds to one of the specific instances of $\\sequence T_m$.\nHence the number of different instances of $\\sequence T_m$ is found from Cardinality of Set of All Mappings:\n:$\\card S^{\\card {N_m} }$\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "1333": {"score": 0.8251319527626038, "content": {"text": "\\begin{definition}[Definition:Combination with Repetition]\nLet $S$ be a (finite) set with $n$ elements.\nA '''$k$ combination of $S$ with repetition''' is a multiset with $k$ elements selected from $S$.\n\\end{definition}"}}, "8461": {"score": 0.8262731432914734, "content": {"text": "\\begin{definition}[Definition:Set of Strings of Given Length]\nLet $n \\in \\N$ be a natural number.\nLet $\\Sigma$ be an alphabet.\nThe set of all strings from $\\Sigma$ of length $n$ is denoted $\\Sigma^{\\paren n}$.\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/ODE2.json": {"gold": {"2380": 1, "21599": 1, "5197": 1, "20418": 1, "21600": 1}, "retrieved": {"10456": {"score": 0.8143081068992615, "content": {"text": "\\section{Motion of Body Falling through Air}\nTags: Gravity, Examples of Differential Equations\n\n\\begin{theorem}\nThe motion of a body $B$ falling through air can be described using the following differential equation:\n:$m \\dfrac {\\d^2 y} {\\d t^2} = m g - k \\dfrac {d y} {d t}$\nwhere:\n:$m$ denotes mass of $B$\n:$y$ denotes the height of $B$ from an arbitrary reference\n:$t$ denotes time elapsed from an arbitrary reference\n:$g$ denotes the local gravitational constant acting on $B$\n:$k$ denotes the coefficient of resistive force exerted on $B$ by the air (assumed to be proportional to the speed of $B$)\n\\end{theorem}\n\n\\begin{proof}\nFrom Newton's Second Law of Motion, the force on $B$ equals its mass multiplied by its acceleration.\nThus the force $F$ on $B$ is given by:\n:$F = m \\dfrac {\\d^2 y} {\\d t^2}$\nwhere it is assumed that the acceleration is in a downward direction.\nThe force on $B$ due to gravity is $m g$.\nThe force on $B$ due to the air it is passing through is $k$ multiplied by the speed of $B$, in the opposite direction to its travel.\nThat is::\n:$k \\dfrac {d y} {d t}$\nHence the required differential equation:\n:$m \\dfrac {\\d^2 y} {\\d t^2} = m g - k \\dfrac {d y} {d t}$\n{{qed}}\n\\end{proof}\n\n"}}, "23333": {"score": 0.8149998784065247, "content": {"text": "\\section{Velocity of Rocket in Outer Space}\nTags: Rocket Science, Dynamics\n\n\\begin{theorem}\nLet $B$ be a rocket travelling in outer space.\nLet the velocity of $B$ at time $t$ be $\\mathbf v$.\nLet the mass of $B$ at time $t$ be $m$.\nLet the exhaust velocity of $B$ be constant at $\\mathbf b$.\nThen the velocity of $B$ at time $t$ is given by:\n:$\\map {\\mathbf v} t = \\map {\\mathbf v} 0 + \\mathbf b \\ln \\dfrac {\\map m 0} {\\map m t}$\nwhere $\\map {\\mathbf v} 0$ and $\\map m 0$ are the velocity and mass of $B$ at time $t = 0$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Motion of Rocket in Outer Space, the equation of motion of $B$ is given by:\n:$m \\dfrac {\\d \\mathbf v} {\\d t} = -\\mathbf b \\dfrac {\\d m} {\\d t}$\nHence:\n{{begin-eqn}}\n{{eqn | l = \\int_0^t \\dfrac {\\d \\mathbf v} {\\d t} \\rd t\n      | r = -\\int_0^t \\mathbf b \\frac 1 m \\dfrac {\\d m} {\\d t} \\rd t\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\int_{t \\mathop = 0}^t \\rd \\mathbf v\n      | r = -\\int_{t \\mathop = 0}^t \\mathbf b \\dfrac {\\d m} m\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\bigintlimits {\\mathbf v} 0 t\n      | r = -\\mathbf b \\bigintlimits {\\ln m} 0 t\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map {\\mathbf v} t - \\map {\\mathbf v} 0\n      | r = -\\mathbf b \\paren {\\map {\\ln m} t - \\map {\\ln m} 0}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map {\\mathbf v} t\n      | r = \\map {\\mathbf v} 0 + \\mathbf b \\ln \\dfrac {\\map m 0} {\\map m t}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "17235": {"score": 0.8217622637748718, "content": {"text": "\\section{Body under Constant Acceleration/Distance after Time}\nTags: Mechanics\n\n\\begin{theorem}\nLet $B$ be a body under constant acceleration $\\mathbf a$.\nThen:\n:$\\mathbf s = \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\nwhere:\n:$\\mathbf s$ is the displacement of $B$ from its initial position at time $t$\n:$\\mathbf u$ is the velocity at time $t = 0$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Body under Constant Acceleration: Velocity after Time:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\nBy definition of velocity, this can be expressed as:\n:$\\dfrac {\\d \\mathbf s} {\\d t} = \\mathbf u + \\mathbf a t$\nwhere both $\\mathbf u$ and $\\mathbf a$ are constant.\nBy Solution to Linear First Order Ordinary Differential Equation:\n:$\\mathbf s = \\mathbf c + \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\nwhere $\\mathbf c$ is a constant vector.\nWe are (implicitly) given the initial condition:\n:$\\bigvalueat {\\mathbf s} {t \\mathop = 0} = \\mathbf 0$\nfrom which it follows immediately that:\n:$\\mathbf s = \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\n{{qed}}\n\\end{proof}\n\n"}}, "17234": {"score": 0.8169755339622498, "content": {"text": "\\section{Body under Constant Acceleration}\nTags: Physics, Applied Mathematics, Mechanics\n\n\\begin{theorem}\nLet $B$ be a body under constant acceleration $\\mathbf a$.\nThe following equations apply:\n\\end{theorem}\n\n\\begin{proof}\n{{MissingLinks}}\n{{improve|The justification for the derivation of $(1)$ from $(2)$ is handwavey and clumsy. Better to do $(1)$ first and then derive $(2)$ from it, to save having to solve a second order DE whose solution we have not yet put on {{ProofWiki}}. As the separate proofs for the separate parts have now been extracted into their own pages, complete with justifying links throughout (and no handwvery), it is suggested that this proof may be deleted.}}\n$B$ has acceleration $\\mathbf a$.\nLet $\\mathbf x$ be the vector corresponding to the position of $B$ at time $t$.\nThen:\n:$\\dfrac {\\d^2 \\mathbf x} {\\d t^2} = \\mathbf a$\nSolving this differential equation:\n:$\\mathbf x = \\mathbf c_0 + \\mathbf c_1 t + \\frac 1 2 \\mathbf a t^2$\nwith $\\mathbf c_0$ and $\\mathbf c_1$ constant vectors.\nEvaluating $\\mathbf x$ at $t = 0$ shows that $\\mathbf c_0$ is the value $\\mathbf x_0$ of $\\mathbf x$ at time $t=0$.\nTaking the derivative of $\\mathbf x$ at $t = 0$ shows that $\\mathbf c_1$ corresponds to $\\mathbf u$.\nTherefore, since $\\mathbf s = \\mathbf x - \\mathbf x_0$, we have:\n:$\\mathbf s = \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\nand by taking the derivative of $\\mathbf x$, we have:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\nNext, we dot $\\mathbf v$ into itself using the previous statement.\nFrom the linearity and commutativity of the dot product:\n{{begin-eqn}}\n{{eqn | l = \\mathbf v \\cdot \\mathbf v\n      | r = \\paren {\\mathbf u + \\mathbf a t} \\cdot \\paren {\\mathbf u + \\mathbf a t}\n      | c = \n}}\n{{eqn | r = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf u \\cdot \\mathbf a t + t^2 \\mathbf a \\cdot \\mathbf a\n      | c = \n}}\n{{eqn | r = \\mathbf u \\cdot \\mathbf u + \\mathbf a \\cdot \\paren {2 \\mathbf u t + t^2 \\mathbf a}\n      | c = \n}}\n{{end-eqn}}\nThe expression in parentheses is $2 \\mathbf s$, so:\n:$\\mathbf v \\cdot \\mathbf v = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf a \\cdot \\mathbf s$\nand the proof is complete.\n{{qed}}\n\\end{proof}\n\n"}}, "11152": {"score": 0.8159306049346924, "content": {"text": "\\section{Linear First Order ODE/y' = x + y/y(0) = 1}\nTags: Examples of Linear First Order ODEs, Examples of Linear First Order ODE\n\n\\begin{theorem}\nThe linear first order ODE:\n:$(1): \\quad \\dfrac {\\d y} {\\d x} = x + y$\nwith initial condition:\n:$\\map y 0 = 1$\nhas the particular solution:\n:$y = 2 e^x - x - 1$\n\\end{theorem}\n\n\\begin{proof}\nFrom Linear First Order ODE: $y' = x + y$, the general solution of $(1)$ is:\n:$y = C e^x - x - 1$\nSetting $y = 1$ when $x = 0$ gives:\n:$1 = C + 1$\nfrom which $C = 2$.\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "15022": {"score": 0.8311401605606079, "content": {"text": "\\section{Differential Equation governing First-Order Reaction}\nTags: First-Order Reactions\n\n\\begin{theorem}\nLet a substance decompose spontaneously in a '''first-order reaction'''.\nThe differential equation which governs this reaction is given by:\n:$-\\dfrac {\\d x} {\\d t} = k x$\nwhere:\n:$x$ determines the quantity of substance at time $t$.\n:$k \\in \\R_{>0}$.\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of a first-order reaction, the rate of change of the quantity of the substance is proportional to the quantity of the substance present at any time.\nAs the rate of change is a decrease, this rate will be negative.\nThus the differential equation governing this reaction is given by:\n:$-\\dfrac {\\d x} {\\d t} = k x$\nfor some $k \\in \\R_{>0}$.\n{{qed}}\n\\end{proof}\n\n"}}, "21603": {"score": 0.8242102265357971, "content": {"text": "\\section{Solution to First Order Initial Value Problem}\nTags: Ordinary Differential Equations, First Order ODEs\n\n\\begin{theorem}\nLet $\\map y x$ be a solution to the first order ordinary differential equation:\n:$\\dfrac {\\d y} {\\d x} = \\map f {x, y}$\nwhich is subject to an initial condition: $\\tuple {a, b}$.\nThen this problem is equivalent to the integral equation:\n:$\\ds y = b + \\int_a^x \\map f {t, \\map y t} \\rd t$\n\\end{theorem}\n\n\\begin{proof}\nFrom Solution to First Order ODE, the general solution of:\n:$\\dfrac {\\d y} {\\d x} = \\map f {x, y}$\nis:\n:$\\ds y = \\int \\map f {x, \\map y x} \\rd x + C$\nWhen $x = a$, we have $y = b$.\nThus:\n:$\\ds b = \\valueat {\\int \\map f {x, \\map y x} \\rd x + C} a$\n{{MissingLinks|to the notation $[..]_a$}}\nwhich gives:\n:$\\ds C = b - \\valueat {\\int \\map f {x, \\map y x} \\rd x} a$\nand so:\n$\\ds y = b + \\int \\map f {x, \\map y x} \\rd x - \\valueat {\\int \\map f {x, \\map y x} \\rd x} a$\nwhence the result, by the Fundamental Theorem of Calculus.\n{{qed}}\nCategory:First Order ODEs\n\\end{proof}\n\n"}}, "13512": {"score": 0.8315421342849731, "content": {"text": "\\section{First-Order Reaction}\nTags: Decay Equation, Ordinary Differential Equations, First Order ODEs, Physics, Chemistry, First-Order Reactions\n\n\\begin{theorem}\nLet a substance decompose spontaneously in a '''first-order reaction'''.\nLet $x_0$ be a measure of the quantity of that substance at time $t = 0$.\nLet the quantity of the substance that remains after time $t$ be $x$.\nThen:\n:$x = x_0 e^{-k t}$\nwhere $k$ is the rate constant.\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of a first-order reaction, the rate of change of the quantity of the substance is proportional to the quantity of the substance present at any time.\nAs the rate of change is a decrease, this rate will be negative.\nThus the differential equation governing this reaction is given by:\n:$-\\dfrac {\\mathrm d x}{\\mathrm d t} = k x, k > 0$\nThis is an instance of the Decay Equation, and has the solution:\n:$x = x_0 e^{-k t}$\n{{qed}}\n\\end{proof}\n\n"}}, "15336": {"score": 0.8247578144073486, "content": {"text": "\\section{Decay Equation}\nTags: Decay Equation, Examples of Linear First Order ODEs, First Order ODEs, Examples of Separation of Variables, Examples of First Order ODE\n\n\\begin{theorem}\nThe first order ordinary differential equation:\n:$\\dfrac {\\d y} {\\d x} = k \\paren {y_a - y}$\nwhere $k \\in \\R: k > 0$\nhas the general solution:\n:$y = y_a + C e^{-k x}$\nwhere $C$ is an arbitrary constant.\nIf $y = y_0$ at $x = 0$, then:\n:$y = y_a + \\paren {y_0 - y_a} e^{-k x}$\nThis differential equation is known as the '''decay equation'''.\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\frac {\\d y} {\\d x}\n      | r = -k \\paren {y - y_a}\n      | c =\n}}\n{{eqn | ll= \\leadsto\n      | l = \\int \\frac {\\d y} {y - y_a}\n      | r = -\\int k \\rd x\n      | c = Separation of Variables\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map \\ln {y - y_a}\n      | r = -k x + C_1\n      | c = Primitive of Reciprocal and Derivatives of Function of $a x + b$\n}}\n{{eqn | ll= \\leadsto\n      | l = y - y_a\n      | r = e^{-k x + C_1}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = y\n      | r = y_a + C e^{-k x}\n      | c = where we put $C = e^{C_1}$\n}}\n{{end-eqn}}\nThis is our general solution.\n{{qed|lemma}}\nSuppose we have the initial condition:\n:$y = y_0$ when $x = 0$\nThen:\n:$y_0 = y_a + C e^{-k \\cdot 0} = y_a + C$\nand so:\n:$C = y_0 - y_a$\nHence the solution:\n:$y = y_a + \\paren {y_0 - y_a} e^{-k x}$\n{{qed}}\n\\end{proof}\n\n"}}, "17237": {"score": 0.8249652981758118, "content": {"text": "\\section{Body under Constant Acceleration/Velocity after Time}\nTags: Mechanics\n\n\\begin{theorem}\nLet $B$ be a body under constant acceleration $\\mathbf a$.\nThen:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\nwhere:\n:$\\mathbf u$ is the velocity at time $t = 0$\n:$\\mathbf v$ is the velocity at time $t$.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of acceleration:\n: $\\dfrac {\\d \\mathbf v} {\\d t} = \\mathbf a$\nBy Solution to Linear First Order Ordinary Differential Equation:\n:$\\mathbf v = \\mathbf c + \\mathbf a t$\nwhere $\\mathbf c$ is a constant vector.\nWe are given the initial condition:\n:$\\bigvalueat {\\mathbf v} {t \\mathop = 0} = \\mathbf u$\nfrom which it follows immediately that:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\n{{qed}}\nCategory:Mechanics\n\\end{proof}\n\n"}}}}, "TheoremQA_mingyin/series2.json": {"gold": {"8392": 1, "8388": 1}, "retrieved": {"22218": {"score": 0.8931072354316711, "content": {"text": "\\section{Sum of Infinite Geometric Sequence}\nTags: Examples of Power Series, Geometric Sequences, Convergence Tests, Sum of Infinite Geometric Progression, Geometric Progressions, Sum of Geometric Progression, Sum of Infinite Geometric Sequence, Series, Sums of Sequences, Sum of Geometric Sequence\n\n\\begin{theorem}\nLet $S$ be a standard number field, that is $\\Q$, $\\R$ or $\\C$.\nLet $z \\in S$.\nLet $\\size z < 1$, where $\\size z$ denotes:\n:the absolute value of $z$, for real and rational $z$\n:the complex modulus of $z$ for complex $z$.\nThen $\\ds \\sum_{n \\mathop = 0}^\\infty z^n$ converges absolutely to $\\dfrac 1 {1 - z}$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Sum of Geometric Progression, we have:\n: $\\displaystyle s_N = \\sum_{n \\mathop = 0}^N z^n = \\frac {1 - z^{N+1}} {1 - z}$\nWe have that $\\left \\vert {z}\\right \\vert < 1$.\nSo by Power of Number less than One:\n: $z^{N+1} \\to 0$ as $N \\to \\infty$\nHence $s_N \\to \\dfrac 1 {1 - z}$ as $N \\to \\infty$.\nThe result follows.\n{{qed}}\nTo demonstrate absolute convergence we note that the absolute value of $\\left \\vert {z}\\right \\vert$ is just $\\left \\vert {z}\\right \\vert$, and by assumption we have $\\left \\vert {z}\\right \\vert < 1$, so $\\left \\vert {z}\\right \\vert$ fulfils the same condition for convergence as $z$, and we get:\n{{tidy|rewrite in house style}}\n:$\\displaystyle \\sum_{n \\mathop = 0}^\\infty \\left \\vert {z}\\right \\vert^n = \\frac 1 {1 - \\left \\vert {z}\\right \\vert}$.\n{{qed}}\n\\end{proof}\n\n"}}, "8391": {"score": 0.8943057656288147, "content": {"text": "\\begin{definition}[Definition:Series/Number Field]\nLet $S$ be one of the standard number fields $\\R$, or $\\C$.\nLet $\\sequence {a_n}$ be a sequence in $S$.\nThe '''series''' is what results when $\\sequence {a_n}$ is summed to infinity:\n:$\\ds \\sum_{n \\mathop = 1}^\\infty a_n = a_1 + a_2 + a_3 + \\cdots$\n\\end{definition}"}}, "22204": {"score": 0.9030958414077759, "content": {"text": "\\section{Sum of Geometric Sequence/Examples/Index to Minus 2}\nTags: Sum of Geometric Sequence, Sum of Geometric Progression\n\n\\begin{theorem}\nLet $x$ be an element of one of the standard number fields: $\\Q, \\R, \\C$ such that $x \\ne 1$.\nThen the formula for Sum of Geometric Sequence:\n:$\\ds \\sum_{j \\mathop = 0}^n x^j = \\frac {x^{n + 1} - 1} {x - 1}$\nbreaks down when $n = -2$:\n:$\\ds \\sum_{j \\mathop = 0}^{-2} x^j \\ne \\frac {x^{-1} - 1} {x - 1}$\n\\end{theorem}\n\n\\begin{proof}\nThe summation on the {{LHS}} is vacuous:\n:$\\ds \\sum_{j \\mathop = 0}^{-2} x^j = 0$\nwhile on the {{RHS}} we have:\n{{begin-eqn}}\n{{eqn | l = \\frac {x^{\\paren {-2} + 1} - 1} {x - 1}\n      | r = \\frac {x^{-1} - 1} {x - 1}\n      | c = \n}}\n{{eqn | r = \\frac {1 / x - 1} {x - 1}\n      | c = \n}}\n{{eqn | r = \\frac {\\paren {1 - x} / x} {x - 1}\n      | c = \n}}\n{{eqn | r = \\frac {1 - x} {x \\paren {x - 1} }\n      | c = \n}}\n{{eqn | r = -\\frac 1 x\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "8388": {"score": 0.9025366306304932, "content": {"text": "\\begin{definition}[Definition:Series/Complex]\nLet $\\sequence {a_n}$ be a sequence in $\\C$.\nA '''complex series''' $S_n$ is the limit to infinity of the sequence of partial sums of a complex sequence $\\sequence {a_n}$:\n{{begin-eqn}}\n{{eqn | l = S_n\n      | r = \\lim_{N \\mathop \\to \\infty} \\sum_{n \\mathop = 1}^N a_n\n      | c = \n}}\n{{eqn | r = \\sum_{n \\mathop = 1}^\\infty a_n\n      | c = \n}}\n{{eqn | r = a_1 + a_2 + a_3 + \\cdots\n      | c = \n}}\n{{end-eqn}}\n\\end{definition}"}}, "22268": {"score": 0.9022854566574097, "content": {"text": "\\section{Sum of Sequence of Products of 3 Consecutive Reciprocals/Corollary}\nTags: Sum of Sequence of Products of 3 Consecutive Reciprocals, Limit of Series, Limits of Series, Reciprocals\n\n\\begin{theorem}\n:$\\ds \\sum_{j \\mathop = 1}^\\infty \\frac 1 {j \\paren {j + 1} \\paren {j + 2} } = \\frac 1 4$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\sum_{j \\mathop = 1}^\\infty \\frac 1 {j \\paren {j + 1} \\paren {j + 2} }\n      | r = \\lim_{n \\mathop \\to \\infty} \\sum_{j \\mathop = 1}^n \\frac 1 {j \\paren {j + 1} \\paren {j + 2} }\n}}\n{{eqn | r = \\lim_{n \\mathop \\to \\infty} \\frac {n \\paren {n + 3} } {4 \\paren {n + 1} \\paren {n + 2} }\n      | c = Sum of Sequence of Products of 3 Consecutive Reciprocals\n}}\n{{eqn | r = \\lim_{n \\mathop \\to \\infty} \\frac {1 + \\frac 3 n} {4 \\paren {1 + \\frac 1 n} \\paren {1 + \\frac 2 n} }\n      | c = dividing top and bottom by $n^2$\n}}\n{{eqn | r = \\frac 1 4\n      | c = Basic Null Sequences\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Limits of Series\nCategory:Reciprocals\nCategory:Sum of Sequence of Products of 3 Consecutive Reciprocals\n\\end{proof}\n\n"}}, "22599": {"score": 0.9102756977081299, "content": {"text": "\\section{Telescoping Series/Example 1}\nTags: Telescoping Series, Series\n\n\\begin{theorem}\nLet $\\sequence {b_n}$ be a sequence in $\\R$.\nLet $\\sequence {a_n}$ be a sequence whose terms are defined as:\n:$a_k = b_k - b_{k + 1}$\nThen:\n:$\\ds \\sum_{k \\mathop = 1}^n a_k = b_1 - b_{n + 1}$\nIf $\\sequence {b_n}$ converges to zero, then:\n:$\\ds \\sum_{k \\mathop = 1}^\\infty a_k = b_1$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\ds \\sum_{k \\mathop = 1}^n a_k\n      | r = \\sum_{k \\mathop = 1}^n \\paren {b_k - b_{k + 1} }\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n b_k - \\sum_{k \\mathop = 1}^n b_{k + 1}\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n b_k - \\sum_{k \\mathop = 2}^{n + 1} b_k\n      | c = Translation of Index Variable of Summation\n}}\n{{eqn | r = b_1 + \\sum_{k \\mathop = 2}^n b_k - \\sum_{k \\mathop = 2}^n b_k - b_{n + 1}\n      | c = \n}}\n{{eqn | r = b_1 - b_{n + 1}\n      | c = \n}}\n{{end-eqn}}\nIf $\\sequence {b_k}$ converges to zero, then $b_{n + 1} \\to 0$ as $n \\to \\infty$.\nThus:\n:$\\ds \\lim_{n \\mathop \\to \\infty} s_n = b_1 - 0 = b_1$\nSo:\n:$\\ds \\sum_{k \\mathop = 1}^\\infty a_k = b_1$\n{{Qed}}\n\\end{proof}\n\n"}}, "10311": {"score": 0.9048376679420471, "content": {"text": "\\section{Newton-Mercator Series/Examples/2}\nTags: Examples of Power Series, Reciprocals, Reciprocal, Newton-Mercator Series, Logarithms\n\n\\begin{theorem}\nThe Newton-Mercator Series for $x = 1$ converges to the natural logarithm of $2$:\n{{begin-eqn}}\n{{eqn | l = \\sum_{n \\mathop = 1}^\\infty \\frac {\\paren {-1}^\\paren {n - 1} } n\n      | r = 1 - \\frac 1 2 + \\frac 1 3 - \\frac 1 4 + \\dotsb\n      | c = \n}}\n{{eqn | r = \\ln 2\n      | c = \n}}\n{{end-eqn}}\nThis real number is known as Mercator's constant.\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of the Newton-Mercator Series:\n{{begin-eqn}}\n{{eqn | l = \\map \\ln {1 + x}\n      | r = x - \\dfrac {x^2} 2 + \\dfrac {x^3} 3 - \\dfrac {x^4} 4 + \\cdots\n      | c = \n}}\n{{eqn | r = \\sum_{n \\mathop = 1}^\\infty \\frac {\\paren {-1}^{n + 1} } n x^n\n      | c = \n}}\n{{end-eqn}}\nThis is valid for $-1 < x \\le 1$.\nSetting $x = 1$:\n{{begin-eqn}}\n{{eqn | l = \\map \\ln {1 + 1}\n      | r = 1 - \\dfrac {1^2} 2 + \\dfrac {1^3} 3 - \\dfrac {1^4} 4 + \\cdots\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\ln 2\n      | r = 1 - \\dfrac 1 2 + \\dfrac 1 3 - \\dfrac 1 4 + \\dfrac 1 5 - \\cdots\n      | c = \n}}\n{{end-eqn}}\nThe fact that it does indeed converge is shown in Alternating Harmonic Series is Conditionally Convergent.\n{{qed}}\n\\end{proof}\n\n"}}, "3811": {"score": 0.9131635427474976, "content": {"text": "\\begin{definition}[Definition:Grandi's Series]\n'''Grandi's series''' is the divergent series:\n:$\\ds \\sum_{n \\mathop = 0}^\\infty \\paren {-1}^n$\n{{NamedforDef|Luigi Guido Grandi|cat = Grandi}}\nCategory:Definitions/Convergence\nCategory:Definitions/Series\n\\end{definition}"}}, "22217": {"score": 0.9071228504180908, "content": {"text": "\\section{Sum of Infinite Arithmetic-Geometric Sequence}\nTags: Arithmetic-Geometric Sequences, Sums of Sequences\n\n\\begin{theorem}\nLet $\\sequence {a_k}$ be an arithmetic-geometric sequence defined as:\n:$a_k = \\paren {a + k d} r^k$ for $n = 0, 1, 2, \\ldots$\nLet:\n:$\\size r < 1$\nwhere $\\size r$ denotes the absolute value of $r$.\nThen:\n:$\\ds \\sum_{n \\mathop = 0}^\\infty \\paren {a + k d} r^k = \\frac a {1 - r} + \\frac {r d} {\\paren {1 - r}^2}$\n\\end{theorem}\n\n\\begin{proof}\nFrom Sum of Arithmetic-Geometric Sequence, we have:\n:$\\ds s_n = \\sum_{k \\mathop = 0}^{n - 1} \\paren {a + k d} r^k = \\frac {a \\paren {1 - r^n} } {1 - r} + \\frac {r d \\paren {1 - n r^{n - 1} + \\paren {n - 1} r^n} } {\\paren {1 - r}^2}$\nWe have that $\\size r < 1$.\nSo by Sequence of Powers of Number less than One:\n:$r^n \\to 0$ as $n \\to \\infty$\nand\n:$r^{n - 1} \\to 0$ as $n - 1 \\to \\infty$\nHence:\n:$s_n \\to \\dfrac a {1 - r} + \\dfrac {r d} {\\paren {1 - r}^2}$\nas $n \\to \\infty$.\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "8392": {"score": 0.9080844521522522, "content": {"text": "\\begin{definition}[Definition:Series/Real]\nA '''real series''' $S_n$ is the limit to infinity of the sequence of partial sums of a real sequence $\\sequence {a_n}$:\n{{begin-eqn}}\n{{eqn | l = S_n\n      | r = \\lim_{N \\mathop \\to \\infty} \\sum_{n \\mathop = 1}^N a_n\n      | c = \n}}\n{{eqn | r = \\sum_{n \\mathop = 1}^\\infty a_n\n      | c = \n}}\n{{eqn | r = a_1 + a_2 + a_3 + \\cdots\n      | c = \n}}\n{{end-eqn}}\n\\end{definition}"}}}}, "TheoremQA_panlu/rigid-body3.json": {"gold": {"15865": 1, "230": 1}, "retrieved": {"17236": {"score": 0.7205369472503662, "content": {"text": "\\section{Body under Constant Acceleration/Velocity after Distance}\nTags: Mechanics\n\n\\begin{theorem}\nLet $B$ be a body under constant acceleration $\\mathbf a$.\nThen:\n:$\\mathbf v \\cdot \\mathbf v = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf a \\cdot \\mathbf s$\nwhere:\n:$\\mathbf v$ is the velocity at time $t$\n:$\\mathbf u$ is the velocity at time $t = 0$\n:$\\mathbf s$ is the displacement of $B$ from its initial position at time $t$\n:$\\cdot$ denotes the dot product.\n\\end{theorem}\n\n\\begin{proof}\nFrom Body under Constant Acceleration: Velocity after Time\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\mathbf v \\cdot \\mathbf v\n      | r = \\paren {\\mathbf u + \\mathbf a t} \\cdot \\paren {\\mathbf u + \\mathbf a t}\n      | c = \n}}\n{{eqn | r = \\mathbf u \\cdot \\mathbf u + \\mathbf u \\cdot \\mathbf a t + \\mathbf a t \\cdot \\mathbf u + \\paren {\\mathbf a t} \\cdot \\paren {\\mathbf a t}\n      | c = Dot Product Distributes over Addition\n}}\n{{eqn | r = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf u \\cdot \\mathbf a t + \\paren {\\mathbf a t} \\cdot \\paren {\\mathbf a t}\n      | c = Dot Product Operator is Commutative\n}}\n{{eqn | r = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf u \\cdot \\mathbf a t + \\mathbf a \\cdot \\mathbf a t^2\n      | c = Dot Product Associates with Scalar Multiplication\n}}\n{{eqn | n = 1\n      | r = \\mathbf u \\cdot \\mathbf u + \\mathbf a \\cdot \\paren {2 \\mathbf u t + \\mathbf a t^2}\n      | c = Dot Product Distributes over Addition\n}}\n{{end-eqn}}\nFrom Body under Constant Acceleration: Distance after Time:\n:$\\mathbf s = \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\nSubstituting for $\\mathbf s$ in $(1)$ gives:\n:$\\mathbf v \\cdot \\mathbf v = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf a \\cdot \\mathbf s$\nand the proof is complete.\n{{qed}}\nCategory:Mechanics\n\\end{proof}\n\n"}}, "10458": {"score": 0.7223100066184998, "content": {"text": "\\section{Motion of Particle in Polar Coordinates}\nTags: Definitions: Mathematical Physics, Mathematical Physics\n\n\\begin{theorem}\nConsider a particle $p$ of mass $m$ moving in the plane under the influence of a force $\\mathbf F$.\nLet the position of $p$ at time $t$ be given in polar coordinates as $\\polar {r, \\theta}$.\nLet $\\mathbf F$ be expressed as:\n:$\\mathbf F = F_r \\mathbf u_r + F_\\theta \\mathbf u_\\theta$\nwhere:\n:$\\mathbf u_r$ is the unit vector in the direction of the radial coordinate of $p$\n:$\\mathbf u_\\theta$ is the unit vector in the direction of the angular coordinate of $p$\n:$F_r$ and $F_\\theta$ are the magnitudes of the components of $\\mathbf F$ in the directions of $\\mathbf u_r$ and $\\mathbf u_\\theta$ respectively.\nThen the second order ordinary differential equations governing the motion of $m$ under the force $\\mathbf F$ are:\n{{begin-eqn}}\n{{eqn | l = F_\\theta\n      | r = m \\paren {r \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t} }\n      | c =\n}}\n{{eqn | l = F_r\n      | r = m \\paren {\\dfrac {\\d^2 r} {\\d t^2} - r \\paren {\\dfrac {\\d \\theta} {\\d t} }^2}\n      | c =\n}}\n{{end-eqn}}\n\\end{theorem}\n\n\\begin{proof}\nLet the radius vector $\\mathbf r$ from the origin to $p$ be expressed as:\n:$(1): \\quad \\mathbf r = r \\mathbf u_r$\nFrom Velocity Vector in Polar Coordinates, the velocity $\\mathbf v$ of $p$ can be expressed in vector form as:\n:$\\mathbf v = r \\dfrac {\\d \\theta} {\\d t} \\mathbf u_\\theta + \\dfrac {\\d r} {\\d t} \\mathbf u_r$\nFrom Acceleration Vector in Polar Coordinates, the the acceleration $\\mathbf a$ of $p$ can be expressed as:\n:$\\mathbf a = \\paren {r \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t} } \\mathbf u_\\theta + \\paren {\\dfrac {\\d^2 r} {\\d t^2} - r \\paren {\\dfrac {\\d \\theta} {\\d t} }^2} \\mathbf u_r$\nWe have:\n:$\\mathbf F = F_r \\mathbf u_r + F_\\theta \\mathbf u_\\theta$\nand from Newton's Second Law of Motion:\n:$\\mathbf F = m \\mathbf a$\nHence:\n:$\\mathbf F = m \\paren {r \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t} } \\mathbf u_\\theta + m \\paren {\\dfrac {\\d^2 r} {\\d t^2} - r \\paren {\\dfrac {\\d \\theta} {\\d t} }^2} \\mathbf u_r$\nEquating components:\n{{begin-eqn}}\n{{eqn | l = F_r \\mathbf u_r\n      | r = m \\paren {\\dfrac {\\d^2 r} {\\d t^2} - r \\paren {\\dfrac {\\d \\theta} {\\d t} }^2} \\mathbf u_r\n      | c =\n}}\n{{eqn | l = F_\\theta \\mathbf u_\\theta\n      | r = m \\paren {r \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t} } \\mathbf u_\\theta\n      | c =\n}}\n{{end-eqn}}\nHence the result:\n{{begin-eqn}}\n{{eqn | l = F_\\theta\n      | r = m \\paren {r \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t} }\n      | c =\n}}\n{{eqn | l = F_r\n      | r = m \\paren {\\dfrac {\\d^2 r} {\\d t^2} - r \\paren {\\dfrac {\\d \\theta} {\\d t} }^2}\n      | c =\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "5181": {"score": 0.7356286644935608, "content": {"text": "\\begin{definition}[Definition:Linear Momentum]\nThe '''linear momentum''' of a body is its mass multiplied by its velocity.\n:$\\mathbf p = m \\mathbf v$\nAs mass is a scalar quantity and velocity is a vector quantity, it follows that linear momentum is a vector quantity.\n\\end{definition}"}}, "10157": {"score": 0.7332085371017456, "content": {"text": "\\begin{definition}[Definition:Work]\nLet $P$ be a particle whose position vector at time $t$ is $\\mathbf r$.\nLet a force applied to $P$ be represented by the vector $\\mathbf F$.\nSuppose that, during the time interval $\\delta t$, $P$ moves from $\\mathbf r$ to $\\mathbf r + \\delta \\mathbf r$.\nThe '''work done''' by $\\mathbf F$ during $\\delta t$ is defined to be:\n:$\\delta W = \\mathbf F \\cdot \\delta \\mathbf r$\nwhere $\\cdot$ denotes the dot product.\n\\end{definition}"}}, "4373": {"score": 0.7312150001525879, "content": {"text": "\\begin{definition}[Definition:Inertia]\n'''Inertia''' is the tendency of a body to maintain the same velocity in the absence of an external force, in accordance with Newton's First Law of Motion.\nEquivalently put, '''inertia''' is the resistance of a body to a change in its motion.\n'''Inertia''' is equivalent to mass.\n\\end{definition}"}}, "230": {"score": 0.8074324131011963, "content": {"text": "\\begin{definition}[Definition:Angular Momentum]\nThe '''angular momentum''' of a body about a point $P$ is its moment of inertia about $P$ multiplied by its angular velocity about $P$.\nAngular momentum is a vector quantity.\n{{expand|Separate out into orbital angular momentum and spin angular momentum.}}\n\\end{definition}"}}, "10457": {"score": 0.7364344000816345, "content": {"text": "\\section{Motion of Body with Variable Mass}\nTags: Dynamics\n\n\\begin{theorem}\nLet $B$ be a body undergoing a force $\\mathbf F$.\nLet $B$ be travelling at a velocity $\\mathbf v$ at time $t$.\nLet mass travelling at a velocity $\\mathbf v + \\mathbf w$ be added to $B$ at a rate of $\\dfrac {\\d m} {\\d t}$.\nLet $m$ be the mass of $B$ at time $t$.\nThen the equation of motion of $B$ is given by:\n:$\\mathbf w \\dfrac {\\d m} {\\d t} + \\mathbf F = m \\dfrac {\\d \\mathbf v} {\\d t}$\n\\end{theorem}\n\n\\begin{proof}\nFrom Newton's Second Law of Motion:\n:$\\mathbf F = \\map {\\dfrac \\d {\\d t} } {m \\mathbf v}$\nThen the added momentum being added to $B$ by the mass being added to it is given by:\n:$\\paren {\\mathbf v + \\mathbf w} \\dfrac {\\d m} {\\d t}$\nHence:\n{{begin-eqn}}\n{{eqn | l = \\paren {\\mathbf v + \\mathbf w} \\dfrac {\\d m} {\\d t} + \\mathbf F\n      | r = \\map {\\dfrac \\d {\\d t} } {m \\mathbf v}\n      | c = \n}}\n{{eqn | r = m \\dfrac {\\d \\mathbf v} {\\d t} + \\mathbf v \\dfrac {\\d m} {\\d t}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\mathbf w \\dfrac {\\d m} {\\d t} + \\mathbf F\n      | r = m \\dfrac {\\d \\mathbf v} {\\d t}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "15865": {"score": 0.8334255218505859, "content": {"text": "\\section{Conservation of Angular Momentum}\nTags: Classical Mechanics\n\n\\begin{theorem}\nNewton's Laws of Motion imply the conservation of angular momentum in systems of masses in which no external force is acting.\n\\end{theorem}\n\n\\begin{proof}\nWe start by stating Newton's Third Law of Motion in all its detail.\nWe consider a collection of massive bodies denoted by the subscripts $1$ to $N$.\nThese bodies interact with each other and exert forces on each other and these forces occur in equal and opposite pairs.\nThe force $F_{i j}$ exerted by body $i$ on body $j$ is related to the force exerted by body $j$ on body $i$ by:\n:$(1): \\quad \\vec {F_{i j}} = -\\vec {F_{j i}}$\nThe final part of Newton's Third Law of Motion is that these equal and opposite forces act through the line that connects the two bodies in question.\nThis can be stated thus:\n:$(2): \\quad \\vec {F_{i j}} = a_{i j} \\paren {\\vec {r_j} - \\vec {r_i} }$\nwhere:\n:$\\vec{r_i}$ is the position of body $i$\n:$a_{i j} $ is the magnitude of the force.\nAs a consequence of $(1)$:\n:$a_{j i} = a_{i j}$\nLet the total torque $\\vec {\\tau_{\\operatorname {total} } }$ on the system be measured about an origin located at $\\vec {r_0}$.\nThus:\n{{begin-eqn}}\n{{eqn | l = \\vec {\\tau_{\\operatorname {total} } }\n      | r = \\sum_i \\vec{\\tau_i}\n      | c = \n}}\n{{eqn | r = \\sum_i \\paren {\\paren {\\vec {r_i} - \\vec {r_0} } \\times \\sum_j \\vec {F_{j i} } }\n      | c = \n}}\n{{eqn | r = \\sum_{i, j} \\paren {\\paren {\\paren {\\vec {r_j} - \\vec {r_0} } - \\paren {\\vec {r_j} - \\vec {r_i} } } \\times \\vec {F_{j i} } }\n      | c = \n}}\n{{eqn | r = \\sum_{i, j} \\paren {\\paren {\\paren {\\vec {r_j} - \\vec {r_0} } - \\paren {\\vec {r_j} - \\vec {r_i} } } \\times a_{i j} \\paren {\\vec {r_j} - \\vec {r_i} } }\n      | c = \n}}\n{{eqn | r = \\sum_{i, j} \\paren {\\vec {r_j} - \\vec {r_0} } \\times  a_{i j} \\paren {\\vec {r_j} - \\vec {r_i} }\n      | c = \n}}\n{{eqn | r = \\sum_{i, j} \\vec {r_j} \\times  a_{i j} \\paren {\\vec {r_j} - \\vec {r_i} } - \\paren {\\vec {r_0} \\times \\sum_{i, j} \\vec {F_{i j} } }\n      | c = \n}}\n{{end-eqn}}\nBy hypothesis there is no external force.\nThus the second term disappears, and:\n{{begin-eqn}}\n{{eqn | l = \\vec {\\tau_{\\operatorname {total} } }\n      | r = \\sum_{i, j} \\vec {r_j} \\times a_{i j} \\paren {\\vec {r_j} - \\vec {r_i} }\n      | c = \n}}\n{{eqn | r = \\sum_{i \\mathop > j} \\vec {r_j} \\times a_{i j} \\paren {\\vec {r_j} - \\vec {r_i} } + \\vec {r_i} \\times  a_{j i} \\paren {\\vec {r_j} - \\vec {r_i} }\n      | c = \n}}\n{{eqn | r = \\sum_{i \\mathop > j} \\vec {r_j} \\times \\vec {r_i} \\paren {a_{j i} - a_{i j} }\n      | c = \n}}\n{{eqn | r = \\vec 0\n      | c = \n}}\n{{end-eqn}}\nIn summary, in a system of masses in which there is no external force, the total torque on the system is equal to $0$.\nThis is because the pair of torque between two bodies must cancel out.\nSince the rate of change of angular momentum is proportional to the torque, the angular momentum of a system is conserved when no external force is applied.\n{{qed}}\nCategory:Classical Mechanics\n\\end{proof}\n\n"}}, "15200": {"score": 0.7627326250076294, "content": {"text": "\\section{Derivative of Angular Component under Central Force}\nTags: Mechanics\n\n\\begin{theorem}\nLet a point mass $p$ of mass $m$ be under the influence of a central force $\\mathbf F$.\nLet the position of $p$ at time $t$ be given in polar coordinates as $\\polar {r, \\theta}$.\nLet $\\mathbf r$ be the radius vector from the origin to $p$.\nThen the rate of change of the angular coordinate of $p$ is inversely proportional to the square of the radial coordinate of $p$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\mathbf F$ be expressed as:\n:$\\mathbf F = F_r \\mathbf u_r + F_\\theta \\mathbf u_\\theta$\nwhere:\n:$\\mathbf u_r$ is the unit vector in the direction of the radial coordinate of $p$\n:$\\mathbf u_\\theta$ is the unit vector in the direction of the angular coordinate of $p$\n:$F_r$ and $F_\\theta$ are the magnitudes of the components of $\\mathbf F$ in the directions of $\\mathbf u_r$ and $\\mathbf u_\\theta$ respectively.\nFrom Motion of Particle in Polar Coordinates, the second order ordinary differential equations governing the motion of $m$ under the force $\\mathbf F$ are:\n{{begin-eqn}}\n{{eqn | l = F_\\theta\n      | r = m \\paren {r \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t} }\n      | c = \n}}\n{{eqn | l = F_r\n      | r = m \\paren {\\dfrac {\\d^2 r} {\\d t^2} - r \\paren {\\dfrac {\\d \\theta} {\\d t} }^2}\n      | c = \n}}\n{{end-eqn}}\nHowever, we are given that $\\mathbf F$ is a central force.\n:600px\nThus:\n{{begin-eqn}}\n{{eqn | l = F_\\theta\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = r \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t}\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = r^2 \\dfrac {\\d^2 \\theta} {\\d t^2} + 2 r \\dfrac {\\d r} {\\d t} \\dfrac {\\d \\theta} {\\d t}\n      | r = 0\n      | c = multiplying through by $r$\n}}\n{{eqn | ll= \\leadsto\n      | l = \\dfrac \\d {\\d t} \\paren {r^2 \\dfrac {\\d \\theta} {\\d t} }\n      | r = 0\n      | c = Product Rule for Derivatives\n}}\n{{eqn | ll= \\leadsto\n      | l = r^2 \\dfrac {\\d \\theta} {\\d t}\n      | r = h\n      | c = Derivative of Constant\n}}\n{{end-eqn}}\nfor some constant $h$.\nThat is:\n:$\\dfrac {\\d \\theta} {\\d t} = \\dfrac h {r^2}$\nHence the result, by definition of inverse proportion.\n{{qed}}\n\\end{proof}\n\n"}}, "231": {"score": 0.7791560292243958, "content": {"text": "\\begin{definition}[Definition:Angular Momentum/Dimension]\nThe dimension of measurement of '''angular momentum''' is $\\mathsf {M L}^2 \\mathsf T^{-1}$.\nCategory:Definitions/Dimensions of Measurement\nCategory:Definitions/Angular Momentum\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/trapezoidal_rule2.json": {"gold": {"17471": 1}, "retrieved": {"19990": {"score": 0.8186241388320923, "content": {"text": "\\section{Quadrature of Parabola}\nTags: Parabolas, Euclidean Geometry\n\n\\begin{theorem}\nLet $T$ be a parabola.\nConsider the parabolic segment bounded by an arbitrary chord $AB$.\nLet $C$ be the point on $T$ where the tangent to $T$ is parallel to $AB$.\nLet\nThen the area $S$ of the parabolic segment $ABC$ of $T$ is given by:\n:$S = \\dfrac 4 3 \\triangle ABC$\n\\end{theorem}\n\n\\begin{proof}\n{{WLOG}}, consider the parabola $y = a x^2$.\nLet $A, B, C$ be the points:\n{{begin-eqn}}\n{{eqn | l = A\n      | r = \\tuple {x_0, a {x_0}^2}\n}}\n{{eqn | l = B\n      | r = \\tuple {x_2, a {x_2}^2}\n}}\n{{eqn | l = C\n      | r = \\tuple {x_1, a {x_1}^2}\n}}\n{{end-eqn}}\n:500px\nThe slope of the tangent at $C$ is given by using:\n:$\\dfrac {\\d y} {\\d x} 2 a x_1$\nwhich is parallel to $AB$.\nThus:\n:$2 a x_1 = \\dfrac {a {x_0}^2 - a {x_2}^2} {x_0 - x_2}$\nwhich leads to:\n:$x_1 = \\dfrac {x_0 + x_2} 2$\nSo the vertical line through $C$ is a bisector of $AB$, at point $P$.\nComplete the parallelogram $CPBQ$.\nAlso, find $E$ which is the point where the tangent to $T$ is parallel to $BC$.\nBy the same reasoning, the vertical line through $E$ is a bisector of $BC$, and so it also bisects $BP$ at $H$.\nNext:\n{{begin-eqn}}     \n{{eqn | l = EF\n      | r = a \\paren {\\frac {x_1 + x_2} 2}^2 - \\paren {a x_1^2 + 2 a x_1 \\frac {x_2 - x_1} 2}\n      | c = \n}}\n{{eqn | r = \\frac a 4 \\paren {\\paren {x_1 + x_2}^2 - 4 {x_1}^2 + 4 x_1 \\paren {x_2 - x_1} }\n      | c = \n}}\n{{eqn | r = \\frac a 4 \\paren { {x_1}^2 - 2 x_1 x_2 + {x_2}^2}\n      | c = \n}}\n{{eqn | r = \\frac a 4 \\paren {x_2 - x_1}^2\n      | c = \n}}\n{{end-eqn}}\nAt the same time:\n{{begin-eqn}}     \n{{eqn | l = QB\n      | r = a {x_2}^2 - \\paren {a {x_1}^2 + 2 a x_1 \\paren {x_2 - x_2} }\n      | c = \n}}\n{{eqn | r = a \\paren { {x_1}^2 - 2 x_1 x_2 + {x_2}^2}\n      | c = \n}}\n{{eqn | r = a \\paren {x_2 - x_1}^2\n      | c = \n}}\n{{end-eqn}}\nSo:\n:$QB = 4 FE = FH$\nand because $CB$ is the diagonal of a parallelogram:\n:$2 FE = 2 EG = FG$\nThis implies that:\n:$2 \\triangle BEG = \\triangle BGH$\nand:\n:$2 \\triangle CEG = \\triangle BGH$\nSo:\n:$\\triangle BCE = \\triangle BGH$\nand so as $\\triangle BCP = 4 \\triangle BGH$ we have that:\n:$BCE = \\dfrac {\\triangle BCP} 4$\nA similar relation holds for $\\triangle APC$:\n:500px\nso it can be seen that:\n:$\\triangle ABC = 4 \\paren {\\triangle ADC + \\triangle CEB}$\nSimilarly, we can create four more triangles underneath $\\triangle ADC$ and $\\triangle CEB$ which are $\\dfrac 1 4$ the area of those combined, or $\\dfrac 1 {4^2} \\triangle ABC$.\nThis process can continue indefinitely.\nSo the area $S$ is given as:\n:$S = \\triangle ABC \\paren {1 + \\dfrac 1 4 + \\dfrac 1 {4^2} + \\cdots}$\nBut from Sum of Geometric Sequence it follows that:\n:$S = \\triangle ABC \\paren {\\dfrac 1 {1 - \\dfrac 1 4} } = \\dfrac 4 3 \\triangle ABC$\n{{qed}}\n\\end{proof}\n\n"}}, "11524": {"score": 0.8199523687362671, "content": {"text": "\\section{Largest Rectangle with Given Perimeter is Square}\nTags: Squares, Rectangles\n\n\\begin{theorem}\nLet $\\SS$ be the set of all rectangles with a given perimeter $L$.\nThe element of $\\SS$ with the largest area is the square with length of side $\\dfrac L 4$.\n\\end{theorem}\n\n\\begin{proof}\nConsider an arbitrary element $R$ of $\\SS$.\nLet $B$ be half the perimeter of $R$.\nLet $x$ be the length of one side of $R$.\nThen the length of an adjacent side is $B - x$.\nThe area $\\AA$ of $R$ is then given by:\n:$\\AA = x \\paren {B - x}$\nLet $\\AA$ be expressed in functional notation as:\n:$\\map f x = x \\paren {B - x}$\nWe have:\n{{begin-eqn}}\n{{eqn | l = \\map f {x + E} - \\map f x\n      | r = \\paren {x + E} \\paren {B - \\paren {x + E} } - x \\paren {B - x}\n      | c = \n}}\n{{eqn | r = E B - 2 E x - E^2\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac {\\map f {x + E} - \\map f x} E\n      | r = B - 2 x - E\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\intlimits {\\frac {\\map f {x + E} - \\map f x} E} {E \\mathop = 0} {}\n      | r = B - 2 x\n      | c = \n}}\n{{end-eqn}}\nThus from Derivative at Maximum or Minimum, when $B - 2 x = 0$, the area of $R$ is at a maximum or a minimum.\nThat is:\n:$x = \\dfrac B 2 = \\dfrac L 4$\nGeometrical analysis shows that for this length of side the area is not a minimum because that happens when $x = 0$ or $x = B$.\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "10384": {"score": 0.8252227902412415, "content": {"text": "\\section{Multiplication using Parabola}\nTags: Multiplication, Parabolas, Algebra, Quadratics\n\n\\begin{theorem}\n:500pxrightthumb\nLet the parabola $P$ defined as $y = x^2$ be plotted on the Cartesian plane.\nLet $A = \\tuple {x_a, y_a}$ and $B = \\tuple {x_b, y_b}$ be points on the curve $\\map f x$ so that $x_a < x_b$.\nThen the line segment joining $A B$ will cross the $y$-axis at $-x_a x_b$.\nThus $P$ can be used as a nomogram to calculate the product of two numbers $x_a$ and $x_b$, as follows:\n:$(1) \\quad$ Find the points $-x_a$ and $x_b$ on the $x$-axis.\n:$(2) \\quad$ Find the points $A$ and $B$ where the lines $x = -x_a$ and $x = x_b$ cut $P$.\n:$(3) \\quad$ Lay a straightedge on the straight line joining $A$ and $B$ and locate its $y$-intercept $c$.\nThen $x_a x_b$ can be read off from the $y$-axis as the position of $c$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map f x = x^2$.\nThen:\n:$\\map f {x_a} = x_a^2$\nand:\n:$\\map f {x_b} = x_b^2$\nThen the slope $m$ of the line segment joining $A B$ will be:\n{{begin-eqn}}\n{{eqn | l = m \n      | r = \\frac {x_b^2 - x_a^2} {x_b - x_a}\n      | c = Equation of Straight Line in Plane: Point-Slope Form\n}}\n{{eqn | r = \\frac {\\paren {x_b - x_a} \\paren {x_b + x_a} } {x_b - x_a}\n      | c = Difference of Two Squares\n}}\n{{eqn | r = x_b + x_a\n      | c = cancelling, $x_a \\ne x_b$\n}}\n{{end-eqn}}\nFrom Equation of Straight Line in Plane: Slope-Intercept Form:\n:$y = \\paren {x_b + x_a} x + c$\nwhere $c$ denotes the $y$-intercept.\nSubstituting the coordinates of point $A = \\tuple {x_a, x_a^2}$ for $\\tuple {x, y}$:\n{{begin-eqn}}\n{{eqn | l = x_a^2 \n      | r = \\paren {x_b + x_a} x_a + c\n}}\n{{eqn | ll= \\leadsto\n      | l = c\n      | r = x_a^2 - \\paren {x_a + x_b} x_a\n}}\n{{eqn | r = x_a^2 - x_a^2 - x_b x_a\n}}\n{{eqn | r = -x_b x_a\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "5561": {"score": 0.8217863440513611, "content": {"text": "\\begin{definition}[Definition:Mean Square]\nLet $S$ be a set of numbers.\nThe '''mean square''' of $S$ is the arithmetic mean of the squares of the elements of $S$:\n:$s^2 = \\dfrac 1 n \\ds \\sum_{i \\mathop = 1}^n {x_i}^2$\nwhere $S = \\set {x_1, x_2, \\ldots, x_n}$.\n\\end{definition}"}}, "12865": {"score": 0.8201655149459839, "content": {"text": "\\section{Harmonic Mean of two Real Numbers is Between them}\nTags: Harmonic Mean\n\n\\begin{theorem}\nLet $a, b \\in \\R_{\\ne 0}$ be non-zero real numbers such that $a < b$.\nLet $\\map H {a, b}$ denote the narmonic mean of $a$ and $b$.\nThen:\n:$a < \\map H {a, b} < b$\n\\end{theorem}\n\n\\begin{proof}\nBy definition of harmonic mean:\n:$\\dfrac 1 {\\map H {a, b} } := \\dfrac 1 2 \\paren {\\dfrac 1 a + \\dfrac 1 b}$\nThus:\n{{begin-eqn}}\n{{eqn | l = a\n      | o = <\n      | r = b\n      | c = by assumption\n}}\n{{eqn | ll= \\leadsto\n      | l = \\dfrac 1 b\n      | o = <\n      | r = \\dfrac 1 a\n      | c = Reciprocal Function is Strictly Decreasing\n}}\n{{end-eqn}}\nBut $\\dfrac 1 {\\map H {a, b} }$ is the arithmetic mean of $\\dfrac 1 b$ and $\\dfrac 1 a$.\nHence from Arithmetic Mean of two Real Numbers is Between them:\n:$\\dfrac 1 b < \\dfrac 1 {\\map H {a, b} } < \\dfrac 1 a$\nSo by Reciprocal Function is Strictly Decreasing:\n:$b > \\map H {a, b} > a$\nHence the result.\n{{qed}}\nCategory:Harmonic Mean\n\\end{proof}\n\n"}}, "433": {"score": 0.8368417620658875, "content": {"text": "\\begin{definition}[Definition:Average Value of Function]\nLet $f$ be an integrable function on some closed interval $\\closedint a b$.\nThe '''average value of $f$''' (or '''mean value of $f$''') on $\\closedint a b$ is defined as:\n:$\\ds \\frac 1 {b - a} \\int_a^b \\map f x \\rd x$\n\\end{definition}"}}, "13047": {"score": 0.8264116644859314, "content": {"text": "\\section{Geometric Mean of two Positive Real Numbers is Between them}\nTags: Geometric Mean\n\n\\begin{theorem}\nLet $a, b \\in \\R$ be real numbers such that $0 < a < b$.\nLet $\\map G {a, b}$ denote the geometric mean of $a$ and $b$.\nThen:\n:$a < \\map G {a, b} < b$\n\\end{theorem}\n\n\\begin{proof}\nBy definition of geometric mean:\n:$\\map G {a, b} := \\sqrt {a b}$\nwhere $\\sqrt {a b}$ specifically denotes the positive square root of $a$ and $b$.\nThus:\n{{begin-eqn}}\n{{eqn | l = a\n      | o = <\n      | r = b\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = a^2\n      | o = <\n      | r = a b\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = a\n      | o = <\n      | r = \\sqrt {a b}\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = a\n      | o = <\n      | r = b\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = a b\n      | o = <\n      | r = b^2\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\sqrt {a b}\n      | o = <\n      | r = b\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Geometric Mean\n\\end{proof}\n\n"}}, "343": {"score": 0.8397344350814819, "content": {"text": "\\begin{definition}[Definition:Arithmetic Mean]\nLet $x_1, x_2, \\ldots, x_n \\in \\R$ be real numbers.\nThe '''arithmetic mean''' of $x_1, x_2, \\ldots, x_n$ is defined as:\n:$\\ds A_n := \\dfrac 1 n \\sum_{k \\mathop = 1}^n x_k$\nThat is, to find out the '''arithmetic mean''' of a set of numbers, add them all up and divide by how many there are.\n\\end{definition}"}}, "4000": {"score": 0.8323749899864197, "content": {"text": "\\begin{definition}[Definition:Heronian Mean]\nThe '''Heronian mean''' of two numbers $x$ and $y$ is defined as:\n:$H = \\dfrac {x + \\sqrt {x y} + y} 3$\nIt can also be defined as:\n:$H = \\dfrac 2 3 \\paren {\\dfrac {x + y} 2} + \\dfrac 1 3 \\sqrt {x y}$\nThus it is seen to be a weighted mean of their arithmetic mean and geometric mean.\n{{NamedforDef|Heron of Alexandria|cat = Heron}}\nCategory:Definitions/Algebra\n\\end{definition}"}}, "17379": {"score": 0.8341356515884399, "content": {"text": "\\section{Arithmetic Mean of two Real Numbers is Between them}\nTags: Arithmetic Mean\n\n\\begin{theorem}\nLet $a, b \\in \\R_{\\ne 0}$ be non-zero real numbers such that $a < b$.\nLet $\\map A {a, b}$ denote the narmonic mean of $a$ and $b$.\nThen:\n:$a < \\map A {a, b} < b$\n\\end{theorem}\n\n\\begin{proof}\nBy definition of arithmetic mean:\n:$\\map A {a, b} := \\dfrac {a + b} 2$\nThus:\n{{begin-eqn}}\n{{eqn | l = a\n      | o = <\n      | r = b\n      | c = by assumption\n}}\n{{eqn | ll= \\leadsto\n      | l = 2 a\n      | o = <\n      | r = a + b\n      | c = adding $a$ to both sides\n}}\n{{eqn | ll= \\leadsto\n      | l = a\n      | o = <\n      | r = \\dfrac {a + b} 2\n      | c = dividing both sides by $2$\n}}\n{{eqn | r = \\map A {a, b}\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = b\n      | o = >\n      | r = a\n      | c = by assumption\n}}\n{{eqn | ll= \\leadsto\n      | l = 2 b\n      | o = >\n      | r = a + b\n      | c = adding $b$ to both sides\n}}\n{{eqn | ll= \\leadsto\n      | l = b\n      | o = >\n      | r = \\dfrac {a + b} 2\n      | c = dividing both sides by $2$\n}}\n{{eqn | r = \\map A {a, b}\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\nCategory:Arithmetic Mean\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/t_test2.json": {"gold": {"9320": 1, "4138": 1}, "retrieved": {"20820": {"score": 0.7218647003173828, "content": {"text": "\\section{Sample Mean is Unbiased Estimator of Population Mean}\nTags: Inductive Statistics, Descriptive Statistics\n\n\\begin{theorem}\nLet $X_1, X_2, \\ldots, X_n$ form a random sample from a population with mean $\\mu$ and variance $\\sigma^2$.\nThen: \n:$\\ds \\bar X = \\frac 1 n \\sum_{i \\mathop = 1}^n X_i$\nis an unbiased estimator of $\\mu$.\n\\end{theorem}\n\n\\begin{proof}\nIf $\\bar X$ is an unbiased estimator of $\\mu$, then: \n:$\\ds \\expect {\\bar X} = \\mu$\nWe have: \n{{begin-eqn}}\n{{eqn\t| l = \\expect {\\bar X}\n\t| r = \\expect {\\frac 1 n \\sum_{i \\mathop = 1}^n X_i}\n}}\n{{eqn\t| r = \\frac 1 n \\sum_{i \\mathop = 1}^n \\expect {X_i}\n\t| c = Linearity of Expectation Function\n}}\n{{eqn\t| r = \\frac 1 n \\sum_{i \\mathop = 1}^n \\mu\n\t| c = as $\\expect {X_i} = \\mu$\n}}\n{{eqn\t| r = \\frac n n \\mu\n\t| c = as $\\ds \\sum_{i \\mathop = 1}^n 1 = n$\n}}\n{{eqn\t| r = \\mu\n}}\n{{end-eqn}}\nSo $\\bar X$ is an unbiased estimator of $\\mu$. \n{{qed}}\nCategory:Inductive Statistics\n\\end{proof}\n\n"}}, "23292": {"score": 0.7241344451904297, "content": {"text": "\\section{Variance of Sample Mean}\nTags: Inductive Statistics, Variance\n\n\\begin{theorem}\nLet $X_1, X_2, \\ldots, X_n$ form a random sample from a population with mean $\\mu$ and variance $\\sigma^2$.\nLet: \n:$\\ds \\overline X = \\frac 1 n \\sum_{i \\mathop = 1}^n X_i$\nThen: \n:$\\var {\\overline X} = \\dfrac {\\sigma^2} n$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn\t| l = \\var {\\overline X}\n\t| r = \\var {\\frac 1 n \\sum_{i \\mathop = 1}^n X_i}\n}}\n{{eqn\t| r = \\frac 1 {n^2} \\sum_{i \\mathop = 1}^n \\var {X_i}\n\t| c = repeated application of Variance of Linear Combination of Random Variables: Corollary\n}}\n{{eqn\t| r = \\frac 1 {n^2} \\sum_{i \\mathop = 1}^n \\sigma^2\n}}\n{{eqn\t| r = \\frac {\\sigma^2 n} {n^2}\n\t| c = as $\\ds \\sum_{i \\mathop = 1}^n 1 = n$\n}}\n{{eqn\t| r = \\frac {\\sigma^2} n\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Variance\nCategory:Inductive Statistics\n\\end{proof}\n\n"}}, "8159": {"score": 0.7319713830947876, "content": {"text": "\\begin{definition}[Definition:Sample Statistic/Continuous]\nData which can be described with a continuous variable are known as '''continuous data'''.\n\\end{definition}"}}, "2314": {"score": 0.7310611009597778, "content": {"text": "\\begin{definition}[Definition:Deviation]\nLet $S$ be a set of observations.\nLet $x \\in S$.\nThe '''deviation''' of $x$ is the difference between $x$ and some other value, whose nature depends upon the context.\n\\end{definition}"}}, "4137": {"score": 0.7252839803695679, "content": {"text": "\\begin{definition}[Definition:Hypothesis (Inductive Statistics)]\n;Context: Inductive statistics\nA '''hypothesis''' is a statement about a population parameter.\n\\end{definition}"}}, "4138": {"score": 0.7610486745834351, "content": {"text": "\\begin{definition}[Definition:Hypothesis Test]\nA '''hypothesis test''' is a rule that specifies, for a null hypothesis $H_0$ and alternative hypothesis $H_1$: \n* For which sample values the decision is made to accept $H_0$.\n* For which sample values $H_0$ is rejected and $H_1$ is accepted.\n\\end{definition}"}}, "9008": {"score": 0.7393324971199036, "content": {"text": "\\begin{definition}[Definition:Student's t-Distribution]\nLet $X$ be a continuous random variable on a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $\\Img X = \\R$.\n$X$ is said to have a '''$t$-distribution''' with $k$ degrees of freedom {{iff}} it has probability density function: \n:$\\map {f_X} x = \\dfrac {\\map \\Gamma {\\frac {k + 1} 2} } {\\sqrt {\\pi k} \\map \\Gamma {\\frac k 2} } \\paren {1 + \\dfrac {x^2} k}^{-\\frac {k + 1} 2}$\nfor some $k \\in \\R_{> 0}$.\nThis is written: \n:$X \\sim \\StudentT k$\n\\end{definition}"}}, "9320": {"score": 0.7674239277839661, "content": {"text": "\\begin{definition}[Definition:Test Statistic]\nLet $\\theta$ be a population parameter of some population $P$. \nLet $\\Omega$ be the parameter space of $\\theta$. \nLet $\\mathbf X$ be a random sample from $P$. \nLet $T = \\map f {\\mathbf X}$ be a sample statistic.\nLet $\\delta$ be a test procedure of the form: \n:reject $H_0$ if $T \\in C$\nfor some null hypothesis $H_0$ and some $C \\subset \\Omega$.\nWe refer to $T$ as the '''test statistic''' of $\\delta$.\n\\end{definition}"}}, "8158": {"score": 0.7495073676109314, "content": {"text": "\\begin{definition}[Definition:Sample Statistic]\nA '''sample statistic''' is a numerical description of a sample.\n\\end{definition}"}}, "8843": {"score": 0.7608606815338135, "content": {"text": "\\begin{definition}[Definition:Statistic]\nA '''statistic''' is broadly defined as '''a quantity which is calculated from sample data.\n\\end{definition}"}}}}, "TheoremQA_panlu/similarity3.json": {"gold": {"7763": 1}, "retrieved": {"21305": {"score": 0.8975194692611694, "content": {"text": "\\section{Sides of Equiangular Triangles are Reciprocally Proportional}\nTags: Triangles\n\n\\begin{theorem}\n{{:Euclid:Proposition/VI/15}}\nNote: in the above, ''equal'' is to be taken to mean ''of equal area''.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\triangle ABC, \\triangle ADE$ be triangles of equal area which have one angle equal to one angle, namely $\\angle BAC = \\angle DAE$.\nWe need to show that $CA : AD = EA : AB$, that is, the sides about the equal angles are reciprocally proportional.\n:250px\nPlace them so $CA$ is in a straight line with $AD$.\nFrom Two Angles making Two Right Angles make Straight Line $EA$ is also in a straight line with $AB$.\nJoin $BD$.\nIt follows from Ratios of Equal Magnitudes that:\n: $\\triangle CAB : \\triangle BAD = \\triangle EAD : \\triangle BAD$\nBut from Areas of Triangles and Parallelograms Proportional to Base:\n: $\\triangle CAB : \\triangle BAD = CA : AD$\nAlso from Areas of Triangles and Parallelograms Proportional to Base:\n:$\\triangle EAD : \\triangle BAD = EA : AB$\nSo from Equality of Ratios is Transitive:\n: $CA : AD = EA : AB$\n{{qed|lemma}}\nNow let the sides in $\\triangle ABC, \\triangle ADE$ be reciprocally proportional.\nThat is, $CA : AD = EA : AB$.\nJoin $BD$.\nFrom Areas of Triangles and Parallelograms Proportional to Base:\n: $\\triangle CAB : \\triangle BAD = CA : AD$\nAlso from Areas of Triangles and Parallelograms Proportional to Base:\n: $\\triangle EAD : \\triangle BAD = EA : AB$\nIt follows from Equality of Ratios is Transitive that:\n: $\\triangle CAB : \\triangle BAD = \\triangle EAD : \\triangle BAD$\nSo from Magnitudes with Same Ratios are Equal:\n: $\\triangle ABC = \\triangle ADE$\n{{qed}}\n{{Euclid Note|15|VI}}\n\\end{proof}\n\n"}}, "12293": {"score": 0.8979995250701904, "content": {"text": "\\section{Inscribed Squares in Right-Angled Triangle/Side Lengths/Shared Right Angle}\nTags: Inscribed Squares in Right-Angled Triangle\n\n\\begin{theorem}\nLet $a, b, c$ be the side lengths of a right-angled triangle, where $c$ is the length of the hypotenuse.\nThen the side length $l$ of the inscribed square that shares a right angle with the right-angled triangle is given by:\n:$l = \\dfrac {a b} {a + b}$\n\\end{theorem}\n\n\\begin{proof}\n:200px\nIn the figure above, let $BC = a$ and $AC = b$.\nNote that $DE \\parallel CF$.\nTherefore $\\triangle BDE \\sim \\triangle BCA$ by Equiangular Triangles are Similar.\nThus:\n{{begin-eqn}}\n{{eqn | l = \\frac {BD} {DE}\n      | r = \\frac {BC} {CA}\n      | c = {{Defof|Similar Triangles}}\n}}\n{{eqn | l = \\frac {a - l} l\n      | r = \\frac a b\n}}\n{{eqn | l = b \\paren {a - l}\n      | r = a l\n}}\n{{eqn | l = b a\n      | r = a l + b l\n}}\n{{eqn | l = l\n      | r = \\frac {a b} {a + b}\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "8533": {"score": 0.9013521671295166, "content": {"text": "\\begin{definition}[Definition:Similar Planes]\n'''Similar planes''' are plane figures which are similar.\nCategory:Definitions/Euclidean Geometry\n\\end{definition}"}}, "21304": {"score": 0.8989970684051514, "content": {"text": "\\section{Sides of Equal and Equiangular Parallelograms are Reciprocally Proportional}\nTags: Parallelograms\n\n\\begin{theorem}\n{{:Euclid:Proposition/VI/14}}\nNote: in the above, ''equal'' is to be taken to mean ''of equal area''.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\Box AB$ and $\\Box BC$ be two equiangular parallelograms of equal area such that the angles at $B$ are equal.\nLet $DB, BE$ be placed in a straight line.\nBy Two Angles making Two Right Angles make Straight Line it follows that $FB, BG$ also make a straight line.\nWe need to show that $DB : BE = GB : BF$, that is, the sides about the equal angles are reciprocally proportional.\n:300px\nLet the parallelogram $\\Box FE$ be completed.\nWe have that $\\Box AB$ is of equal area with $\\Box BC$, and $\\Box FE$ is another area.\nSo from Ratios of Equal Magnitudes:\n: $\\Box AB : \\Box FE = \\Box BC : \\Box FE$\nBut from Areas of Triangles and Parallelograms Proportional to Base:\n: $\\Box AB : \\Box FE = DB : BE$\nAlso from Areas of Triangles and Parallelograms Proportional to Base:\n: $\\Box BC : \\Box FE = GB : BF$\nSo from Equality of Ratios is Transitive:\n: $DB : BE = GB : BF$\n{{qed|lemma}}\nNext, suppose that $DB : BE = GB : BF$.\nFrom Areas of Triangles and Parallelograms Proportional to Base:\n: $DB : BE = \\Box AB : \\Box FE$\nAlso from Areas of Triangles and Parallelograms Proportional to Base:\n: $GB : BF = \\Box BC : \\Box FE$\nSo from Equality of Ratios is Transitive:\n: $\\Box AB : \\Box FE = \\Box BC : \\Box FE$\nSo from Magnitudes with Same Ratios are Equal:\n: $\\Box AB = \\Box BC$\n{{qed}}\n{{Euclid Note|14|VI}}\n\\end{proof}\n\n"}}, "20143": {"score": 0.8981691002845764, "content": {"text": "\\section{Ratio of Areas of Similar Triangles}\nTags: Triangles, Ratio of Areas of Similar Triangles, Areas of Triangles\n\n\\begin{theorem}\n{{:Euclid:Proposition/VI/19}}\nThat is, the ratio of the areas of the similar triangles is the square of the ratio of the corresponding sides.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\triangle ABC$ and $\\triangle DEF$ be similar, such that $\\angle ABC = \\angle DEF$ and $AB : BC = DE : EF$ such that $BC$ corresponds to $EF$.\n:400px\nLet $BG$ be constructed such that $EF : BG = BC : EF$, and join $AG$.\nFrom Proportional Magnitudes are Proportional Alternately $AB : DE = BC : EF$.\nSo from Equality of Ratios is Transitive $AB : DE = EF : BG$.\nSo in $\\triangle ABC$ and $\\triangle DEF$ the sides about the equal angles are reciprocally proportional.\nFrom Sides of Equiangular Triangles are Reciprocally Proportional, the area of $\\triangle ABG$ equals the area of $\\triangle DEF$.\nNow we have that $BC : EF = EF : BG$.\nSo from {{EuclidDefLink|V|9|Duplicate Ratio}} $BC$ has to $BG$ a ratio duplicate to that which $CB$ has to $EF$.\nBut from Areas of Triangles and Parallelograms Proportional to Base, $CB : BG = \\triangle ABC : \\triangle ABG$.\nSo $\\triangle ABC$ has to $\\triangle ABG$ a ratio duplicate to that which $BC$ has to $EF$.\nBut $\\triangle ABC = \\triangle DEF$.\nSo $\\triangle ABC$ has to $\\triangle DEF$ a ratio duplicate to that which $BC$ has to $EF$.\n{{qed}}\n\\end{proof}\n\n"}}, "15800": {"score": 0.9207293391227722, "content": {"text": "\\section{Construction of Similar Polygon}\nTags: Polygons\n\n\\begin{theorem}\nOn any given straight line it is possible to construct a polygon similar to any given polygon.\n{{:Euclid:Proposition/VI/18}}\n\\end{theorem}\n\n\\begin{proof}\nFrom Sum of Angles of Triangle Equals Two Right Angles $\\angle CFD = \\angle AGB$.\nSo $\\triangle FCD$ is equiangular with $\\triangle GAB$.\nSo from Equiangular Triangles are Similar, $\\triangle FCD$ is similar to $\\triangle GAB$.\nSo $FD : GB = FC : GA = CD : AB$.\nSimilarly from Sum of Angles of Triangle Equals Two Right Angles $\\angle GHB = \\angle FED$.\nSo $\\triangle FDE$ is equiangular with $\\triangle GBH$.\nSo from Equiangular Triangles are Similar, $\\triangle FDE$ is similar to $\\triangle GBH$.\nSo $FD : GB = FE : GH = ED : HB$.\nThus we have that:\n:$FC : AG = CD : AB = FE : GH = ED : HB$\nSince $\\angle CFD = \\angle AGB$ and $\\angle DFE = \\angle BGH$, we have that\n:$\\angle CFE = \\angle CFD + \\angle DFE = \\angle AGB + \\angle BGH = \\angle AGH$\nFor the same reason:\n:$\\angle CDE = \\angle CDF + \\angle FDE = \\angle ABG + \\angle GBH = \\angle ABH$\nSo $CDEF$ is equiangular with $ABHG$.\nAs has been shown, the sides of these polygons are proportional about their equal angles.\nSo from {{EuclidDefLink|VI|1|Similar Rectilineal Figures}}, $CDEF$ is similar $ABHG$.\n{{qed}}\n{{Euclid Note|18|VI|In {{EuclidPropLink|book=VI|prop=20|title=Similar Polygons are composed of Similar Triangles}}, it is shown by dividing any polygon into triangles, any two similar polygons are composed of similar triangles.<br/>Thus the construction as given here can be seen directly to extend to polygons with any number of sides.}}\n\\end{proof}\n\n"}}, "22806": {"score": 0.9043003916740417, "content": {"text": "\\section{Triangles with Proportional Sides are Similar}\nTags: Triangles\n\n\\begin{theorem}\nLet two triangles have corresponding sides which are proportional.\nThen their corresponding angles are equal.\nThus, by definition, such triangles are similar.\n{{:Euclid:Proposition/VI/5}}\n\\end{theorem}\n\n\\begin{proof}\nLet $\\triangle ABC, \\triangle DEF$ be triangles whose sides are proportional, so that:\n:$ AB : BC = DE : EF$\n:$ BC : CA = EF : FD$\n:$ BA : AC = ED : DF$\nWe need to show that\n: $\\angle ABC = \\angle DEF$\n: $\\angle BCA = \\angle EFD$\n: $\\angle BAC = \\angle EDF$\n:400px\nOn the straight line $EF$, and at the points $E, F$ on it, construct $\\angle FEG = \\angle ABC$ and $\\angle EFG = \\angle ACB$.\nFrom Sum of Angles of Triangle Equals Two Right Angles, the remaining angle at $A$ equals the remaining angle at $G$.\nTherefore $\\triangle ABC$ is equiangular with $\\triangle GEF$.\nFrom Equiangular Triangles are Similar, the sides about the equal angles are proportional, and those are corresponding sides which subtend the equal angles.\nSo:\n: $AB : BD = GE : EF$\nBut by hypothesis:\n: $AB : BC = DE : EF$\nSo from Equality of Ratios is Transitive\n: $DE : EF = GE : EF$\nSo each of $DE, GE$ has the same ratio to $EF$.\nSo from Magnitudes with Same Ratios are Equal:\n: $DE = GE$\nFor the same reason:\n: $DF = GF$\nSo we have that $DE = EG$, $EF$ is common and $DF = FG$.\nSo from Triangle Side-Side-Side Equality:\n: $\\triangle DEF = \\triangle GEF$\nThat is:\n: $\\angle DEF = \\angle GEF, \\angle DFE = \\angle GFE, \\angle EDF = \\angle EGF$\nAs $\\angle GEF = \\angle ABC$ it follows that:\n: $\\angle ABC = \\angle DEF$\nFor the same reason $\\angle ACB = \\angle DFE$ and $\\angle BAC = \\angle EDF$.\nHence the result.\n{{Qed}}\n{{Euclid Note|5|VI}}\n\\end{proof}\n\n"}}, "7763": {"score": 0.934489905834198, "content": {"text": "\\begin{definition}[Definition:Reciprocal Proportion]\nLet $P$ and $Q$ be geometric figures of the same type (that is, having the same number and configuration of sides).\nLet $A$ and $B$ be sides of $P$, and let $C$ and $D$ be sides of $Q$, such that $A$ and $C$ are corresponding sides, and $B$ and $D$ also be corresponding sides.\nThen $P$ and $Q$ have sides which are '''in reciprocal proportion''', or are '''reciprocally proportional''', if:\n:$A : D = B : C$\nwhere $A : D$ is the ratio of the lengths of $A$ and $D$.\n\\end{definition}"}}, "8536": {"score": 0.9092279672622681, "content": {"text": "\\begin{definition}[Definition:Similar Triangles]\nSimilar triangles are triangles whose corresponding angles are the same, but whose corresponding sides may be of different lengths.\n:360px\nThus $\\triangle ABC$ is similar to $\\triangle DEF$:\n:$\\angle ABC = \\angle EFD$\n:$\\angle BCA = \\angle EDF$\n:$\\angle CAB = \\angle DEF$\n\\end{definition}"}}, "8531": {"score": 0.9202175140380859, "content": {"text": "\\begin{definition}[Definition:Similar Figures]\nTwo rectilineal figures are '''similar''' {{iff}}:\n:They have corresponding angles, all of which are equal\n:They have corresponding sides, all of which are proportional.\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/jensen1.json": {"gold": {"11618": 1, "11619": 1}, "retrieved": {"11638": {"score": 0.861303448677063, "content": {"text": "\\section{Jordan's Inequality}\nTags: Sine Function\n\n\\begin{theorem}\n:300pxthumbright\n:$\\dfrac 2 \\pi x \\le \\sin x \\le x$\nfor all $x$ in the interval $\\closedint 0 {\\dfrac \\pi 2}$\n\\end{theorem}\n\n\\begin{proof}\nThe {{RHS}} inequality is true by Sine Inequality.\nThe {{LHS}} inequality is true for $x = 0$ and $x = \\dfrac \\pi 2$, where we have equality.\nNow consider $x \\in \\openint 0 {\\dfrac \\pi 2}$.\nFrom Shape of Sine Function, $\\sin x$ is concave on the interval $\\closedint 0 \\pi$.\nLetting $x_1 = 0$, $x_2 = x$ and $x_3 = \\dfrac \\pi 2$ in the definition of Concave Real Function we obtain:\n:$\\dfrac {\\sin x} x \\ge \\dfrac {1 - \\sin x} {\\dfrac \\pi 2 - x}$\nRearranging gives the desired inequality.\n{{qed}}\n{{Namedfor|Marie Ennemond Camille Jordan|cat = Jordan}}\n\\end{proof}\n\n"}}, "22786": {"score": 0.8633211255073547, "content": {"text": "\\section{Triangle Inequality/Complex Numbers/Corollary 1}\nTags: Complex Modulus\n\n\\begin{theorem}\nLet $z_1, z_2 \\in \\C$ be complex numbers.\nLet $\\cmod z$ be the modulus of $z$.\nThen:\n: $\\cmod {z_1 + z_2} \\ge \\cmod {z_1} - \\cmod {z_2}$\n\\end{theorem}\n\n\\begin{proof}\nLet $z_3 := z_1 + z_2$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\cmod {z_3} + \\cmod {\\paren {-z_2} }\n      | o = \\ge\n      | r = \\cmod {z_3 + \\paren {-z_2} }\n      | c = Triangle Inequality for Complex Numbers\n}}\n{{eqn | ll= \\leadsto\n      | l = \\cmod {z_3} + \\cmod {z_2}\n      | o = \\ge\n      | r = \\cmod {z_3 - z_2}\n      | c = Complex Modulus of Additive Inverse\n}}\n{{eqn | ll= \\leadsto\n      | l = \\cmod {z_1 + z_2} + \\cmod {z_2}\n      | o = \\ge\n      | r = \\cmod {z_1}\n      | c = substituting $z_3 = z_1 + z_2$\n}}\n{{eqn | ll= \\leadsto\n      | l = \\cmod {z_1 + z_2}\n      | o = \\ge\n      | r = \\cmod {z_1} - \\cmod {z_2}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "21339": {"score": 0.8652538657188416, "content": {"text": "\\section{Sign of Haversine}\nTags: Haversines\n\n\\begin{theorem}\nThe haversine is non-negative for all $\\theta \\in \\R$.\n\\end{theorem}\n\n\\begin{proof}\nThe haversine is conventionally defined on the real numbers only.\nWe have that:\n:$\\forall \\theta \\in \\R: -1 < \\cos \\theta < 1$\nand so:\n:$\\forall \\theta \\in \\R: 0 < 1 - \\cos \\theta < 2$\nfrom which the result follows by definition of haversine.\n{{qed}}\n\\end{proof}\n\n"}}, "20334": {"score": 0.8646411895751953, "content": {"text": "\\section{Real Sine Function is Bounded}\nTags: Sine Function\n\n\\begin{theorem}\nLet $x \\in \\R$.\nThen:\n:$\\size {\\sin x} \\le 1$\n\\end{theorem}\n\n\\begin{proof}\nFrom the algebraic definition of the real sine function:\n:$\\ds \\sin x = \\sum_{n \\mathop = 0}^\\infty \\paren {-1}^n \\frac {x^{2 n + 1} } {\\paren {2 n + 1}!}$\nit follows that $\\sin x$ is a real function.\nThus $\\sin^2 x \\ge 0$.\nFrom Sum of Squares of Sine and Cosine, we have that $\\cos^2 x + \\sin^2 x = 1$.\nThus it follows that:\n:$\\sin^2 x = 1 - \\cos^2 x \\le 1$\nFrom Ordering of Squares in Reals and the definition of absolute value, we have that:\n:$x^2 \\le 1 \\iff \\size x \\le 1$\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "21677": {"score": 0.8643901348114014, "content": {"text": "\\section{Spherical Law of Sines}\nTags: Spherical Law of Sines, Spherical Trigonometry, Named Theorems\n\n\\begin{theorem}\nLet $\\triangle ABC$ be a spherical triangle on the surface of a sphere whose center is $O$.\nLet the sides $a, b, c$ of $\\triangle ABC$ be measured by the angles subtended at $O$, where $a, b, c$ are opposite $A, B, C$ respectively.\nThen:\n:$\\dfrac {\\sin a} {\\sin A} = \\dfrac {\\sin b} {\\sin B} = \\dfrac {\\sin c} {\\sin C}$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\sin b \\sin c \\cos A\n      | r = \\cos a - \\cos b \\cos c\n      | c = Spherical Law of Cosines\n}}\n{{eqn | ll= \\leadsto\n      | l = \\sin^2 b \\sin^2 c \\cos^2 A\n      | r = \\cos^2 a - 2 \\cos a \\cos b \\cos c + \\cos^2 b \\cos^2 c\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\sin^2 b \\sin^2 c \\paren {1 - \\sin^2 A}\n      | r = \\cos^2 a - 2 \\cos a \\cos b \\cos c + \\cos^2 b \\cos^2 c\n      | c = Sum of Squares of Sine and Cosine\n}}\n{{eqn | ll= \\leadsto\n      | l = \\sin^2 b \\sin^2 c - \\sin^2 b \\sin^2 c \\sin^2 A\n      | r = \\cos^2 a - 2 \\cos a \\cos b \\cos c + \\cos^2 b \\cos^2 c\n      | c = multiplying out\n}}\n{{eqn | ll= \\leadsto\n      | l = \\paren {1 - \\cos^2 b} \\paren {1 - \\cos^2 c} - \\sin^2 b \\sin^2 c \\sin^2 A\n      | r = \\cos^2 a - 2 \\cos a \\cos b \\cos c + \\cos^2 b \\cos^2 c\n      | c = Sum of Squares of Sine and Cosine\n}}\n{{eqn | ll= \\leadsto\n      | l = 1 - \\cos^2 b - \\cos^2 c + \\cos^2 b \\cos^2 c - \\sin^2 b \\sin^2 c \\sin^2 A\n      | r = \\cos^2 a - 2 \\cos a \\cos b \\cos c + \\cos^2 b \\cos^2 c\n      | c = multiplying out\n}}\n{{eqn | n = 1\n      | ll= \\leadsto\n      | l = \\sin^2 b \\sin^2 c \\sin^2 A\n      | r = 1 - \\cos^2 a - \\cos^2 b - \\cos^2 c + 2 \\cos a \\cos b \\cos c\n      | c = rearranging and simplifying\n}}\n{{end-eqn}}\nLet $X \\in \\R_{>0}$ such that:\n:$X^2 \\sin^2 a \\sin^2 b \\sin^2 c = 1 - \\cos^2 a - \\cos^2 b - \\cos^2 c + 2 \\cos a \\cos b \\cos c$\nThen from $(1)$:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {X^2 \\sin^2 a \\sin^2 b \\sin^2 c} {\\sin^2 b \\sin^2 c \\sin^2 A}\n      | o = =\n      | r = \\dfrac {1 - \\cos^2 a - \\cos^2 b - \\cos^2 c + 2 \\cos a \\cos b \\cos c} {1 - \\cos^2 a - \\cos^2 b - \\cos^2 c + 2 \\cos a \\cos b \\cos c}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = X^2\n      | r = \\dfrac {\\sin^2 A} {\\sin^2 a}\n      | c = \n}}\n{{end-eqn}}\nIn a spherical triangle, all of the sides are less than $\\pi$ radians.\nThe same applies to the angles.\nFrom Shape of Sine Function:\n:$\\sin \\theta > 0$ for all $0 < \\theta < \\pi$\nHence the negative root of $\\dfrac {\\sin^2 A} {\\sin^2 a}$ does not apply, and so:\n:$X = \\dfrac {\\sin A} {\\sin a}$\nSimilarly, from applying the Spherical Law of Cosines to $\\cos B$ and $\\cos C$:\n{{begin-eqn}}\n{{eqn | l = \\sin a \\sin c \\cos B\n      | r = \\cos b - \\cos a \\cos c\n}}\n{{eqn | l = \\sin a \\sin b \\cos C\n      | r = \\cos c - \\cos a \\cos b\n}}\n{{end-eqn}}\nwe arrive at the same point:\n{{begin-eqn}}\n{{eqn | l = X\n      | r = \\dfrac {\\sin B} {\\sin b}\n}}\n{{eqn | r = \\dfrac {\\sin A} {\\sin a}\n}}\n{{end-eqn}}\nwhere:\n:$X^2 \\sin^2 a \\sin^2 b \\sin^2 c = 1 - \\cos^2 a - \\cos^2 b - \\cos^2 c + 2 \\cos a \\cos b \\cos c$\nas before.\nHence we have:\n:$\\dfrac {\\sin a} {\\sin A} = \\dfrac {\\sin b} {\\sin B} = \\dfrac {\\sin c} {\\sin C}$\n{{qed}}\n\\end{proof}\n\n"}}, "23452": {"score": 0.8743870854377747, "content": {"text": "\\section{Yff's Conjecture}\nTags: Triangles\n\n\\begin{theorem}\nLet $\\triangle ABC$ be a triangle.\nLet $\\omega$ be the Brocard angle of $\\triangle ABC$.\nThen:\n:$8 \\omega^3 < ABC$\nwhere $A, B, C$ are measured in radians.\n\\end{theorem}\n\n\\begin{proof}\nThe Abi-Khuzam Inequality states that\n:$\\sin A \\cdot \\sin B \\cdot \\sin C \\le  \\paren {\\dfrac {3 \\sqrt 3} {2 \\pi} }^3 A \\cdot B \\cdot C$\nThe maximum value of $A B C - 8 \\omega^3$ occurs when two of the angles are equal.\nSo taking $A = B$, and using $A + B + C = \\pi$, the maximum occurs at the maximum of:\n:$\\map f A = A^2 \\paren {\\pi - 2 A} - 8 \\paren {\\map \\arccot {2 \\cot A - \\cot 2 A} }^3$ \t\nwhich occurs when:\n:$2 A \\paren {\\pi - 3 A} - \\dfrac {48 \\paren {\\map \\arccot {\\frac 1 2 \\paren {3 \\cot A + \\tan A} } }^2 \\paren {1 + 2 \\cos 2 A} } {5 + 4 \\cos 2 A} = 0$\n{{finish|Needs expanding and completing}}\n\\end{proof}\n\n"}}, "7452": {"score": 0.8673539161682129, "content": {"text": "\\begin{definition}[Definition:Prosthaphaeresis/Trigonometry]\nThe trigonometric technique of prosthaphaeresis is based on the Prosthaphaeresis Formula for Sine plus Sine:\n:$\\dfrac {\\sin \\alpha + \\sin \\beta} 2 = \\map \\sin {\\dfrac {\\alpha + \\beta} 2} \\, \\map \\cos {\\dfrac {\\alpha - \\beta} 2}$\n\\end{definition}"}}, "22787": {"score": 0.8755698204040527, "content": {"text": "\\section{Triangle Inequality/Complex Numbers/General Result}\nTags: Triangle Inequality, Complex Modulus, Complex Analysis\n\n\\begin{theorem}\nLet $z_1, z_2, \\dotsc, z_n \\in \\C$ be complex numbers.\nLet $\\cmod z$ be the modulus of $z$.\nThen:\n:$\\cmod {z_1 + z_2 + \\dotsb + z_n} \\le \\cmod {z_1} + \\cmod {z_2} + \\dotsb + \\cmod {z_n}$\n\\end{theorem}\n\n\\begin{proof}\nProof by induction:\nFor all $n \\in \\N_{> 0}$, let $\\map P n$ be the proposition:\n:$\\cmod {z_1 + z_2 + \\dotsb + z_n} \\le \\cmod {z_1} + \\cmod {z_2} + \\dotsb + \\cmod {z_n}$\n$\\map P 1$ is true by definition of the usual ordering on real numbers:\n:$\\cmod {z_1} \\le \\cmod {z_1}$\n\\end{proof}\n\n"}}, "10301": {"score": 0.8706863522529602, "content": {"text": "\\section{Nesbitt's Inequality}\nTags: Named Theorems, Algebra\n\n\\begin{theorem}\nLet $a$, $b$ and $c$ be positive real numbers.\nThen:\n:$\\dfrac a {b + c} + \\dfrac b {a + c} + \\dfrac c {a + b} \\ge \\dfrac 3 2$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\frac a {b + c} + \\frac b {a + c} + \\frac c {a + b}\n      | o = \\ge\n      | r = \\dfrac 3 2\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\frac {a + b + c} {b + c} + \\frac {a + b + c} {a + c} + \\frac {a + b + c} {a + b}\n      | o = \\ge\n      | r = \\frac 9 2\n      | c = by adding 3\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\frac {a + b + c} {b + c} + \\frac {a + b + c} {a + c} + \\frac {a + b + c} {a + b}\n      | o = \\ge\n      | r = \\frac {9 \\paren {a + b + c} } {\\paren {b + c} + \\paren {a + c} + \\paren {a + b} }\n      | c = as $\\dfrac {a + b + c} {\\paren {b + c} + \\paren {a + c} + \\paren {a + b} } = \\dfrac 1 2$\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\frac {\\frac 1 {b + c} + \\frac 1 {a + c} + \\frac 1 {a + b} } 3\n      | o = \\ge\n      | r = \\frac 3 {\\paren {b + c} + \\paren {a + c} + \\paren {a + b} }\n      | c = dividing by $3 \\paren {a + b + c}$\n}}\n{{end-eqn}}\nThese are the arithmetic mean and the harmonic mean of $\\dfrac 1 {b + c}$, $\\dfrac 1 {a + c}$ and $\\dfrac 1 {a + b}$.\nFrom Arithmetic Mean is Never Less than Harmonic Mean the last inequality is true.\nThus Nesbitt's Inequality holds.\n{{qed}}\n{{Namedfor|A.M. Nesbitt|cat = Nesbitt, A.M.}}\n\\end{proof}\n\n"}}, "10525": {"score": 0.8729088306427002, "content": {"text": "\\section{Modulus of Sine of x Less Than or Equal To Absolute Value of x}\nTags: Inequalities, Sine Function, Absolute Value Function\n\n\\begin{theorem}\nLet $x$ be a real number.\nThen:\n:$\\size {\\sin x} \\le \\size x$\n\\end{theorem}\n\n\\begin{proof}\nClearly the inequality holds if $x = 0$.\nTake $x \\ne 0$. \nFrom the Mean Value Theorem and Derivative of Sine Function, there exists $c \\in \\R$ such that: \n:$\\ds \\frac {\\sin x - \\sin 0} {x - 0} = \\cos c$\nso:\n:$\\sin x = x \\cos c$\nThen we have: \n{{begin-eqn}}\n{{eqn\t| l = \\size {\\sin x}\n\t| r = \\size {x \\cos c}\n}}\n{{eqn\t| o = \\le\n\t| r = \\size x\n}}\n{{end-eqn}}\nas required.\n{{qed}}\nCategory:Inequalities\nCategory:Sine Function\nCategory:Absolute Value Function\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/determinant1.json": {"gold": {"5727": 1}, "retrieved": {"15063": {"score": 0.9321454167366028, "content": {"text": "\\section{Determinant with Unit Element in Otherwise Zero Column}\nTags: Determinants\n\n\\begin{theorem}\nLet $D$ be the determinant:\n:$D = \\begin{vmatrix}\n  1 & b_{12} & \\cdots & b_{1n} \\\\\n  0 & b_{22} & \\cdots & b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n  0 & b_{n2} & \\cdots & b_{nn}\n\\end{vmatrix}$\nThen:\n:$D = \\begin{vmatrix}\n  b_{22} & \\cdots & b_{2n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n  b_{n2} & \\cdots & b_{nn}\n\\end{vmatrix}$\n\\end{theorem}\n\n\\begin{proof}\nWe note that:\n:$D = \\begin{vmatrix}\n  1 & b_{12} & \\cdots & b_{1n} \\\\\n  0 & b_{22} & \\cdots & b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n  0 & b_{n2} & \\cdots & b_{nn}\n\\end{vmatrix}$\nis the transpose of:\n:$D^\\intercal = \\begin{vmatrix}\n  1 & 0 & \\cdots & 0 \\\\\n  b_{12} & b_{22} & \\cdots & b_{n2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n  b_{1n} & b_{2n} & \\cdots & b_{nn}\n\\end{vmatrix}$\nFrom Determinant with Unit Element in Otherwise Zero Row:\n:$D^\\intercal = \\begin{vmatrix}\n  b_{22} & \\cdots & b_{n2} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n  b_{2n} & \\cdots & b_{nn}\n\\end{vmatrix}$\nThe result follows by Determinant of Transpose.\n{{Qed}}\n\\end{proof}\n\n"}}, "2308": {"score": 0.9349663257598877, "content": {"text": "\\begin{definition}[Definition:Determinant/Matrix/Order]\nThe '''order''' of a determinant is defined as the order of the square matrix on which it is defined.\n\\end{definition}"}}, "2309": {"score": 0.9431910514831543, "content": {"text": "\\begin{definition}[Definition:Determinant/Matrix/Order 1]\nLet $\\mathbf A = \\sqbrk a_1$ be a square matrix of order $1$.\nThat is, let:\n:$\\mathbf A = \\begin {bmatrix} a_{1 1} \\end {bmatrix}$\nThen the determinant of $\\mathbf A$ is defined as:\n:$\\begin {vmatrix} a_{1 1} \\end {vmatrix} = a_{1 1}$\nThus the determinant of an order $1$ matrix is that element itself.\n\\end{definition}"}}, "15167": {"score": 0.9414617419242859, "content": {"text": "\\section{Determinant of Combinatorial Matrix}\nTags: Combinatorial Matrix, Matrix Algebra, Matrix, Matrix Examples, Determinants\n\n\\begin{theorem}\nLet $C_n$ be the combinatorial matrix of order $n$ given by:\n:$C_n = \\begin{bmatrix}\nx + y & y & \\cdots & y \\\\\ny & x + y & \\cdots & y \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny & y & \\cdots & x + y\n\\end{bmatrix}$\nThen the determinant of $C_n$ is given by:\n:$\\map \\det {C_n} = x^{n - 1} \\paren {x + n y}$\n\\end{theorem}\n\n\\begin{proof}\nTake the determinant $\\map \\det {C_n}$:\n:$\\map \\det {C_n} = \\begin{vmatrix}\nx + y & y & y & \\cdots & y \\\\\ny & x + y & y & \\cdots & y \\\\\ny & y & x + y & \\cdots & y \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ny & y & y & \\cdots & x + y\n\\end{vmatrix}$\nSubtract column $1$ from columns $2$ to $n$.\nFrom Multiple of Row Added to Row of Determinant this will have no effect on the value of the determinant:\n:$\\map \\det {C_n} = \\begin{vmatrix}\nx + y & -x & -x & \\cdots & -x \\\\\ny & x & 0 & \\cdots & 0 \\\\\ny & 0 & x & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ny & 0 & 0 & \\cdots & x\n\\end{vmatrix}$\nAdd rows $2$ to $n$ to row $1$.\nAgain, from Multiple of Row Added to Row of Determinant this will have no effect on the value of the determinant:\n:$\\map \\det {C_n} = \\begin{vmatrix}\nx + n y & 0 & 0 & \\cdots & 0 \\\\\ny & x & 0 & \\cdots & 0 \\\\\ny & 0 & x & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ny & 0 & 0 & \\cdots & x\n\\end{vmatrix}$\nThis is now the determinant of a (lower) triangular matrix.\nFrom Determinant of Triangular Matrix, it follows immediately that:\n:$\\map \\det {C_n} = x^{n - 1} \\paren {x + n y}$\n{{qed}}\n\\end{proof}\n\n"}}, "15064": {"score": 0.9382752180099487, "content": {"text": "\\section{Determinant with Unit Element in Otherwise Zero Row}\nTags: Determinants\n\n\\begin{theorem}\nLet $D$ be the determinant:\n:$D = \\begin {vmatrix}\n      1 &       0 & \\cdots &       0 \\\\\nb_{2 1} & b_{2 2} & \\cdots & b_{2 n} \\\\\n \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\nb_{n 1} & b_{n 2} & \\cdots & b_{n n}\n\\end {vmatrix}$\nThen:\n:$D = \\begin {vmatrix}\nb_{2 2} & \\cdots & b_{2 n} \\\\\n \\vdots & \\ddots &  \\vdots \\\\\nb_{n 2} & \\cdots & b_{n n}\n\\end {vmatrix}$\n\\end{theorem}\n\n\\begin{proof}\nWe refer to the elements of:\n:$\\begin {vmatrix}\n      1 &       0 & \\cdots &       0 \\\\\nb_{2 1} & b_{2 2} & \\cdots & b_{2 n} \\\\\n \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\nb_{n 1} & b_{n 2} & \\cdots & b_{n n}\n\\end {vmatrix}$\nas $\\begin {vmatrix} b_{i j} \\end {vmatrix}$.\nThus $b_{1 1} = 1, b_{1 2} = 0, \\ldots, b_{1 n} = 0$.\nThen from the definition of determinant:\n{{begin-eqn}}\n{{eqn | l = D\n      | r = \\sum_\\lambda \\paren {\\map \\sgn \\lambda \\prod_{k \\mathop = 1}^n b_{k \\map \\lambda k} }\n      | c = \n}}\n{{eqn | r = \\sum_{\\lambda} \\map \\sgn \\lambda b_{1 \\map \\lambda 1} b_{2 \\map \\lambda 2} \\cdots b_{n \\map \\lambda n}\n      | c = \n}}\n{{end-eqn}}\nNow we note:\n{{begin-eqn}}\n{{eqn | l = \\map \\lambda 1 = 1\n      | o = \\implies\n      | r = b_{1 \\map \\lambda 1} b_{2 \\map \\lambda 2} \\cdots b_{n \\map \\lambda n} = 1\n      | c = \n}}\n{{eqn | l = \\map \\lambda 1 \\ne 1\n      | o = \\implies\n      | r = b_{1 \\map \\lambda 1} b_{2 \\map \\lambda 2} \\cdots b_{n \\map \\lambda n} = 0\n      | c = \n}}\n{{end-eqn}}\nSo only those permutations on $\\N^*_n$ such that $\\map \\lambda 1 = 1$ contribute towards the final summation.\nThus we have:\n:$\\ds D = \\sum_\\mu \\map \\sgn \\mu b_{2 \\map \\mu 2} \\cdots b_{n \\map \\mu n}$\nwhere $\\mu$ is the collection of all permutations on $\\N^*_n$ which fix $1$.\nHence the result.\n{{Qed}}\n\\end{proof}\n\n"}}, "2306": {"score": 0.9524015188217163, "content": {"text": "\\begin{definition}[Definition:Determinant/Matrix/Definition 2]\nLet $\\mathbf A = \\sqbrk a_n$ be a square matrix of order $n$.\nThat is, let:\n:$\\mathbf A = \\begin {bmatrix}\na_{1 1} & a_{1 2} & \\cdots & a_{1 n} \\\\\na_{2 1} & a_{2 2} & \\cdots & a_{2 n} \\\\\n \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\na_{n 1} & a_{n 2} & \\cdots & a_{n n} \\\\\n\\end {bmatrix}$\nThe '''determinant''' of $\\mathbf A$ is defined as follows:\nFor $n = 1$, the order $1$ determinant is defined as:\n{{:Definition:Determinant of Order 1}}\nFor $n > 1$, the determinant of order $n$ is defined recursively as:\n:$\\ds \\map \\det {\\mathbf A} := \\begin {vmatrix}\na_{1 1} & a_{1 2} & a_{1 3} & \\cdots & a_{1 n} \\\\\na_{2 1} & a_{2 2} & a_{2 3} & \\cdots & a_{2 n} \\\\\na_{3 1} & a_{3 2} & a_{3 3} & \\cdots & a_{3 n} \\\\\n \\vdots &  \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\na_{n 1} & a_{n 2} & a_{n 3} & \\cdots & a_{n n} \\\\\n\\end {vmatrix} = a_{1 1} \\begin {vmatrix}\na_{2 2} & a_{2 3} & \\cdots & a_{2 n} \\\\\na_{3 2} & a_{3 3} & \\cdots & a_{3 n} \\\\\n \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\na_{n 2} & a_{n 3} & \\cdots & a_{n n} \\\\\n\\end {vmatrix} - a_{1 2} \\begin {vmatrix}\na_{2 1} & a_{2 3} & \\cdots & a_{2 n} \\\\\na_{3 1} & a_{3 3} & \\cdots & a_{3 n} \\\\\n \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\na_{n 1} & a_{n 3} & \\cdots & a_{n n} \\\\\n\\end {vmatrix} + \\cdots + \\paren {-1}^{n + 1} a_{1 n} \\begin {vmatrix}\na_{2 1} & a_{2 2} & \\cdots & a_{2, n - 1} \\\\\na_{3 1} & a_{3 3} & \\cdots & a_{3, n - 1} \\\\\n \\vdots &  \\vdots & \\ddots &       \\vdots \\\\\na_{n 1} & a_{n 3} & \\cdots & a_{n, n - 1} \\\\\n\\end {vmatrix}$\n\\end{definition}"}}, "2310": {"score": 0.9488402605056763, "content": {"text": "\\begin{definition}[Definition:Determinant/Matrix/Order 2]\nLet $\\mathbf A = \\sqbrk a_2$ be a square matrix of order $2$.\nThat is, let:\n:$\\mathbf A = \\begin {bmatrix}\na_{1 1} & a_{1 2} \\\\\na_{2 1} & a_{2 2}\n\\end {bmatrix}$\nThen the '''determinant''' of $\\mathbf A$ is defined as:\n{{begin-eqn}}\n{{eqn | l = \\begin {vmatrix} a_{1 1} & a_{1 2} \\\\ a_{2 1} & a_{2 2} \\end{vmatrix}\n      | r = \\map \\sgn {1, 2} a_{1 1} a_{2 2} + \\map \\sgn {2, 1} a_{1 2} a_{2 1}\n      | c = \n}}\n{{eqn | r = a_{1 1} a_{2 2} - a_{1 2} a_{2 1}\n      | c = \n}}\n{{end-eqn}}\nwhere $\\sgn$ denotes the sign of the permutation.\n\\end{definition}"}}, "2311": {"score": 0.9566680788993835, "content": {"text": "\\begin{definition}[Definition:Determinant/Matrix/Order 3]\nLet $\\mathbf A = \\sqbrk a_3$ be a square matrix of order $3$.\nThat is, let:\n:$\\mathbf A = \\begin {bmatrix}\na_{1 1} & a_{1 2} & a_{1 3} \\\\\na_{2 1} & a_{2 2} & a_{2 3} \\\\\na_{3 1} & a_{3 2} & a_{3 3}\n\\end {bmatrix}$\nThe determinant of $\\mathbf A$ is given by:\n:$\\map \\det {\\mathbf A} = \\begin {vmatrix}\na_{1 1} & a_{1 2} & a_{1 3} \\\\\na_{2 1} & a_{2 2} & a_{2 3} \\\\\na_{3 1} & a_{3 2} & a_{3 3}\n\\end {vmatrix}$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map \\det {\\mathbf A}\n      | r = a_{1 1} \\begin {vmatrix} a_{2 2} & a_{2 3} \\\\ a_{3 2} & a_{3 3} \\end {vmatrix} - a_{1 2} \\begin {vmatrix} a_{2 1} & a_{2 3} \\\\ a_{3 1} & a_{3 3} \\end {vmatrix} + a_{1 3} \\begin {vmatrix} a_{2 1} & a_{2 2} \\\\ a_{3 1} & a_{3 2} \\end{vmatrix}\n      | c = \n}}\n{{eqn | r = \\map \\sgn {1, 2, 3} a_{1 1} a_{2 2}  a_{3 3}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\map \\sgn {1, 3, 2} a_{1 1} a_{2 3}  a_{3 2}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\map \\sgn {2, 1, 3} a_{1 2} a_{2 1}  a_{3 3}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\map \\sgn {2, 3, 1} a_{1 2} a_{2 3}  a_{3 1}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\map \\sgn {3, 1, 2} a_{1 3} a_{2 1}  a_{3 2}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\map \\sgn {3, 2, 1} a_{1 3} a_{2 2}  a_{3 1}\n      | c = \n}}\n{{eqn | r = a_{1 1} a_{2 2}  a_{3 3}\n      | c = \n}}\n{{eqn | o = \n      | ro= -\n      | r = a_{1 1} a_{2 3}  a_{3 2}\n      | c = \n}}\n{{eqn | o = \n      | ro= -\n      | r = a_{1 2} a_{2 1}  a_{3 3}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = a_{1 2} a_{2 3}  a_{3 1}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = a_{1 3} a_{2 1}  a_{3 2}\n      | c = \n}}\n{{eqn | o = \n      | ro= -\n      | r = a_{1 3} a_{2 2}  a_{3 1}\n      | c = \n}}\n{{end-eqn}}\nand thence in a single expression as:\n:$\\ds \\map \\det {\\mathbf A} = \\frac 1 6 \\sum_{i \\mathop = 1}^3 \\sum_{j \\mathop = 1}^3 \\sum_{k \\mathop = 1}^3 \\sum_{r \\mathop = 1}^3 \\sum_{s \\mathop = 1}^3 \\sum_{t \\mathop = 1}^3 \\map \\sgn {i, j, k} \\map \\sgn {r, s, t} a_{i r} a_{j s} a_{k t}$\nwhere $\\map \\sgn {i, j, k}$ is the sign of the permutation $\\tuple {i, j, k}$ of the set $\\set {1, 2, 3}$.\nThe values of the various instances of $\\map \\sgn {\\lambda_1, \\lambda_2, \\lambda_3}$ are obtained by applications of Parity of K-Cycle.\n\\end{definition}"}}, "2307": {"score": 0.9501693248748779, "content": {"text": "\\begin{definition}[Definition:Determinant/Matrix/In Full]\nLet $\\mathbf A = \\sqbrk a_n$ be a square matrix of order $n$.\nWhen written out in full, the determinant of $\\mathbf A$ is denoted:\n:$\\map \\det {\\mathbf A} = \\begin {vmatrix}\na_{1 1} & a_{1 2} & \\cdots & a_{1 n} \\\\\na_{2 1} & a_{2 2} & \\cdots & a_{2 n} \\\\\n \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\na_{n 1} & a_{n 2} & \\cdots & a_{n n} \\\\\n\\end {vmatrix}$\nCategory:Definitions/Determinants\n\\end{definition}"}}, "2304": {"score": 0.952059805393219, "content": {"text": "\\begin{definition}[Definition:Determinant/Matrix]\nLet $\\mathbf A = \\sqbrk a_n$ be a square matrix of order $n$.\nThat is, let:\n:$\\mathbf A = \\begin {bmatrix}\na_{1 1} & a_{1 2} & \\cdots & a_{1 n} \\\\\na_{2 1} & a_{2 2} & \\cdots & a_{2 n} \\\\\n \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\na_{n 1} & a_{n 2} & \\cdots & a_{n n} \\\\\n\\end {bmatrix}$\n\\end{definition}"}}}}, "TheoremQA_elainewan/math_algebra_1_2.json": {"gold": {"8569": 1, "21613": 1}, "retrieved": {"7552": {"score": 0.8434568047523499, "content": {"text": "\\begin{definition}[Definition:Quartic Equation]\nA '''quartic equation''' is a polynomial equation of the form:\n:$a x^4 + b x^3 + c x^2 + d x + e$\n\\end{definition}"}}, "2073": {"score": 0.8507857322692871, "content": {"text": "\\begin{definition}[Definition:Cubic Polynomial]\nA '''cubic polynomial''' is a polynomial of degree $3$.\n\\end{definition}"}}, "12952": {"score": 0.8636749982833862, "content": {"text": "\\section{Graph of Quadratic describes Parabola/Corollary 2}\nTags: Conic Sections, Parabolas, Graph of Quadratic describes Parabola, Quadratic Equations\n\n\\begin{theorem}\nThe locus of the equation of the square root function on the non-negative reals:\n:$\\forall x \\in \\R_{\\ge 0}: \\map f x = \\sqrt x$\ndescribes half of a parabola.\n\\end{theorem}\n\n\\begin{proof}\nFrom Graph of Quadratic describes Parabola: Corollary 1, where:\n:$y = x^2$\nis the equation of a parabola.\nLet $f: \\R \\to \\R$ be the real function defined as:\n:$\\map f x = x^2$\nFrom Square of Real Number is Non-Negative, the image of $f$ is $\\R_{\\ge 0}$.\nAlso we have from Positive Real Number has Two Square Roots:\n:$\\forall x \\in \\R: \\paren {-x}^2 = x^2$\nThus it is necessary to apply a bijective restriction upon $f$.\nLet $g: \\R_{\\ge 0} \\to \\R_{\\ge 0}$ be the bijective restriction of $f$ to $\\R_{\\ge 0} \\times \\R_{\\ge 0}$:\n:$\\forall x \\in \\R_{\\ge 0}: \\map g x = x^2$\nFrom Inverse of Bijection is Bijection, $g^{-1}: \\R_{\\ge 0} \\to \\R_{\\ge 0}$ is also a bijection.\nBy definition:\n:$\\forall x \\in \\R_{\\ge 0}: \\map {g^{-1} } x = +\\sqrt x$\nThen from Graph of Inverse Mapping, the graph of $g^{-1}$ is the same as the graph of $g$, reflected in the line $x = y$.\nAs the graph of $f$ is a parabola, the graph of $g$ is also a parabola, but because of the restriction to $\\R_{\\ge 0}$, just half of it.\nThus the graph of $g^{-1}$ is also half a parabola.\n{{qed}}\n\\end{proof}\n\n"}}, "21609": {"score": 0.856107771396637, "content": {"text": "\\section{Solution to Quadratic Equation}\nTags: Polynomial Theory, Direct Proofs, Polynomial Equations, Algebra, Quadratic Equations\n\n\\begin{theorem}\nThe quadratic equation of the form $a x^2 + b x + c = 0$ has solutions:\n:$x = \\dfrac {-b \\pm \\sqrt {b^2 - 4 a c} } {2 a}$\n\\end{theorem}\n\n\\begin{proof}\nLet $a x^2 + b x + c = 0$. Then:\n{{begin-eqn}}\n{{eqn | l = 4 a^2 x^2 + 4 a b x + 4 a c\n      | r = 0\n      | c = multiplying through by $4 a$\n}}\n{{eqn | ll= \\leadsto\n      | l = \\paren {2 a x + b}^2 - b^2 + 4 a c\n      | r = 0\n      | c = Completing the Square\n}}\n{{eqn | ll= \\leadsto\n      | l = \\paren {2 a x + b}^2\n      | r = b^2 - 4 a c\n}}\n{{eqn | ll= \\leadsto\n      | l = x\n      | r = \\frac {-b \\pm \\sqrt {b^2 - 4 a c} } {2 a}\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "7519": {"score": 0.8557606339454651, "content": {"text": "\\begin{definition}[Definition:Quadratic Polynomial]\nA '''quadratic polynomial''' is a polynomial of degree $2$.\n\\end{definition}"}}, "7510": {"score": 0.8821632862091064, "content": {"text": "\\begin{definition}[Definition:Quadratic Equation]\nA '''quadratic equation''' is a polynomial equation of the form:\n:$a x^2 + b x + c = 0$\nsuch that $a \\ne 0$.\nFrom Solution to Quadratic Equation, the solutions are:\n:$x = \\dfrac {-b \\pm \\sqrt {b^2 - 4 a c} } {2 a}$\n\\end{definition}"}}, "12950": {"score": 0.8676556348800659, "content": {"text": "\\section{Graph of Quadratic describes Parabola}\nTags: Conic Sections, Parabolas, Graph of Quadratic describes Parabola, Quadratic Equations\n\n\\begin{theorem}\nThe locus of the equation defining a quadratic:\n:$y = a x^2 + b x + c$\ndescribes a parabola.\n\\end{theorem}\n\n\\begin{proof}\nConsider the focus-directrix property of a parabola $P$.\nLet the focus of $P$ be the point $\\tuple {0, f}$ on a Cartesian plane.\nLet the directrix of $P$ be the straight line $y = -d$.\nLet $\\tuple {x, y}$ be an arbitrary point on $P$.\nThen by the focus-directrix property:\n:$y + d = \\sqrt {\\paren {x - k}^2 + \\tuple {y - f}^2}$\nwhere:\n:$y + d$ is the distance from $\\tuple {x, y}$ to the straight line $y = -d$\n:$\\sqrt {\\paren {x - k}^2 + \\paren {y - f}^2}$ is the distance from $\\tuple {x, y}$ to the point $\\tuple {k, f}$ by the Distance Formula.\nHence:\n{{begin-eqn}}\n{{eqn | l = \\paren {y + d}^2\n      | r = \\paren {x - k}^2 + \\paren {y - f}^2\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = y^2 + 2 y d + d^2\n      | r = x^2 - 2 k x + k^2 + y^2 - 2 f y + f^2\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = 2 y \\paren {f + d}\n      | r = x^2 - 2 k x + f^2 + k^2 - d^2\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = y\n      | r = \\frac 1 {2 \\paren {f + d} } x^2 - \\frac k {\\paren {f + d} } x + \\frac {f - d} 2\n      | c = \n}}\n{{end-eqn}}\nThis is in the form $y = a x^2 + b^2 + c$.\nBy setting $k$, $f$ and $d$ appropriately in terms of $a$, $b$ and $c$, the specific focus and directrix can be appropriately positioned.\n{{qed}}\nCategory:Parabolas\nCategory:Quadratic Equations\nCategory:Graph of Quadratic describes Parabola\n\\end{proof}\n\n"}}, "7513": {"score": 0.9119248986244202, "content": {"text": "\\begin{definition}[Definition:Quadratic Function]\nA '''quadratic function''' is an expression of the form:\n:$\\map Q x := a_0 + a_1 x + a_2 x^2$\nwhere $a_0, a_1, a_2$ are constants.\nThe domain of $x$ is usually defined as the real numbers $\\R$ or the \ncomplex numbers $\\C$.\n\\end{definition}"}}, "18511": {"score": 0.8690995573997498, "content": {"text": "\\section{Parabolas Inscribed in Shared Tangent Lines}\nTags: Algebra, Tangents, Analytic Geometry\n\n\\begin{theorem}\nLet the function $\\map f x = A x^2 + B x + C_1$ be a curve embedded in the Euclidean Plane.\nLet $\\map {y_1} x$ be the equation of the tangent line at $\\tuple {Q, \\map f Q}$ on $f$.\nLet $\\map {y_2} x$ be the equation of the tangent line at $\\tuple {-Q, \\map f {-Q} }$ on $f$.\nThen there exists another function $\\map g x$ also embedded in the Euclidean Plane defined as:\n:$\\map g x = -A x^2 + B x + C_2$.\nwith:\n:tangent lines $\\map {y_3} x$ being the equation of the tangent line at $\\tuple {Q, \\map g Q}$ on $g$\nand:\n:$\\map {y_4} x$ being the equation of the tangent line at $\\tuple {-Q, \\map g {-Q} }$ on $g$.\nso that the tangent lines $y_3$ and $y_4$ inscribe $\\map f x$ and the tangent lines $y_1$ and $y_2$ inscribe $\\map g x$.\n\\end{theorem}\n\n\\begin{proof}\nThe tangent line at $\\tuple {Q, \\map f \\Q}$ on $f$ is defined as:\n:$\\map {y_1} x = \\paren {2 A Q + B} x + b_1$\nwhere $2 A Q + B$ is the slope of the tangent line on the point $\\tuple {Q, \\map g Q}$ on $f$.\n  \nSubstitute in the coordinates of the point $\\tuple {Q, \\map g Q}$ to $y_1$ and solve for $b_1$.\nThis will reveal the $y$-intercept of $y_1$:\n{{begin-eqn}}\n{{eqn | l = A Q^2 + B Q + C_1\n      | r = \\paren {2 A Q + B} Q + b_1\n      | c = The value of $y_1 = \\map f Q$\n}}\n{{eqn | l = A Q^2 + B Q + C_1 -2 A Q^2 - B Q\n      | r = b_1\n}}\n{{eqn | l = -A Q^2 + C_1\n      | r = b_1\n}}\n{{end-eqn}}\n    \nContinue by following the same steps for $y_2$ which is defined:\n:$\\map {y_2} x = \\paren {-2 A Q + B} x + b_2$\nwhere $-2 A Q + B$ is the slope of the Tangent line at the point $\\tuple {-Q, \\map f {-Q} }$ on $f$.\nSubstitute in the coordinates of the point $\\paren {-Q, \\map f {-Q} }$ to $y_2$.\nUse these values to solve for $b_2$, and this will reveal the $y$-intercept of $y_2$:\n{{begin-eqn}}\n{{eqn | l = A \\paren {-Q}^2 + B \\paren {-Q} + C_1\n      | r = \\paren {-2 A Q + B} \\paren {-Q} + b_2\n      | c = the value of $y_2 = \\map f {-Q}$\n}}\n{{eqn | l = A Q^2 - B Q + C_1 -2 A Q^2 + B Q\n      | r = b_2\n}}\n{{eqn | l = -A Q^2 + C_1\n      | r = b_2\n}}\n{{end-eqn}}\nThe $y$-intercepts of both $y_1$ and $y_2$ have been shown to be equivalent.\n{{qed|lemma}}\nSince $b_1 = b_2$ redefine this value as $b$.\nThe distance between $b$ and $C_1$ is $\\size {C_1 - b}$.\nLet $\\map g x = -A x^2 + B x + C_2$.\nThen the Tangent line at the point $\\tuple {Q, \\map g Q}$ on $g$ is defined as:\n:$\\map {y_3} x = \\paren {-2 A Q + B} x + b_3$\nwhere $-2 A Q + B$ is the slope of the tangent line at $\\tuple {Q, \\map g Q}$ on $g$.\nSolve for $b_3$ using the same methods used for $y_1$ and $y_2$.\nThis will reveal the $y$-intercept of $y_3$:\n:$b_3 = A Q^2 + C_2$\nThe result also follows for the Tangent line $\\tuple {-Q, \\map g {-Q} }$ on $g$ which is defined:\n:$y_4 = \\paren {-2 A Q + B} x + b_4$\nSolving for $b_4$ yields the result:\n:$b_4 = A Q^2 + C_2$\nThe $y$-intercepts of both $y_3$ and $y_4$ have been shown to be equivalent.\n{{qed|lemma}}\nNotice that the derivatives of $f$ and $g$ satisfy:\n{{begin-eqn}}\n{{eqn | l = \\map {g'} Q\n      | r = \\map {f'} {-Q}\n}}\n{{eqn | l = \\map {g'} {-Q}\n      | r = \\map {f'} Q\n}}\n{{end-eqn}}\nThen it must be true that:\n:$y_1 = y_4$ and $y_2 = y_3$\nand the functions $y_1$, $y_2$, $y_3$, and $y_4$ share the same $y$-intercept.\n{{qed|lemma}}\nRedefine this the $y$-intercepts of the tangent lines as $b$.\nSolve for $C_2$ to determine the vertical translation of $\\map g x$:\n{{begin-eqn}}\n{{eqn | l = C_2\n      | r = \\paren {-A Q^2 + C_1} - A Q^2\n}}\n{{eqn | l = C_2\n      | r = -2 A Q^2 + C_1\n}}\n{{end-eqn}}\nTherefore the function:\n:$\\map g x = -A x^2 + B x - \\paren {2 A Q^2 + C_1}$\nwill have tangent lines equivalent to the tangent lines on $\\map f x$ at the points $\\tuple {Q, \\map f Q}$, and $\\tuple {-Q, \\map f {-Q} }$.\n{{qed}}\nCategory:Analytic Geometry\nCategory:Tangents\n\\end{proof}\n\n"}}, "12951": {"score": 0.8713402152061462, "content": {"text": "\\section{Graph of Quadratic describes Parabola/Corollary 1}\nTags: Conic Sections, Parabolas, Graph of Quadratic describes Parabola, Quadratic Equations\n\n\\begin{theorem}\nThe locus of the equation of the square function:\n:$y = x^2$\ndescribes a parabola.\n\\end{theorem}\n\n\\begin{proof}\nThis is a particular instance of Graph of Quadratic describes Parabola, where:\n:$y = a x^2 + b x + c$\nis the equation of a parabola.\nThe result follows by setting $a = 1, b = 0, c = 0$.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Multinomial_6.json": {"gold": {"10462": 1, "19708": 1}, "retrieved": {"646": {"score": 0.8304091095924377, "content": {"text": "\\begin{definition}[Definition:Binomial Coefficient/Integers/Definition 2]\nLet $n \\in \\Z_{\\ge 0}$ and $k \\in \\Z$.\nThe number of different ways $k$ objects can be chosen (irrespective of order) from a set of $n$ objects is denoted:\n:$\\dbinom n k$\nThis number $\\dbinom n k$ is known as a '''binomial coefficient'''.\n\\end{definition}"}}, "5855": {"score": 0.8313411474227905, "content": {"text": "\\begin{definition}[Definition:Multinomial Coefficient]\nLet $k_1, k_2, \\ldots, k_m \\in \\Z_{\\ge 0}$ be positive integers.\nThe '''multinomial coefficient''' of $k_1, \\ldots, k_m$ is defined as:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} := \\dfrac {\\left({k_1 + k_2 + \\cdots + k_m}\\right)!} {k_1! \\, k_2! \\, \\ldots k_m!}$\n\\end{definition}"}}, "1332": {"score": 0.8443580865859985, "content": {"text": "\\begin{definition}[Definition:Combination]\nLet $S$ be a set containing $n$ elements.\nAn '''$r$-combination of $S$''' is a subset of $S$ which has $r$ elements.\n\\end{definition}"}}, "1337": {"score": 0.8396265506744385, "content": {"text": "\\begin{definition}[Definition:Combinatorics]\n'''Combinatorics''' is that branch of mathematics concerned with counting things.\n'''Combinatorial''' problems are so named because they are exercises in counting the number of combinations of various objects.\nIt has been stated that it is the core of the discipline of discrete mathematics.\n\\end{definition}"}}, "6877": {"score": 0.8329899907112122, "content": {"text": "\\begin{definition}[Definition:Permutation/Ordered Selection/Notation]\nThe number of $r$-permutations from a set of cardinality $n$ is denoted variously:\n:$P_{n r}$\n:${}^r P_n$\n:${}_r P_n$\n:${}_n P_r$ (extra confusingly)\nThere is little consistency in the literature).\nOn {{ProofWiki}} the notation of choice is ${}^r P_n$.\nCategory:Definitions/Permutation Theory\n\\end{definition}"}}, "17192": {"score": 0.871208906173706, "content": {"text": "\\section{Binomial Coefficient/Examples/Number of Bridge Hands}\nTags: Binomial Coefficients, Examples of Binomial Coefficients\n\n\\begin{theorem}\nThe total number $N$ of possible different hands for a game of [https://en.wikipedia.org/wiki/Contract_bridge bridge] is:\n:$N = \\dfrac {52!} {13! \\, 39!} = 635 \\ 013 \\ 559 \\ 600$\n\\end{theorem}\n\n\\begin{proof}\nThe total number of cards in a standard deck is $52$.\nThe number of cards in a single bridge hand is $13$.\nThus $N$ is equal to the number of ways $13$ things can be chosen from $52$.\nThus:\n{{begin-eqn}}\n{{eqn | l = N\n      | r = \\dbinom {52} {23}\n      | c = Cardinality of Set of Subsets\n}}\n{{eqn | r = \\frac {52!} {13! \\left({52 - 13}\\right)!}\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\frac {52!} {13! \\, 39!}\n      | c = \n}}\n{{eqn | r = 635 \\ 013 \\ 559 \\ 600\n      | c = after calculation\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "19708": {"score": 0.8491077423095703, "content": {"text": "\\section{Product Rule for Counting}\nTags: Product Rule for Counting, Counting Arguments, Combinatorics, combinatorics\n\n\\begin{theorem}\nLet it be possible to choose an element $\\alpha$ from a given set $S$ in $m$ different ways.\nLet it be possible to choose an element $\\beta$ from a given set $T$ in $n$ different ways.\nThen the ordered pair $\\tuple {\\alpha, \\beta}$ can be chosen from the cartesian product $S \\times T$ in $m n$ different ways.\n\\end{theorem}\n\n\\begin{proof}\n{{handwaving}}\nThe validity of this rule follows directly from the definition of multiplication of integers.\nThe product $a b$ (for $a, b \\in \\N_{>0}$) is the number of sequences $\\sequence {A, B}$, where $A$ can be any one of $a$ items and $B$ can be any one of $b$ items.\n{{qed}}\n\\end{proof}\n\n"}}, "16939": {"score": 0.8724231719970703, "content": {"text": "\\section{Cardinality of Set of Combinations with Repetition}\nTags: Combinations with Repetition\n\n\\begin{theorem}\nLet $S$ be a finite set with $n$ elements\nThe number of $k$-combinations of $S$ with repetition is given by:\n:$N = \\dbinom {n + k - 1} k$\n\\end{theorem}\n\n\\begin{proof}\nLet the elements of $S$ be (totally) ordered in some way, by assigning an index to each element.\nThus let $S = \\left\\{ {a_1, a_2, a_3, \\ldots, a_n}\\right\\}$.\nThus each $k$-combination of $S$ with repetition can be expressed as:\n:$\\left\\{ {\\left\\{ {a_{r_1}, a_{r_1}, \\ldots, a_{r_k} }\\right\\} }\\right\\}$\nwhere:\n:$r_1, r_2, \\ldots, r_k$ are all elements of $\\left\\{ {1, 2, \\ldots, n}\\right\\}$\nand such that:\n:$r_1 \\le r_2 \\le \\cdots \\le r_k$\nHence the problem reduces to the number of integer solutions $\\left({r_1, r_2, \\ldots, r_k}\\right)$ such that $1 \\le r_1 \\le r_2 \\le \\cdots \\le r_k \\le n$.\nThis is the same as the number of solutions to:\n:$0 < r_1 < r_2 + 1 < \\cdots < r_k + k - 1 < n + k$\nwhich is the same as the number of solutions to:\n:$0 < s_1 < s_2 < \\cdots < s_k < n + k$\nwhich is the number of ways to choose $k$ elements from $n + k - 1$.\nThis is the same as the number of subsets as a set with $n + k - 1$ elements.\nFrom Cardinality of Set of Subsets:\n:$N = \\dbinom {n + k - 1} k$\n{{qed}}\n\\end{proof}\n\n"}}, "10462": {"score": 0.8550077676773071, "content": {"text": "\\section{Multinomial Coefficient expressed as Product of Binomial Coefficients}\nTags: Multinomial Coefficients, Binomial Coefficients\n\n\\begin{theorem}\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} = \\dbinom {k_1 + k_2} {k_1} \\dbinom {k_1 + k_2 + k_3} {k_1 + k_2} \\cdots \\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1 + k_2 + \\cdots + k_{m - 1} }$\nwhere:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m}$ denotes a multinomial coefficient\n:$\\dbinom {k_1 + k_2} {k_1}$ etc. denotes binomial coefficients.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction.\nFor all $m \\in \\Z_{> 1}$, let $\\map P m$ be the proposition:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} = \\dbinom {k_1 + k_2} {k_1} \\dbinom {k_1 + k_2 + k_3} {k_1 + k_2} \\cdots \\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1 + k_2 + \\cdots + k_{m - 1} }$\n\\end{proof}\n\n"}}, "1333": {"score": 0.8621768355369568, "content": {"text": "\\begin{definition}[Definition:Combination with Repetition]\nLet $S$ be a (finite) set with $n$ elements.\nA '''$k$ combination of $S$ with repetition''' is a multiset with $k$ elements selected from $S$.\n\\end{definition}"}}}}, "TheoremQA_xueguangma/intermediate_value_theorem.json": {"gold": {"20761": 1, "12011": 1, "18841": 1, "12010": 1, "13939": 1, "17066": 1}, "retrieved": {"5714": {"score": 0.8564645648002625, "content": {"text": "\\begin{definition}[Definition:Minimum Value of Real Function/Local]\nLet $f$ be a real function defined on an open interval $\\openint a b$.\nLet $\\xi \\in \\openint a b$.\nThen $f$ has a '''local minimum at $\\xi$''' {{iff}}:\n:$\\exists \\openint c d \\subseteq \\openint a b: \\forall x \\in \\openint c d: \\map f x \\ge \\map f \\xi$\nThat is, {{iff}} there is some subinterval on which $f$ attains a minimum within that interval.\n\\end{definition}"}}, "5557": {"score": 0.8568359613418579, "content": {"text": "\\begin{definition}[Definition:Maximum Value of Real Function/Local]\nLet $f$ be a real function defined on an open interval $\\openint a b$.\nLet $\\xi \\in \\openint a b$.\nThen $f$ has a '''local maximum at $\\xi$''' {{iff}}:\n:$\\exists \\openint c d \\subseteq \\openint a b: \\forall x \\in \\openint c d: \\map f x \\le \\map f \\xi$\nThat is, {{iff}} there is some subinterval on which $f$ attains a maximum within that interval.\n\\end{definition}"}}, "23472": {"score": 0.8625953197479248, "content": {"text": "\\section{Zero Derivative implies Constant Function}\nTags: Differential Calculus, Constant Mappings\n\n\\begin{theorem}\nLet $f$ be a real function which is continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nSuppose that:\n:$\\forall x \\in \\openint a b: \\map {f'} x = 0$\nThen $f$ is constant on $\\closedint a b$.\n\\end{theorem}\n\n\\begin{proof}\nWhen $x = a$ then $\\map f x = \\map f a$ by definition of mapping.\nOtherwise, let $x \\in \\hointl a b$.\nWe have that:\n:$f$ is continuous on the closed interval $\\closedint a b$\n:$f$ is differentiable on the open interval $\\openint a b$\nHence it satisfies the conditions of the Mean Value Theorem on $\\closedint a b$.\nHence:\n:$\\exists \\xi \\in \\openint a x: \\map {f'} \\xi = \\dfrac {\\map f x - \\map f a} {x - a}$\nBut by our supposition:\n:$\\forall x \\in \\openint a b: \\map {f'} x = 0$\nwhich means:\n:$\\forall x \\in \\openint a b: \\map f x - \\map f a = 0$\nand hence:\n:$\\forall x \\in \\openint a b: \\map f x = \\map f a$\n{{qed}}\n\\end{proof}\n\n"}}, "16906": {"score": 0.8600965738296509, "content": {"text": "\\section{Cauchy Mean Value Theorem}\nTags: Differential Calculus, Named Theorems\n\n\\begin{theorem}\nLet $f$ and $g$ be real functions which are continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nSuppose:\n:$\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$\nThen:\n:$\\exists \\xi \\in \\openint a b: \\dfrac {\\map {f'} \\xi} {\\map {g'} \\xi} = \\dfrac {\\map f b - \\map f a} {\\map g b - \\map g a}$\n\\end{theorem}\n\n\\begin{proof}\nFirst we check $\\map g a \\ne \\map g b$.\n{{AimForCont}} $\\map g a = \\map g b$.\nFrom Rolle's Theorem:\n:$\\exists \\xi \\in \\openint a b: \\map {g'} \\xi = 0$.\nThis contradicts $\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$.\nThus by Proof by Contradiction $\\map g a \\ne \\map g b$.\nLet $h = \\dfrac {\\map f b - \\map f a} {\\map g b - \\map g a}$.\nLet $F$ be the real function defined on $\\closedint a b$ by:\n:$\\map F x = \\map f x - h \\map g x$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map F b - \\map F a\n      | r = \\paren {\\map f b - h \\map g b} - \\paren {\\map f a - h \\map g a}\n      | c = as $\\map F x = \\map f x - h \\map g x$\n}}\n{{eqn | r = \\paren {\\map f b - \\map f a} - h \\paren {\\map g b - \\map g a}\n}}\n{{eqn | r = 0\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map F a\n      | r = \\map F b\n}}\n{{eqn | ll= \\leadsto\n      | q = \\exists \\xi \\in \\openint a b\n      | l = \\map {F'} \\xi\n      | r = \\map {f'} \\xi - h \\map {g'} \\xi\n      | c = Sum Rule for Derivatives, Derivative of Constant Multiple\n}}\n{{eqn | r = 0\n      | c = Rolle's Theorem\n}}\n{{eqn | ll= \\leadsto\n      | q = \\exists \\xi \\in \\openint a b\n      | l = \\frac {\\map {f'} \\xi} {\\map {g'} \\xi}\n      | r = h\n      | c = $\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$\n}}\n{{eqn | r = \\frac {\\map f b - \\map f a} {\\map g b - \\map g a}\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "21398": {"score": 0.85847407579422, "content": {"text": "\\section{Sine and Cosine are Periodic on Reals}\nTags: Periodic Functions, Sine Function, Analysis, Cosine Function\n\n\\begin{theorem}\nThe sine and cosine functions are periodic on the set of real numbers $\\R$:\n:$(1): \\quad \\map \\cos {x + 2 \\pi} = \\cos x$\n:$(2): \\quad \\map \\sin {x + 2 \\pi} = \\sin x$\n:800px\n\\end{theorem}\n\n\\begin{proof}\nFrom Cosine of Zero is One we have that $\\cos 0 = 1$.\nFrom Cosine Function is Even we have that $\\cos x = \\map \\cos {-x}$.\nAs the Cosine Function is Continuous, it follows that:\n:$\\exists \\xi > 0: \\forall x \\in \\openint {-\\xi} \\xi: \\cos x > 0$\n{{AimForCont}} $\\cos x$ were positive everywhere on $\\R$.\nFrom Derivative of Cosine Function:\n:$\\map {D_{xx} } {\\cos x} = \\map {D_x} {-\\sin x} = -\\cos x$\nThus $-\\cos x$ would always be negative.\nThus from Second Derivative of Concave Real Function is Non-Positive, $\\cos x$ would be concave everywhere on $\\R$.\nBut from Real Cosine Function is Bounded, $\\cos x$ is bounded on $\\R$.\nBy Differentiable Bounded Concave Real Function is Constant, $\\cos x$ would then be a constant function.\nThis contradicts the fact that $\\cos x$ is not a constant function.\nThus by Proof by Contradiction $\\cos x$ can not be positive everywhere on $\\R$.\nTherefore, there must exist a smallest positive $\\eta \\in \\R_{>0}$ such that $\\cos \\eta = 0$.\nBy definition, $\\cos \\eta = \\map \\cos {-\\eta} = 0$ and $\\cos x > 0$ for $-\\eta < x < \\eta$.\nNow we show that $\\sin \\eta = 1$.\nFrom Sum of Squares of Sine and Cosine:\n:$\\cos^2 x + \\sin^2 x = 1$\nHence as $\\cos \\eta = 0$ it follows that $\\sin^2 \\eta = 1$.\nSo either $\\sin \\eta = 1$ or $\\sin \\eta = -1$.\nBut $\\map {D_x} {\\sin x} = \\cos x$.\nOn the interval $\\closedint {-\\eta} \\eta$, it has been shown that $\\cos x > 0$.\nThus by Derivative of Monotone Function, $\\sin x$ is increasing on $\\closedint {-\\eta} \\eta$.\nSince $\\sin 0 = 0$ it follows that $\\sin \\eta > 0$.\nSo it must be that $\\sin \\eta = 1$.\nNow we apply Sine of Sum and Cosine of Sum:\n:$\\map \\sin {x + \\eta} = \\sin x \\cos \\eta + \\cos x \\sin \\eta = \\cos x$\n:$\\map \\cos {x + \\eta} = \\cos x \\cos \\eta - \\sin x \\sin \\eta = -\\sin x$\nHence it follows, after some algebra, that:\n:$\\map \\sin {x + 4 \\eta} = \\sin x$\n:$\\map \\cos {x + 4 \\eta} = \\cos x$\nThus $\\sin$ and $\\cos$ are periodic on $\\R$ with period $4 \\eta$.\n{{qed}}\n\\end{proof}\n\n"}}, "10794": {"score": 0.8998649716377258, "content": {"text": "\\section{Mean Value Theorem}\nTags: Differential Calculus, Named Theorems, Mean Value Theorem\n\n\\begin{theorem}\nLet $f$ be a real function which is continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nThen:\n:$\\exists \\xi \\in \\openint a b: \\map {f'} \\xi = \\dfrac {\\map f b - \\map f a} {b - a}$\n\\end{theorem}\n\n\\begin{proof}\nFor any constant $h \\in \\R$ we may construct the real function defined on $\\closedint a b$ by:\n:$\\map F x = \\map f x + h x$\nWe have that $h x$ is continuous on $\\closedint a b$ from Linear Function is Continuous.\nFrom the Sum Rule for Continuous Functions, $F$ is continuous on $\\closedint a b$ and differentiable on $\\openint a b$.\nLet us calculate what the constant $h$ has to be such that $\\map F a = \\map F b$:\n{{begin-eqn}}\n{{eqn | l = \\map F a\n      | r = \\map F b\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map f a + h a\n      | r = \\map f b + h b\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map f a - \\map f b\n      | r = h b - h a\n      | c = rearranging\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map f a - \\map f b\n      | r = h \\paren {b - a}\n      | c = Real Multiplication Distributes over Real Addition\n}}\n{{eqn | ll= \\leadsto\n      | l = h\n      | r = -\\dfrac {\\map f b - \\map f a} {b - a}\n      | c = rearranging\n}}\n{{end-eqn}}\nSince $F$ satisfies the conditions for the application of Rolle's Theorem:\n:$\\exists \\xi \\in \\openint a b: \\map {F'} \\xi = 0$\nBut then:\n:$\\map {F'} \\xi = \\map {f'} \\xi + h = 0$\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "12010": {"score": 0.8711596727371216, "content": {"text": "\\section{Intermediate Value Theorem}\nTags: Proofs, Named Theorems, Analysis\n\n\\begin{theorem}\nLet $f: S \\to \\R$ be a real function on some subset $S$ of $\\R$.\nLet $I \\subseteq S$ be a real interval.\nLet $f: I \\to \\R$ be continuous on $I$.\nThen $f$ is a Darboux function.\nThat is:\nLet $a, b \\in I$.\nLet $k \\in \\R$ lie between $\\map f a$ and $\\map f b$.\nThat is, either:\n:$\\map f a < k < \\map f b$\nor:\n:$\\map f b < k < \\map f a$\nThen $\\exists c \\in \\openint a b$ such that $\\map f c = k$.\n\\end{theorem}\n\n\\begin{proof}\nThis theorem is a restatement of Image of Interval by Continuous Function is Interval.\nFrom Image of Interval by Continuous Function is Interval, the image of $\\openint a b$ under $f$ is also a real interval (but not necessarily open).\nThus if $k$ lies between $\\map f a$ and $\\map f b$, it must be the case that:\n:$k \\in \\Img {\\openint a b}$\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "12011": {"score": 0.9018237590789795, "content": {"text": "\\section{Intermediate Value Theorem/Corollary}\nTags: Named Theorems, Analysis\n\n\\begin{theorem}\nLet $I$ be a real interval.\nLet $a, b \\in I$ such that $\\openint a b$ is an open interval.\nLet $f: I \\to \\R$ be a real function which is continuous on $\\openint a b$.\nLet $0 \\in \\R$ lie between $\\map f a$ and $\\map f b$.\nThat is, either:\n:$\\map f a < 0 < \\map f b$\nor:\n:$\\map f b < 0 < \\map f a$\nThen $f$ has a root in $\\openint a b$.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from the Intermediate Value Theorem and from the definition of root.\n{{qed}}\n\\end{proof}\n\n"}}, "12013": {"score": 0.8824609518051147, "content": {"text": "\\section{Intermediate Value Theorem for Derivatives}\nTags: Real Analysis, Analysis\n\n\\begin{theorem}\nLet $I$ be an open interval.\nLet $f : I \\to \\R$ be everywhere differentiable.\nThen $f'$ satisfies the Intermediate Value Property.\n\\end{theorem}\n\n\\begin{proof}\nSince $\\forall \\set {a, b \\in I: a < b}: \\openint a b \\subseteq I$, the result follows from Image of Interval by Derivative.\n{{qed}}\nCategory:Real Analysis\n\\end{proof}\n\n"}}, "20759": {"score": 0.8864362835884094, "content": {"text": "\\section{Rolle's Theorem}\nTags: Continuity, Differential Calculus, Continuous Real Functions, Rolle's Theorem, Named Theorems, Continuous Functions, Differentiable Real Functions, Differential Real Functions\n\n\\begin{theorem}\nLet $f$ be a real function which is:\n:continuous on the closed interval $\\closedint a b$\nand:\n:differentiable on the open interval $\\openint a b$.\nLet $\\map f a = \\map f b$.\nThen:\n:$\\exists \\xi \\in \\openint a b: \\map {f'} \\xi = 0$\n\\end{theorem}\n\n\\begin{proof}\nWe have that $f$ is continuous on $\\closedint a b$.\nIt follows from Continuous Image of Closed Interval is Closed Interval that $f$ attains:\n:a maximum $M$ at some $\\xi_1 \\in \\closedint a b$\nand:\n:a minimum $m$ at some $\\xi_2 \\in \\closedint a b$.\nSuppose $\\xi_1$ and $\\xi_2$ are both end points of $\\closedint a b$.\nBecause $\\map f a = \\map f b$ it follows that $m = M$ and so $f$ is constant on $\\closedint a b$.\nThen, by Derivative of Constant, $\\map {f'} \\xi = 0$ for all $\\xi \\in \\openint a b$.\nSuppose $\\xi_1$ is not an end point of $\\closedint a b$.\nThen $\\xi_1 \\in \\openint a b$ and $f$ has a local maximum at $\\xi_1$.\nHence the result follows from Derivative at Maximum or Minimum.\nSimilarly, suppose $\\xi_2$ is not an end point of $\\closedint a b$.\nThen $\\xi_2 \\in \\openint a b$ and $f$ has a local minimum at $\\xi_2$.\nHence the result follows from Derivative at Maximum or Minimum.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_elainewan/math_algebra_4.json": {"gold": {"9996": 1, "17877": 1, "11116": 1}, "retrieved": {"17877": {"score": 0.8585755228996277, "content": {"text": "\\section{Null Space is Subspace}\nTags: Linear Algebra, Null Spaces\n\n\\begin{theorem}\nLet:\n:$\\map {\\mathrm N} {\\mathbf A} = \\set {\\mathbf x \\in \\R^n: \\mathbf {A x} = \\mathbf 0}$\nbe the null space of $\\mathbf A$, where:\n:$\\mathbf A_{m \\times n} = \\begin {bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end {bmatrix}$,  $\\mathbf x_{n \\times 1} = \\begin {bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end {bmatrix}$, $\\mathbf 0_{m \\times 1} = \\begin {bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end {bmatrix}$\nare matrices.\nThen $\\map {\\mathrm N} {\\mathbf A}$ is a linear subspace of $\\R^n$.\n\\end{theorem}\n\n\\begin{proof}\n$\\map {\\mathrm N} {\\mathbf A} \\subseteq \\R^n$, by construction.\nWe have:\n:$\\mathbf 0 \\in \\map {\\mathrm N} {\\mathbf A}$, from Null Space Contains Zero Vector\n:$\\forall \\mathbf v, \\mathbf w \\in \\map {\\mathrm N} {\\mathbf A}: \\mathbf v + \\mathbf w \\in \\map {\\mathrm N} {\\mathbf A}$, from Null Space Closed under Vector Addition\n:$\\forall \\mathbf v \\in \\map {\\mathrm N} {\\mathbf A}, \\lambda \\in \\R: \\lambda \\mathbf v \\in \\map {\\mathrm N} {\\mathbf A}$, from Null Space Closed under Scalar Multiplication\nThe result follows from Vector Subspace of Real Vector Space.\n{{qed}}\n\\end{proof}\n\n"}}, "1329": {"score": 0.8609825968742371, "content": {"text": "\\begin{definition}[Definition:Column Space]\nLet $R$ be a ring.\nLet:\n:$\\mathbf A_{m \\times n} = \\begin{bmatrix}\na_{1 1} & a_{1 2} & \\cdots & a_{1 n} \\\\\na_{2 1} & a_{2 2} & \\cdots & a_{2 n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m 1} & a_{m 2} & \\cdots & a_{m n} \\\\\n\\end{bmatrix}$\nbe a matrix over $R$ such that every column is defined as a vector:\n:$\\forall i: 1 \\le i \\le m: \\begin {bmatrix} a_{1 i} \\\\ a_{2 i} \\\\ \\vdots \\\\ a_{m i} \\end {bmatrix} \\in \\mathbf V$\nwhere $\\mathbf V$ is some vector space.\nThen the '''column space of $\\mathbf A$''' is the linear span of all such column vectors:\n:$\\map {\\mathrm C} {\\mathbf A} = \\map \\span {\\begin {bmatrix} a_{1 1} \\\\ a_{2 1} \\\\ \\vdots \\\\ a_{m 1} \\end {bmatrix}, \\begin {bmatrix} a_{1 2} \\\\ a_{2 2} \\\\ \\vdots \\\\ a_{m 2} \\end {bmatrix}, \\cdots, \\begin {bmatrix} a_{1 n} \\\\ a_{2 n} \\\\ \\vdots \\\\ a_{ mn} \\end {bmatrix} }$\n\\end{definition}"}}, "17875": {"score": 0.8654496073722839, "content": {"text": "\\section{Null Space Contains Only Zero Vector iff Columns are Independent}\nTags: Linear Algebra, Null Spaces\n\n\\begin{theorem}\nLet:\n{{begin-eqn}}\n{{eqn | l = \\mathbf A_{m \\times n}\n      | r = \\begin{bmatrix} \\mathbf a_1 & \\mathbf a_2 & \\cdots & \\mathbf a_n \\end{bmatrix}\n}}\n{{end-eqn}}\nbe a matrix where:\n:$\\forall i: 1 \\le i \\le n: \\mathbf a_i = \\begin{bmatrix} a_{1i} \\\\ a_{2i} \\\\ \\vdots \\\\ a_{mi} \\end{bmatrix} \\in \\R^m$\nare vectors.\nThen:\n:$\\set {\\mathbf a_1, \\mathbf a_2, \\cdots, \\mathbf a_n}$ is a linearly independent set \n{{iff}}:\n:$\\map {\\mathrm N} {\\mathbf A} = \\set {\\mathbf 0_{n \\times 1} }$\nwhere $\\map {\\mathrm N} {\\mathbf A}$ is the null space of $\\mathbf A$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\mathbf x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\R^m$.\nWe have that:\n{{begin-eqn}}\n{{eqn | l = \\mathbf x\n      | o = \\in \n      | r = \\map {\\mathrm N} {\\mathbf A}\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\mathbf A \\mathbf x_{n \\times 1}\n      | r = \\mathbf 0_{m \\times 1}\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\begin{bmatrix} \\mathbf a_1 & \\mathbf a_2 & \\cdots & \\mathbf a_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n      | r = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\sum_{k \\mathop = 1}^n x_k \\mathbf a_k\n      | r = \\mathbf 0\n}}\n{{end-eqn}}\n\\end{proof}\n\n"}}, "17925": {"score": 0.8649130463600159, "content": {"text": "\\section{Number of Matrix Equivalence Classes}\nTags: Matrix Algebra\n\n\\begin{theorem}\nLet $K$ be a field.\nLet $\\map {\\MM_K} {m, n}$ be the $m \\times n$ matrix space over $K$.\nLet $\\mathbf A$ be an $m \\times n$ matrix of rank $r$ over $K$.\nThen:\n:$\\mathbf A \\equiv \\begin{cases}\n\\sqbrk {0_K}_{m n} & : r = 0 \\\\\n& \\\\\n\\begin{bmatrix}\n  \\mathbf I_r & \\bszero \\\\\n  \\bszero & \\bszero \n\\end{bmatrix} & : 0 < r < \\min \\set {n, m} \\\\\n& \\\\\n\\begin{bmatrix}\n  \\mathbf I_r & \\bszero \n\\end{bmatrix} & : r = m < n \\\\\n& \\\\\n\\begin{bmatrix}\n  \\mathbf I_r \\\\\n  \\bszero \n\\end{bmatrix} & : r = n < m \\\\\n& \\\\\n\\mathbf I_r & : r = m = n\n\\end{cases}$\nThus there are exactly $\\min \\set {m, n} + 1$ equivalence classes for the relation of equivalence on $\\map {\\MM_K} {m, n}$, one of which contains only the zero matrix.\n\\end{theorem}\n\n\\begin{proof}\nFollows from Equivalent Matrices have Equal Rank.\n{{qed}}\n\\end{proof}\n\n"}}, "20780": {"score": 0.8642573952674866, "content": {"text": "\\section{Row Equivalent Matrix for Homogeneous System has same Solutions/Corollary}\nTags: Linear Algebra, Matrix Theory\n\n\\begin{theorem}\nLet $\\mathbf A$ be a matrix in the matrix space $\\map {\\MM_\\R} {m, n}$ such that:\n:$\\mathbf A \\mathbf x = \\mathbf 0$\nrepresents a homogeneous system of linear equations.\nThen:\n:$\\set {\\mathbf x: \\mathbf A \\mathbf x = \\mathbf 0} = \\set {\\mathbf x: \\map {\\mathrm {ref} } {\\mathbf A} \\mathbf x = \\mathbf 0}$\nwhere $\\map {\\mathrm {ref} } {\\mathbf A}$ is the reduced echelon form of $\\mathbf A$.\n\\end{theorem}\n\n\\begin{proof}\nFollows from Row Equivalent Matrix for Homogeneous System has same Solutions and from Matrix is Row Equivalent to Reduced Echelon Matrix.\n{{qed}}\nCategory:Matrix Theory\nCategory:Linear Algebra\n\\end{proof}\n\n"}}, "20779": {"score": 0.8729716539382935, "content": {"text": "\\section{Row Equivalent Matrix for Homogeneous System has same Solutions}\nTags: Linear Algebra\n\n\\begin{theorem}\nLet $\\mathbf A$ be a matrix in the matrix space $\\map {\\MM_\\R} {m, n}$ such that:\n:$\\mathbf A \\mathbf x = \\mathbf 0$\nrepresents a homogeneous system of linear equations.\nLet $\\mathbf H$ be row equivalent to $\\mathbf A$.\nThen the solution set of $\\mathbf H \\mathbf x = \\mathbf 0$ equals the solution set of $\\mathbf A \\mathbf x = \\mathbf 0$.\nThat is:\n:$\\mathbf A \\sim \\mathbf H \\implies \\set {\\mathbf x: \\mathbf A \\mathbf x = \\mathbf 0} = \\set {\\mathbf x: \\mathbf H \\mathbf x = \\mathbf 0}$\nwhere $\\sim$ represents row equivalence.\n\\end{theorem}\n\n\\begin{proof}\nLet:\n{{begin-eqn}}\n{{eqn | l = \\alpha_{1 1} x_1 + \\alpha_{1 2} x_2 + \\ldots + \\alpha_{1 n} x_n\n      | r = 0\n      | c = \n}}\n{{eqn | l = \\alpha_{2 1} x_1 + \\alpha_{2 2} x_2 + \\ldots + \\alpha_{2 n} x_n\n      | r = 0\n      | c = \n}}\n{{eqn | o = \\vdots\n}}\n{{eqn | l = \\alpha_{m 1} x_1 + \\alpha_{m 2} x_2 + \\ldots + \\alpha_{m n} x_n\n      | r = 0\n      | c = \n}}\n{{end-eqn}}\nbe the system of equations to be solved.\nSuppose the elementary row operation of multiplying one row $i$ by a non-zero scalar $\\lambda$ is performed.\nRecall, the $i$th row of the matrix represents the $i$th equation of the system to be solved.\nThen this is logically equivalent to multiplying the $i$th equation on both sides by the scalar $\\lambda$:\n{{begin-eqn}}\n{{eqn | l = \\alpha_{i 1} x_1 + \\alpha_{i 2} x_2 + \\ldots + \\alpha_{i n} x_n\n      | r = 0\n}}\n{{eqn | ll= \\to\n      | l = \\lambda \\alpha_{i 1} x_1 + \\lambda \\alpha_{i 2} x_2 + \\ldots + \\lambda \\alpha_{i n} x_n\n      | r = 0\n      | c = $r_i \\to \\lambda r_i$\n}}\n{{end-eqn}}\nwhich clearly has the same solutions as the original equation.\nSuppose the elementary row operation of adding a scalar multiple of row $i$ to another row $j$ is performed.\nRecall that the $i$th and $j$th row of the matrix represent the $i$th and $j$th equation in the system to be solved.\n{{explain|Woolly. The matrix (by which I presume you mean $\\mathbf A$) contains the coefficients and so no part of it \"represents\" an equation. The act of multiplying $\\mathbf x$ by it to obtain $\\mathbf b$ represents the equation.}}\nThus this is logically equivalent to manipulating the $i$th and $j$th equations as such:\n{{begin-eqn}}\n{{eqn | l = \\alpha_{i 1} x_1 + \\alpha_{i 2} x_2 + \\ldots + \\alpha_{i n} x_n\n      | r = 0\n      | c = \n}}\n{{eqn | l = \\alpha_{j 1} x_1 + \\alpha_{j 2} x_2 + \\ldots + \\alpha_{j n} x_n\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\to\n      | l = \\alpha_{j 1} x_1 + \\alpha_{j 2} x_2 + \\ldots + \\alpha_{j n} x_n + \\lambda \\paren {\\alpha_{i 1} x_1 + \\alpha_{i 2} x_2 + \\ldots + \\alpha_{i n} x_n}\n      | r = 0\n      | c = $r_i \\to r_i + \\lambda r_j$\n}}\n{{end-eqn}}\nAs both sides of equation $i$ are equal to each other, this operation is simply performing the same act on both sides of equation $j$.\nThis clearly will have no effect on the solution set of the system of equations.\nSuppose the elementary row operation of interchanging row $i$ and row $j$ is performed.\nRecall that the $i$th and $j$th row of the matrix represent the $i$th and $j$th equation in the system to be solved.\nThen, interchanging row $i$ and row $j$ is logically equivalent to switching the $i$th equation and the $j$th equation of the system to be solved.\nBut clearly the system containing the following two equations:\n{{begin-eqn}}\n{{eqn | l = \\alpha_{i 1} x_1 + \\alpha_{i 2} x_2 + \\cdots + \\alpha_{i n} x_n\n      | r = 0\n      | c = \n}}\n{{eqn | l = \\alpha_{j 1} x_1 + \\alpha_{j 2} x_2 + \\cdots + \\alpha_{j n} x_n\n      | r = 0\n      | c = \n}}\n{{end-eqn}}\nhas the same solution set as a system instead containing the following two equations:\n{{begin-eqn}}\n{{eqn | l = \\alpha_{j 1} x_1 + \\alpha_{j 2} x_2 + \\cdots + \\alpha_{j n} x_n\n      | r = 0\n      | c = \n}}\n{{eqn | l = \\alpha_{i 1} x_1 + \\alpha_{i 2} x_2 + \\cdots + \\alpha_{i n} x_n\n      | r = 0\n      | c = $r_i \\leftrightarrow r_j$\n}}\n{{end-eqn}}\nHence the result, by the definition of row equivalence.\n{{qed}}\n{{proofread}}\nCategory:Linear Algebra\n\\end{proof}\n\n"}}, "8138": {"score": 0.8664409518241882, "content": {"text": "\\begin{definition}[Definition:Row Operation]\nLet $\\mathbf A = \\sqbrk a_{m n}$ be an $m \\times n$ matrix over a field $K$.\nA '''row operation''' on $\\mathbf A$ is a sequence of '''elementary row operations''' performed on $\\mathbf A$ in turn.\n\\end{definition}"}}, "8136": {"score": 0.8735967874526978, "content": {"text": "\\begin{definition}[Definition:Row Equivalence]\nTwo matrices $\\mathbf A = \\sqbrk a_{m n}, \\mathbf B = \\sqbrk b_{m n}$ are '''row equivalent''' if one can be obtained from the other by a finite sequence of elementary row operations.\nThis relationship can be denoted $\\mathbf A \\sim \\mathbf B$.\n\\end{definition}"}}, "1326": {"score": 0.869075357913971, "content": {"text": "\\begin{definition}[Definition:Column Equivalence]\nTwo matrices $\\mathbf A = \\sqbrk a_{m n}, \\mathbf B = \\sqbrk b_{m n}$ are '''column equivalent''' if one can be obtained from the other by a finite sequence of elementary column operations.\nThis relationship can be denoted $\\mathbf A \\sim \\mathbf B$.\n\\end{definition}"}}, "17878": {"score": 0.8690847158432007, "content": {"text": "\\section{Null Space of Reduced Echelon Form}\nTags: Linear Algebra, Matrix Algebra, Echelon Matrices, Null Spaces\n\n\\begin{theorem}\nLet $\\mathbf A$ be a matrix in the matrix space $\\map {\\MM_\\R} {m, n}$ such that:\n:$\\mathbf A \\mathbf x = \\mathbf 0$\nrepresents a homogeneous system of linear equations.\nThe null space of $\\mathbf A$ is the same as that of the null space of the reduced row echelon form of $\\mathbf A$:\n:$\\map {\\mathrm N} {\\mathbf A} = \\map {\\mathrm N} {\\map {\\mathrm {rref} } {\\mathbf A} }$\n\\end{theorem}\n\n\\begin{proof}\nBy the definition of null space:\n:$\\mathbf x \\in \\map {\\mathrm N} {\\mathbf A} \\iff \\mathbf A \\mathbf x = \\mathbf 0$\nFrom the corollary to Row Equivalent Matrix for Homogeneous System has same Solutions:\n:$\\mathbf A \\mathbf x = \\mathbf 0 \\iff \\map {\\mathrm {rref} } {\\mathbf A} \\mathbf x = \\mathbf 0$\nHence the result, by the definition of set equality.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/Rolle's_theorem.json": {"gold": {"20759": 1, "13833": 1, "10794": 1}, "retrieved": {"16906": {"score": 0.8934398293495178, "content": {"text": "\\section{Cauchy Mean Value Theorem}\nTags: Differential Calculus, Named Theorems\n\n\\begin{theorem}\nLet $f$ and $g$ be real functions which are continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nSuppose:\n:$\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$\nThen:\n:$\\exists \\xi \\in \\openint a b: \\dfrac {\\map {f'} \\xi} {\\map {g'} \\xi} = \\dfrac {\\map f b - \\map f a} {\\map g b - \\map g a}$\n\\end{theorem}\n\n\\begin{proof}\nFirst we check $\\map g a \\ne \\map g b$.\n{{AimForCont}} $\\map g a = \\map g b$.\nFrom Rolle's Theorem:\n:$\\exists \\xi \\in \\openint a b: \\map {g'} \\xi = 0$.\nThis contradicts $\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$.\nThus by Proof by Contradiction $\\map g a \\ne \\map g b$.\nLet $h = \\dfrac {\\map f b - \\map f a} {\\map g b - \\map g a}$.\nLet $F$ be the real function defined on $\\closedint a b$ by:\n:$\\map F x = \\map f x - h \\map g x$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map F b - \\map F a\n      | r = \\paren {\\map f b - h \\map g b} - \\paren {\\map f a - h \\map g a}\n      | c = as $\\map F x = \\map f x - h \\map g x$\n}}\n{{eqn | r = \\paren {\\map f b - \\map f a} - h \\paren {\\map g b - \\map g a}\n}}\n{{eqn | r = 0\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map F a\n      | r = \\map F b\n}}\n{{eqn | ll= \\leadsto\n      | q = \\exists \\xi \\in \\openint a b\n      | l = \\map {F'} \\xi\n      | r = \\map {f'} \\xi - h \\map {g'} \\xi\n      | c = Sum Rule for Derivatives, Derivative of Constant Multiple\n}}\n{{eqn | r = 0\n      | c = Rolle's Theorem\n}}\n{{eqn | ll= \\leadsto\n      | q = \\exists \\xi \\in \\openint a b\n      | l = \\frac {\\map {f'} \\xi} {\\map {g'} \\xi}\n      | r = h\n      | c = $\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$\n}}\n{{eqn | r = \\frac {\\map f b - \\map f a} {\\map g b - \\map g a}\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "23472": {"score": 0.898453950881958, "content": {"text": "\\section{Zero Derivative implies Constant Function}\nTags: Differential Calculus, Constant Mappings\n\n\\begin{theorem}\nLet $f$ be a real function which is continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nSuppose that:\n:$\\forall x \\in \\openint a b: \\map {f'} x = 0$\nThen $f$ is constant on $\\closedint a b$.\n\\end{theorem}\n\n\\begin{proof}\nWhen $x = a$ then $\\map f x = \\map f a$ by definition of mapping.\nOtherwise, let $x \\in \\hointl a b$.\nWe have that:\n:$f$ is continuous on the closed interval $\\closedint a b$\n:$f$ is differentiable on the open interval $\\openint a b$\nHence it satisfies the conditions of the Mean Value Theorem on $\\closedint a b$.\nHence:\n:$\\exists \\xi \\in \\openint a x: \\map {f'} \\xi = \\dfrac {\\map f x - \\map f a} {x - a}$\nBut by our supposition:\n:$\\forall x \\in \\openint a b: \\map {f'} x = 0$\nwhich means:\n:$\\forall x \\in \\openint a b: \\map f x - \\map f a = 0$\nand hence:\n:$\\forall x \\in \\openint a b: \\map f x = \\map f a$\n{{qed}}\n\\end{proof}\n\n"}}, "21788": {"score": 0.9012117385864258, "content": {"text": "\\section{Steiner's Calculus Problem}\nTags: Euler's Number\n\n\\begin{theorem}\nLet $f: \\R_{>0} \\to \\R$ be the real function defined as:\n:$\\forall x \\in \\R_{>0}: \\map f x = x^{1/x}$\nThen $\\map f x$ reaches its maximum at $x = e$ where $e$ is Euler's number .\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\map {f'} x\n      | r = \\frac \\d {\\d x} x^{1/x}\n}}\n{{eqn | r = \\frac \\d {\\d x} e^{\\ln x / x}\n}}\n{{eqn | r = e^{\\ln x / x} \\paren {\\frac 1 {x^2} - \\frac {\\ln x} {x^2} }\n}}\n{{eqn | r = \\frac {x^{1/x} } {x^2} \\paren {1 - \\ln x}\n}}\n{{end-eqn}}\n$\\dfrac {x^{1/x} } {x^2}$ is always greater than $0$.\nTherefore:\n:$\\map {f'} x > 0$ for $\\ln x < 1$\n:$\\map {f'} x = 0$ for $\\ln x = 1$\n:$\\map {f'} x < 0$ for $\\ln x > 1$\nBy Derivative at Maximum or Minimum, maximum is obtained when $\\ln x = 1$,\nthat is, when $x = e$.\n{{qed}}\n{{Namedfor|Jakob Steiner|cat = Steiner}}\n\\end{proof}\n\n"}}, "12011": {"score": 0.9006659984588623, "content": {"text": "\\section{Intermediate Value Theorem/Corollary}\nTags: Named Theorems, Analysis\n\n\\begin{theorem}\nLet $I$ be a real interval.\nLet $a, b \\in I$ such that $\\openint a b$ is an open interval.\nLet $f: I \\to \\R$ be a real function which is continuous on $\\openint a b$.\nLet $0 \\in \\R$ lie between $\\map f a$ and $\\map f b$.\nThat is, either:\n:$\\map f a < 0 < \\map f b$\nor:\n:$\\map f b < 0 < \\map f a$\nThen $f$ has a root in $\\openint a b$.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from the Intermediate Value Theorem and from the definition of root.\n{{qed}}\n\\end{proof}\n\n"}}, "10775": {"score": 0.8994014859199524, "content": {"text": "\\section{Maximum Abscissa for Loop of Folium of Descartes}\nTags: Folium of Descartes\n\n\\begin{theorem}\nConsider the folium of Descartes defined in parametric form as:\n:$\\begin {cases} x = \\dfrac {3 a t} {1 + t^3} \\\\ y = \\dfrac {3 a t^2} {1 + t^3} \\end {cases}$\n:500px\nThe point on the loop at which the $x$ value is at a maximum occurs when $t = \\sqrt [3] {\\dfrac 1 2}$, corresponding to the point $P$ defined as:\n:$P = \\tuple {2^{2/3} a, 2^{1/3} a}$\n\\end{theorem}\n\n\\begin{proof}\nWe calculate the derivative of $x$ {{WRT|Differentiation}} $t$:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {\\d x} {\\d t}\n      | r = \\map {\\dfrac \\d {\\d t} } {\\dfrac {3 a t} {1 + t^3} }\n      | c = \n}}\n{{eqn | r = \\dfrac {\\paren {1 + t^3} \\times 3 a - 3 a t \\paren {3 t^2} } {\\paren {1 + t^3}^2}\n      | c = Quotient Rule for Derivatives\n}}\n{{eqn | r = \\dfrac {3 a - 6 a t^3} {\\paren {1 + t^3}^2}\n      | c = simplifying\n}}\n{{end-eqn}}\nThus $x$ is stationary when:\n{{begin-eqn}}\n{{eqn | l = 3 a - 6 a t^3\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = t\n      | r = \\paren {\\dfrac 1 2}^{1/3}\n      | c = \n}}\n{{end-eqn}}\nFrom Behaviour of Parametric Equations for Folium of Descartes according to Parameter, it is clear from the geometry that $x$ is a local maximum for this value of $t$.\nThen we have:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = \\dfrac {3 a \\times \\paren {1/2}^{1/3} } {1 + \\paren {\\paren {1/2}^{1/3} }^3}\n      | c = \n}}\n{{eqn | r = \\dfrac {3 a \\times \\paren {1/2}^{1/3} } {1 + 1/2}\n      | c = \n}}\n{{eqn | r = 2 a \\times \\paren {\\dfrac 1 2}^{1/3}\n      | c = \n}}\n{{eqn | r = \\paren {\\dfrac {2^3} 2}^{1/3} a\n      | c = \n}}\n{{eqn | r = 2^{2/3} a\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = y\n      | r = \\dfrac {3 a \\times \\paren {\\paren {1/2}^{1/3} }^2} {1 + \\paren {\\paren {1/2}^{1/3} }^3}\n      | c = \n}}\n{{eqn | r = \\dfrac {3 a \\times \\paren {1/2}^{2/3} } {1 + 1/2}\n      | c = \n}}\n{{eqn | r = 2 a \\times \\paren {\\dfrac 1 2}^{2/3}\n      | c = \n}}\n{{eqn | r = \\paren {\\dfrac {2^3} {2^2} }^{1/3} a\n      | c = \n}}\n{{eqn | r = 2^{1/3} a\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\nCategory:Folium of Descartes\n\\end{proof}\n\n"}}, "10794": {"score": 0.9155281782150269, "content": {"text": "\\section{Mean Value Theorem}\nTags: Differential Calculus, Named Theorems, Mean Value Theorem\n\n\\begin{theorem}\nLet $f$ be a real function which is continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nThen:\n:$\\exists \\xi \\in \\openint a b: \\map {f'} \\xi = \\dfrac {\\map f b - \\map f a} {b - a}$\n\\end{theorem}\n\n\\begin{proof}\nFor any constant $h \\in \\R$ we may construct the real function defined on $\\closedint a b$ by:\n:$\\map F x = \\map f x + h x$\nWe have that $h x$ is continuous on $\\closedint a b$ from Linear Function is Continuous.\nFrom the Sum Rule for Continuous Functions, $F$ is continuous on $\\closedint a b$ and differentiable on $\\openint a b$.\nLet us calculate what the constant $h$ has to be such that $\\map F a = \\map F b$:\n{{begin-eqn}}\n{{eqn | l = \\map F a\n      | r = \\map F b\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map f a + h a\n      | r = \\map f b + h b\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map f a - \\map f b\n      | r = h b - h a\n      | c = rearranging\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map f a - \\map f b\n      | r = h \\paren {b - a}\n      | c = Real Multiplication Distributes over Real Addition\n}}\n{{eqn | ll= \\leadsto\n      | l = h\n      | r = -\\dfrac {\\map f b - \\map f a} {b - a}\n      | c = rearranging\n}}\n{{end-eqn}}\nSince $F$ satisfies the conditions for the application of Rolle's Theorem:\n:$\\exists \\xi \\in \\openint a b: \\map {F'} \\xi = 0$\nBut then:\n:$\\map {F'} \\xi = \\map {f'} \\xi + h = 0$\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "15195": {"score": 0.9025890827178955, "content": {"text": "\\section{Derivative at Maximum or Minimum}\nTags: Differential Calculus, Derivative at Maximum or Minimum\n\n\\begin{theorem}\nLet $f$ be a real function which is differentiable on the open interval $\\openint a b$.\nLet $f$ have a local minimum or local maximum at $\\xi \\in \\openint a b$.\nThen:\n:$\\map {f'} \\xi = 0$\n\\end{theorem}\n\n\\begin{proof}\nBy definition of derivative at a point:\n:$\\dfrac {\\map f x - \\map f \\xi} {x - \\xi} \\to \\map {f'} \\xi$ as $x \\to \\xi$\nSuppose $\\map {f'} \\xi > 0$.\nThen from Behaviour of Function Near Limit it follows that:\n:$\\exists I = \\openint {\\xi - h} {\\xi + h}: \\dfrac {\\map f x - \\map f \\xi} {x - \\xi} > 0$\nprovided that $x \\in I$ and $x \\ne \\xi$.\nNow let $x_1$ be any number in the open interval $\\openint {\\xi - h} \\xi$.\nThen:\n:$x_1 - \\xi < 0$\nand hence from:\n:$\\dfrac {\\map f {x_1} - \\map f \\xi} {x_1 - \\xi} > 0$\nit follows that:\n:$\\map f {x_1} < \\map f \\xi$\nThus $f$ can not have a local minimum at $\\xi$.\nNow let $x_2$ be any number in the open interval $\\openint \\xi {\\xi + h}$.\nThen:\n:$x_2 - \\xi > 0$\nand hence from:\n:$\\dfrac {\\map f {x_2} - \\map f \\xi} {x_2 - \\xi} > 0$\nit follows that:\n:$\\map f {x_2} > \\map f \\xi$\nThus $f$ can not have a local maximum at $\\xi$ either.\nA similar argument can be applied to $-f$ to handle the case where $\\map {f'} \\xi < 0$.\nThe only other possibility is that $\\map {f'} \\xi = 0$, hence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "20759": {"score": 0.921074390411377, "content": {"text": "\\section{Rolle's Theorem}\nTags: Continuity, Differential Calculus, Continuous Real Functions, Rolle's Theorem, Named Theorems, Continuous Functions, Differentiable Real Functions, Differential Real Functions\n\n\\begin{theorem}\nLet $f$ be a real function which is:\n:continuous on the closed interval $\\closedint a b$\nand:\n:differentiable on the open interval $\\openint a b$.\nLet $\\map f a = \\map f b$.\nThen:\n:$\\exists \\xi \\in \\openint a b: \\map {f'} \\xi = 0$\n\\end{theorem}\n\n\\begin{proof}\nWe have that $f$ is continuous on $\\closedint a b$.\nIt follows from Continuous Image of Closed Interval is Closed Interval that $f$ attains:\n:a maximum $M$ at some $\\xi_1 \\in \\closedint a b$\nand:\n:a minimum $m$ at some $\\xi_2 \\in \\closedint a b$.\nSuppose $\\xi_1$ and $\\xi_2$ are both end points of $\\closedint a b$.\nBecause $\\map f a = \\map f b$ it follows that $m = M$ and so $f$ is constant on $\\closedint a b$.\nThen, by Derivative of Constant, $\\map {f'} \\xi = 0$ for all $\\xi \\in \\openint a b$.\nSuppose $\\xi_1$ is not an end point of $\\closedint a b$.\nThen $\\xi_1 \\in \\openint a b$ and $f$ has a local maximum at $\\xi_1$.\nHence the result follows from Derivative at Maximum or Minimum.\nSimilarly, suppose $\\xi_2$ is not an end point of $\\closedint a b$.\nThen $\\xi_2 \\in \\openint a b$ and $f$ has a local minimum at $\\xi_2$.\nHence the result follows from Derivative at Maximum or Minimum.\n{{qed}}\n\\end{proof}\n\n"}}, "12013": {"score": 0.9086843729019165, "content": {"text": "\\section{Intermediate Value Theorem for Derivatives}\nTags: Real Analysis, Analysis\n\n\\begin{theorem}\nLet $I$ be an open interval.\nLet $f : I \\to \\R$ be everywhere differentiable.\nThen $f'$ satisfies the Intermediate Value Property.\n\\end{theorem}\n\n\\begin{proof}\nSince $\\forall \\set {a, b \\in I: a < b}: \\openint a b \\subseteq I$, the result follows from Image of Interval by Derivative.\n{{qed}}\nCategory:Real Analysis\n\\end{proof}\n\n"}}, "13833": {"score": 0.9116587042808533, "content": {"text": "\\section{Extended Rolle's Theorem}\nTags: Differential Calculus, Rolle's Theorem, Continuous Real Functions, Continuous Functions, Differentiable Real Functions\n\n\\begin{theorem}\nLet $f: D \\to \\R$ be differentiable on a closed interval $I \\subseteq \\R$.\nLet $x_0 < x_1 < \\dots < x_n \\in I$.\nLet $\\map f {x_i} = 0$ for $i = 0, \\ldots, n$.\nThen for all $i = 0, \\ldots, n-1$:\n:$\\exists \\xi_i \\in \\openint {x_i} {x_{i + 1} }: \\map {f'} {\\xi_i} = 0$\n\\end{theorem}\n\n\\begin{proof}\nSince $f$ is differentiable on $I$, f is differentiable on $\\closedint {x_i} {x_{i + 1} }$ for $i = 0, \\ldots, n - 1$.\nThus a fortiori, $f$ is also continuous on the closed interval $\\closedint {x_i} {x_{i + 1} }$ and differentiable on the open interval $\\openint {x_i} {x_{i + 1} }$.\nFor $i = 0, \\ldots, n$ we have $\\map f {x_i} = 0$.\nHence the conditions of Rolle's Theorem are fulfilled on each interval which yields the result.\n\\end{proof}\n\n"}}}}, "TheoremQA_xinyi/dag_1.json": {"gold": {"3778": 1, "23699": 1}, "retrieved": {"2454": {"score": 0.829792320728302, "content": {"text": "\\begin{definition}[Definition:Directed Graph]\nA '''directed graph''' or '''digraph''' is a graph each of whose edges has a '''direction''':\n310px\nIn the above graph, the vertices are $v_1, v_2, v_3$ and $v_4$.\n\\end{definition}"}}, "2462": {"score": 0.8308126926422119, "content": {"text": "\\begin{definition}[Definition:Directed Hamilton Cycle Problem]\nThere are two versions of the '''Directed Hamilton Cycle Problem'''.\n\\end{definition}"}}, "2459": {"score": 0.8314511179924011, "content": {"text": "\\begin{definition}[Definition:Directed Graph/Formal Definition]\n310px\nA '''directed graph''' or '''digraph''' $D$ is a non-empty set $V$ together with an antireflexive relation $E$ on $V$.\nThe elements of $E$ are the arcs.\nThus the above '''digraph''' can be defined as:\n:$D = \\struct {V, E}:$\n::$V = \\set {v_1, v_2, v_3, v_4}$\n::$E = \\set {\\tuple {v_1, v_2}, \\tuple {v_2, v_4}, \\tuple {v_4, v_3}, \\tuple {v_4, v_1}, \\tuple {v_1, v_4} }$\n\\end{definition}"}}, "10058": {"score": 0.8308796882629395, "content": {"text": "\\begin{definition}[Definition:Walk (Graph Theory)/Closed]\nA '''closed walk''' is a walk whose first vertex is the same as the last.\nThat is, it is a walk which ends where it starts.\n\\end{definition}"}}, "302": {"score": 0.8308477997779846, "content": {"text": "\\begin{definition}[Definition:Arborescence/Also defined as]\nSome sources, for example {{BookReference|The Art of Computer Programming: Volume 1: Fundamental Algorithms||Donald E. Knuth}}, define an arborescence of root $r$ so as to reverse the orientation of $G$, so that the arcs are all directed toward the root rather than away from it.\nCategory:Definitions/Arborescences\n\\end{definition}"}}, "2474": {"score": 0.845033586025238, "content": {"text": "\\begin{definition}[Definition:Directed Walk]\nLet $G = \\struct {V, A}$ be a directed graph.\nA '''directed walk''' in $G$ is a finite or infinite sequence $\\sequence {x_k}$ such that:\n:$\\forall k \\in \\N: k + 1 \\in \\Dom {\\sequence {x_k} }: \\tuple {x_k, x_{k + 1} } \\in A$\n\\end{definition}"}}, "5352": {"score": 0.8317236304283142, "content": {"text": "\\begin{definition}[Definition:Loop-Graph/Loop-Digraph/Loop]\nAs per the definition, a '''loop-digraph''' is a directed graph which allows an arc to start and end at the same vertex.\nSuch an arc is called a loop.\nCategory:Definitions/Loop-Digraphs\n\\end{definition}"}}, "20765": {"score": 0.8673496842384338, "content": {"text": "\\section{Rooted Tree Corresponds to Arborescence}\nTags: Rooted Trees, Graph Theory\n\n\\begin{theorem}\nLet $T = \\struct {V, E}$ be a rooted tree with root $r$.\nThen there is a unique orientation of $T$ which is an $r$-arborescence.\n\\end{theorem}\n\n\\begin{proof}\nRecall that a tree is connected and has no cycles.\nThus there is exactly one path from each node of $T$ to each other node of $T$.\n{{explain|This is in fact a result that already exists and can be quoted directly.}}\nLet $A$ be the set of all ordered pairs $x, y \\in V$ such that:\n:$\\tuple {x, y} \\in E$ and\n:The unique path from $r$ to $y$ passes through $x$.\n{{finish}}\n\\end{proof}\n\n"}}, "7695": {"score": 0.8398916721343994, "content": {"text": "\\begin{definition}[Definition:Reachable/Definition 1]\nLet $G = \\struct {V, A}$ be a directed graph.\nLet $u, v \\in V$.\nThen $v$ is '''reachable''' from $u$ {{iff}} there exists a directed walk from $u$ to $v$.\n\\end{definition}"}}, "306": {"score": 0.8404632210731506, "content": {"text": "\\begin{definition}[Definition:Arborescence/Definition 3]\nLet $G = \\struct {V, A}$ be a directed graph.\nLet $r \\in V$.\n$G$ is an '''arborescence of root $r$''' {{iff}}:\n: $(1): \\quad$ Each vertex $v \\ne r$ is the final vertex of exactly one arc.\n: $(2): \\quad$ $r$ is not the final vertex of any arc.\n: $(3): \\quad$ For each $v \\in V$ such that $v \\ne r$ there is a directed walk from $r$ to $v$.\n\\end{definition}"}}}}, "TheoremQA_panlu/similarity2.json": {"gold": {"14371": 1, "22806": 1}, "retrieved": {"14419": {"score": 0.8712953329086304, "content": {"text": "\\section{Equality of Ratios is Transitive}\nTags: Ratios\n\n\\begin{theorem}\n{{:Euclid:Proposition/V/11}}\nThat is:\n:$A : B = C : D, C : D = E : F \\implies A : B = E : F$\n\\end{theorem}\n\n\\begin{proof}\nLet $A : B = C : D$ and $C : D = E : F$.\n:550px\nOf $A, C, E$ let equimultiples $G, H, K$ be taken.\nOf $B, D, F$ let other arbitrary equimultiples $L, M, N$ be taken.\nWe have that:\n: $A : B = C : D$\n: $G, H$ are equimultiples of $A, C$\n: $L, M$ are equimultiples of $B, D$\nSo:\n: $G > L \\implies H > M$\n: $G = L \\implies H = M$\n: $G < L \\implies H < M$\nAlso, we have that:\n: $C : D = E : F$\n: $H, K$ are equimultiples of $C, E$\n: $M, N$ are equimultiples of $D, F$\nSo:\n: $H > M \\implies K > N$\n: $H = M \\implies K = N$\n: $H < M \\implies K < N$\nAlso, we have that:\n: $G, K$ are equimultiples of $A, E$\n: $L, N$ are equimultiples of $B, F$\nTherefore $A : B = E : F$.\n{{qed}}\n{{Euclid Note|11|V}}\n\\end{proof}\n\n"}}, "10485": {"score": 0.871321976184845, "content": {"text": "\\section{Multiples of Terms in Equal Ratios}\nTags: Ratios, Multiples of Terms in Equal Ratios\n\n\\begin{theorem}\nLet $a, b, c, d$ be quantities.\nLet $a : b = c : d$ where $a : b$ denotes the ratio between $a$ and $b$.\nThen for any numbers $m$ and $n$:\n:$m a : n b = m c : n d$\n{{:Euclid:Proposition/V/4}}\n\\end{theorem}\n\n\\begin{proof}\nLet a first magnitude $A$ have to a second magnitude $B$ the same ratio as a third $C$ to a fourth $D$.\nLet equimultiples $E, F$ be taken of $A, C$, and let different equimultiples $G, H$ be taken of $B, D$.\nWe need to show that $E : G = F : H$.\n:500px\nLet equimultiples $K, L$ be taken of $E, F$ and other arbitrary equimultiples $M, N$ be taken of $G, H$.\nWe have that $E$ is the same multiple of $A$ that $L$ is of $C$.\nSo from Proposition 3: Multiplication of Numbers is Associative, $K$ is the same multiple of $A$ that $L$ is of $C$.\nFor the same reason, $M$ is the same multiple of $B$ that $N$ is of $D$.\nWe have that:\n: $A$ is to $B$ as $C$ is to $D$\n: of $A, C$ equimultiples $K, L$ have been taken\n: of $B, D$ other equimultiples $M, N$ have been taken.\nSo from the definition of equality of ratios:\n: if $K$ is in excess of $M$, $L$ is also in excess of $N$\n: if $K$ is equal to $M$, $L$ is equal to $N$\n: if $K$ is less than $M$, $L$ is less than $N$\nBut $K, L$ are equimultiples of $E, F$.\nTherefore as $E$ is to $G$, so is $F$ to $H$.\n{{qed}}\n{{Euclid Note|4|V}}\n60216\n60213\n2011-07-31T19:23:57Z\nPrime.mover\n59\n60216\nwikitext\ntext/x-wiki\n\\end{proof}\n\n"}}, "20522": {"score": 0.8746284246444702, "content": {"text": "\\section{Relative Sizes of Elements in Perturbed Proportion}\nTags: Ratios\n\n\\begin{theorem}\n{{:Euclid:Proposition/V/21}}\nThat is, let:\n:$a : b = e : f$\n:$b : c = d : e$\nThen:\n:$a > c \\implies d > f$\n:$a = c \\implies d = f$\n:$a < c \\implies d < f$\n\\end{theorem}\n\n\\begin{proof}\nLet there be three magnitudes $A, B, C$, and others $D, E, F$ equal to them in multitude, which taken two and two together are in the same ratio.\nLet the proportion of them be perturbed, that is:\n:$A : B = E : F$\n:$B : C = D : E$\nLet $A > C$.\nThen we need to show that $D > F$.\n:350px\nWe have that $A > C$.\nSo from Relative Sizes of Ratios on Unequal Magnitudes $A : B > C : B$.\nBut $A : B = E : F$, and $C : B = E : D$\nSo from Relative Sizes of Proportional Magnitudes $E : F > E : D$.\nBut from Relative Sizes of Magnitudes on Unequal Ratios $F < D$ and so $D > F$.\nSimilarly we can prove that $A = C \\implies D = F$ and $A < C \\implies D < F$.\n{{qed}}\n{{Euclid Note|21|V}}\n\\end{proof}\n\n"}}, "20523": {"score": 0.8732665181159973, "content": {"text": "\\section{Relative Sizes of Magnitudes on Unequal Ratios}\nTags: Ratios\n\n\\begin{theorem}\n{{:Euclid:Proposition/V/10}}\nThat is:\n:$a : c > b : c \\implies a > b$\n:$c : b > c : a \\implies b < a$\n\\end{theorem}\n\n\\begin{proof}\nLet $A$ have to $C$ a greater ratio than $B$ has to $C$.\n:350px\nSuppose $A = B$.\nThen from Ratios of Equal Magnitudes $A : C = B : C$.\nBut by hypothesis $A : C > B : C$, so $A \\ne B$.\nSuppose $A < B$.\nThen from Relative Sizes of Ratios on Unequal Magnitudes it would follow that $A : C < B : C$.\nBut by hypothesis $A : C > B : C$.\nTherefore it must be that $A > B$.\n{{qed|lemma}}\nLet $C$ have to $B$ a greater ratio than $C$ has to $A$.\nSuppose $B = A$.\nThen from Ratios of Equal Magnitudes $C : B = C : A$.\nBut by hypothesis $C : B > C : A$, so $B \\ne A$.\nSuppose $B > A$.\nThen from Relative Sizes of Ratios on Unequal Magnitudes it would follow that $C : B < C : A$.\nBut by hypothesis $C : B > C : A$.\nTherefore it must be that $B < A$.\n{{qed}}\n{{Euclid Note|10|V|{{EuclidNoteConverse|book=5|prop=8|title=Relative Sizes of Ratios on Unequal Magnitudes}}}}\n\\end{proof}\n\n"}}, "20524": {"score": 0.8718753457069397, "content": {"text": "\\section{Relative Sizes of Proportional Magnitudes}\nTags: Ratios\n\n\\begin{theorem}\n{{:Euclid:Proposition/V/13}}\nThat is:\n:$a : b = c : d, c : d > e : f \\implies a : b > e : f$\n\\end{theorem}\n\n\\begin{proof}\nLet a first magnitude $A$ have to a second $B$ the same ratio as a third $C$ to a fourth $D$.\nLet the third $C$ have to the fourth $D$ a greater ratio than a fifth $E$ has to a sixth $F$.\n:450px\nWe have that $C : D > E : F$.\nFrom {{EuclidDefLink|V|7|Greater Ratio}}, there will be some equimultiples of $C, E$ and other arbitrary equimultiples of $D, F$ such that the multiple of $C$ is in excess of the multiple of $D$, while the multiple of $E$ is not in excess of the multiple of $F$.\nLet these equimultiples be taken.\nLet $G, H$ be equimultiples of $C, E$, and $K, L$ be other arbitrary equimultiples of $D, F$, so that $G > K$ but $H \\le L$.\nWhatever multiple $G$ is of $C$, let $M$ be also that multiple of $A$.\nAlso, whatever multiple $K$ is of $D$, let $N$ be also that multiple of $B$.\nNow we have that $A : B = C : D$ and of $A, C$ equimultiples $M, G$ have been taken.\nWe also have that of $B, D$ other arbitrary equimultiples $N, K$ have been taken.\nTherefore:\n: $M > N \\implies G > K$\n: $M = N \\implies G = K$\n: $M < N \\implies G < K$\nfrom {{EuclidDefLink|V|5|Equality of Ratios}}.\nBut $G > K$ and so $M > N$.\nBut $H \\le L$, and:\n: $M, H$ are equimultiples $A, E$\n: $N, L$ are other, arbitrary equimultiples $B, F$.\nTherefore from {{EuclidDefLink|V|7|Greater Ratio}}, $A : B > E : F$.\n{{qed}}\n{{Euclid Note|13|V}}\n\\end{proof}\n\n"}}, "20525": {"score": 0.8792245984077454, "content": {"text": "\\section{Relative Sizes of Ratios on Unequal Magnitudes}\nTags: Ratios\n\n\\begin{theorem}\n{{:Euclid:Proposition/V/8}}\nThat is:\n:$a > b \\implies a : c > b : c$\n:$a > b \\implies c : a < c : b$\n\\end{theorem}\n\n\\begin{proof}\nLet $AB, C$ be unequal magnitudes, and let $AB$ be greater.\nLet $D$ be another arbitrary magnitude.\nWe are to show that $AB$ has to $D$ a greater ratio than $C$ has to $D$, and $D$ has to $C$ a greater ratio than it has to $AB$.\nWe have that $AB > C$, so let $BE = C$.\nThen from {{EuclidDefLink|V|4|Existence of Ratio}}, the lesser of the magnitudes $AE, EB$, if multiplied, will eventually be greater than $D$.\nThere are two cases.\n;First, let $AE < EB$.\n:410px\nLet $AE$ be multiplied by some number, and let $FG$ be a multiple of it which is greater than $D$.\nThen whatever multiple $FG$ is of $AE$, let $GH$ be made the same multiple of $EB$ and $K$ of $C$.\nLet $L := 2D, M := 3D, N := 4D \\ldots$ until one of these multiples is greater than $K$.\nSuppose $M \\le K$ while $N > K$.\nWE have that $FG$ is the same multiple of $AE$ that $GH$ is of $EB$.\nSo from Multiplication of Numbers is Left Distributive over Addition, $FG$ is the same multiple of $AE$ that $FHK$ is of $AB$.\nBut $FG$ is the same multiple of $AE$ that $K$ is of $C$.\nTherefore $FH, K$ are equimultiples of $AB, C$.\nAgain, we have that $GH$ is the same multiple of $EB$ that $K$ is of $C$, and $EB = C$.\nTherefore $GH = K$.\nBut $K \\ge M$ and so $GH \\ge M$.\nAlso, $FG > D$ so $FH > D + M$.\nBut $D + M = N$ by the construction of $M$ and $N$, and $FH > M + D$.\nSo $FH > N$ while $K \\le N$.\nAlso, $FH, K$ are equimultiples of $AB, C$ while $N$ is a multiple of some arbitrary $D$.\nSo from Ratios of Equal Magnitudes $AB$ has a greater ratio to $D$ than $C$ has to $D$.\nNext, note that with the same construction, we can show similarly that $N > K$ while $N \\le FH$.\nAlso we have that $N$ is a multiple of $D$, while $FH, K$ are other equimultiples of $AB, C$.\nTherefore $D$ has a greater ratio to $C$ than $D$ has to $AB$.\n{{qed|lemma}}\n;Second, let $AE > EB$.\nFrom {{EuclidDefLink|V|4|Existence of Ratio}}, $EB$, if multiplied, will eventually be greater than $D$.\n:410px\nLet it be so multiplied, and let $GH$ be a multiple of $EB$ and greater than $D$.\nWhatever multiple $GH$ is of $EB$, let $FG$ be made the same multiple of $AE$, and $K$ of $C$.\nThen we can prove similarly that $FH, K$ are equimultiples of $AB, C$.\nSimilarly, let $M, N$ be consecutive multiples of $D$ such that $M \\le FG$ and $N > FG$.\nBut $GH > D$, therefore $FH > D + M$, that is, $FH > N$.\nNow $K \\le N$ inasmuch as $FG$ also, which is greater than $GH$, that is, than $K$, is not in excess of $N$.\nHence the result.\n{{qed}}\n{{Euclid Note|8|V|{{EuclidNoteConverse|book=V|prop=10|title=Relative Sizes of Magnitudes on Unequal Ratios}}}}\n\\end{proof}\n\n"}}, "19952": {"score": 0.8749736547470093, "content": {"text": "\\section{Proportion of Power}\nTags: Proportion\n\n\\begin{theorem}\nLet $x$ and $y$ be proportional.\n{{explain|Establish what types of object $x$ and $y$ are. As it stands here, they could be anything.}}\nLet $n \\in \\Z$. \nThen $x^n \\propto y^n$.\n\\end{theorem}\n\n\\begin{proof}\nLet $x \\propto y$.\nThen $\\exists k \\ne 0: x = k \\times y$ by the definition of proportion.\nRaising both sides of this equation to the $n$th power:\n{{begin-eqn}}\n{{eqn | l = x^n\n      | r = \\paren {k \\times y}^n\n}}\n{{eqn | r = k^n \\times y^n\n}}\n{{end-eqn}}\nso $k^n$ is the desired constant of proportion.\nThe result follows from the definition of proportion.\n{{qed}}\nCategory:Proportion\n\\end{proof}\n\n"}}, "7763": {"score": 0.8858063220977783, "content": {"text": "\\begin{definition}[Definition:Reciprocal Proportion]\nLet $P$ and $Q$ be geometric figures of the same type (that is, having the same number and configuration of sides).\nLet $A$ and $B$ be sides of $P$, and let $C$ and $D$ be sides of $Q$, such that $A$ and $C$ are corresponding sides, and $B$ and $D$ also be corresponding sides.\nThen $P$ and $Q$ have sides which are '''in reciprocal proportion''', or are '''reciprocally proportional''', if:\n:$A : D = B : C$\nwhere $A : D$ is the ratio of the lengths of $A$ and $D$.\n\\end{definition}"}}, "20223": {"score": 0.87831050157547, "content": {"text": "\\section{Ratios of Equal Magnitudes}\nTags: Ratios\n\n\\begin{theorem}\n{{:Euclid:Proposition/V/7}}\nThat is:\n:$a = b \\implies a : c = b : c$\n:$a = b \\implies c : a = c : b$\n\\end{theorem}\n\n\\begin{proof}\nLet $A, B$ be equal magnitudes and let $C$ be any other arbitrary magnitude.\nWe need to show that $A : C = B : C$ and $C : A = C : B$.\n:400px\nLet equimultiples $D, E$ of $A, B$ be taken, and another arbitrary multiple $F$ of $C$.\nWe have that $D$ is the same multiple of $A$ that $E$ is of $B$, while $A = B$.\nTherefore $D = E$.\nBut $F$ is another arbitrary magnitude.\nTherefore:\n:$D > F \\implies E > F$\n:$D = F \\implies E = F$\n:$D < F \\implies E < F$\nWe have that $D, E$ are equimultiples of $A, B$ while $F$ is another arbitrary multiple of $C$.\nSo from {{EuclidDefLink|V|5|Equality of Ratios}}, $A : C = B : C$.\nWith the same construction we can show that $D = E$, while $F$ is some other magnitude.\nTherefore:\n:$F > D \\implies F > E$\n:$F = D \\implies F = E$\n:$F < D \\implies F < E$\nBut $F$ is a multiple of $C$, while $D, E$ are equimultiples of $A, B$.\nSo from {{EuclidDefLink|V|5|Equality of Ratios}}, $C : A = C : B$.\n{{qed}}\n{{Euclid Note|7|V|{{EuclidNoteConverse|book = V|prop = 9|title = Magnitudes with Same Ratios are Equal}}}}\n\\end{proof}\n\n"}}, "19953": {"score": 0.8787379860877991, "content": {"text": "\\section{Proportional Magnitudes are Proportional Alternately}\nTags: Ratios\n\n\\begin{theorem}\n{{:Euclid:Proposition/V/16}}\nThat is:\n:$a : b = c : d \\implies a : c = b : d$\n\\end{theorem}\n\n\\begin{proof}\nLet $A, B, C, D$ be four proportional magnitudes, so that as $A$ is to $B$, then so is $C$ to $D$.\nWe need to show that as $A$ is to $C$, then $B$ is to $D$.\n:400px\nLet equimultiples $E, F$ be taken of $A, B$.\nLet other arbitrary equimultiples $G, H$ be taken of $C, D$.\nWe have that $E$ is the same multiple of $A$ that $F$ is of $B$.\nSo from Ratio Equals its Multiples we have that $A : B = E : F$\nBut $A : B = C : D$.\nSo from Equality of Ratios is Transitive it follows that $C : D = E : F$.\nSimilarly, we have that $G, H$ are equimultiples of $C, D$.\nSo from Ratio Equals its Multiples we have that $C : D = G : H$\nSo from Equality of Ratios is Transitive it follows that $E : F = G : H$.\nBut from Relative Sizes of Components of Ratios:\n:$E > G \\implies F > H$\n:$E = G \\implies F = H$\n:$E < G \\implies F < H$\nNow $E, F$ are equimultiples of $A, B$, and $G, H$ are equimultiples of $C, D$.\nTherefore from {{EuclidDefLink|V|5|Equality of Ratios}}:\n:$A : C = B : D$\n{{qed}}\n{{Euclid Note|16|V}}\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/eigen_value1.json": {"gold": {"2724": 1, "2723": 1}, "retrieved": {"14718": {"score": 0.8381020426750183, "content": {"text": "\\section{Eigenvalues of Symmetric Matrix are Orthogonal}\nTags: Symmetric Matrices\n\n\\begin{theorem}\nLet $K$ be a ring. \nLet $A$ be a symmetric matrix over $K$.\nLet $\\lambda_1, \\lambda_2$ be distinct eigenvalues of $A$.\nLet $\\mathbf v_1, \\mathbf v_2$ be eigenvectors of $A$ corresponding to the eigenvalues $\\lambda_1$ and $\\lambda_2$ respectively.\nLet $\\innerprod \\cdot \\cdot$ be the dot product on $K$. \n{{explain|The existing definitions of dot product are not presented in terms of the inner product. Recommend either using the conventional definitions of dot product, or explicitly linking to the result which explains this. Perhaps preferable to bypass the inner product notation altogether, as it is often the case that linear algebra at this level is covered before inner product spaces are encountered at all. This would ensure accessibility to those at that more basic level.}}\nThen $\\mathbf v_1$ and $\\mathbf v_2$ are orthogonal with respect to $\\innerprod \\cdot \\cdot$.\n\\end{theorem}\n\n\\begin{proof}\nWe have:\n:$A \\mathbf v_1 = \\lambda_1 \\mathbf v_1$\nand:\n:$A \\mathbf v_2 = \\lambda_2 \\mathbf v_2$\nWe also have: \n{{begin-eqn}}\n{{eqn\t| l = \\mathbf v_1^\\intercal \\paren {A \\mathbf v_2}\n\t| r = \\mathbf v_1^\\intercal \\paren {\\lambda_2 \\mathbf v_2}\n}}\n{{eqn\t| r = \\lambda_2 \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n\t| c = {{Defof|Dot Product}}\n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn\t| l = \\mathbf v_1^\\intercal \\paren {A \\mathbf v_2}\n\t| r = \\paren {A^\\intercal \\mathbf v_1}^\\intercal \\mathbf v_2\n\t| c = Transpose of Matrix Product\n}}\n{{eqn\t| r = \\paren {A \\mathbf v_1}^\\intercal \\mathbf v_2\n\t| c = {{Defof|Symmetric Matrix}}\n}}\n{{eqn\t| r = \\lambda_1 \\paren {\\mathbf v_1}^\\intercal \\mathbf v_2\n}}\n{{eqn\t| r = \\lambda_1 \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n\t| c = {{Defof|Dot Product}}\n}}\n{{end-eqn}}\nWe therefore have: \n:$0 = \\paren {\\lambda_2 - \\lambda_1} \\innerprod {\\mathbf v_1} {\\mathbf v_2}$\nSince $\\lambda_1 \\ne \\lambda_2$, we have:\n:$\\innerprod {\\mathbf v_1} {\\mathbf v_2} = 0$\nhence $\\mathbf v_1$ and $\\mathbf v_2$ are orthogonal with respect to $\\innerprod \\cdot \\cdot$.\n{{qed}}\nCategory:Symmetric Matrices\n\\end{proof}\n\n"}}, "21363": {"score": 0.8395953178405762, "content": {"text": "\\section{Similar Matrices have same Traces}\nTags: Traces of Matrices\n\n\\begin{theorem}\nLet $\\mathbf A = \\sqbrk a_n$ and $\\mathbf B = \\sqbrk b_n$ be square matrices of order $n$.\nLet $\\mathbf A$ and $\\mathbf B$ be similar.\nThen:\n:$\\map \\tr {\\mathbf A} = \\map \\tr {\\mathbf B}$\nwhere $\\map \\tr {\\mathbf A}$ denotes the trace of $\\mathbf A$.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of similar matrices\n:$\\exists \\mathbf P: \\mathbf P^{-1} \\mathbf A \\mathbf P = \\mathbf B$\nwhere $\\mathbf P$ is an invertible matrix of order $n$.\nThus it remains to show that:\n:$\\map \\tr {\\mathbf P^{-1} \\mathbf A \\mathbf P} = \\map \\tr {\\mathbf A}$\n{{ProofWanted}}\n\\end{proof}\n\n"}}, "2719": {"score": 0.9013432264328003, "content": {"text": "\\begin{definition}[Definition:Eigenspace]\nLet $K$ be a field.\nLet $V$ be a vector space over $K$. \nLet $A : V \\to V$ be a linear operator.\nLet $I : V \\to V$ be the identity mapping on $V$. \nLet $\\lambda \\in K$ be an eigenvalue of $A$. \nLet $\\map \\ker {A - \\lambda I}$ be the kernel of $A - \\lambda I$.\nWe say that $\\map \\ker {A - \\lambda I}$ is the '''eigenspace''' corresponding to the eigenvalue $\\lambda$.\n\\end{definition}"}}, "12767": {"score": 0.848946213722229, "content": {"text": "\\section{Hermitian Matrix has Real Eigenvalues}\nTags: Hermitian Matrices, Linear Algebra, Eigenvectors, Hermitian Matrix has Real Eigenvalues\n\n\\begin{theorem}\nEvery Hermitian matrix has eigenvalues which are all real numbers.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\mathbf A$ be a Hermitian matrix.\nThen, by definition, $\\mathbf A = \\mathbf{A}^*$, where $^*$ designates the conjugate transpose.\nLet $\\lambda$ be an eigenvalue of $\\mathbf A$.\nLet $\\mathbf v$ be the eigenvector corresponding to the eigenvalue $\\lambda$, of the matrix $\\mathbf A$.\nNow, by definition of eigenvector, $\\mathbf{A v} = \\lambda \\mathbf v$.\nLeft-multiplying both sides by $\\mathbf{v}^*$, we obtain:\n: $\\mathbf{v}^* \\mathbf{A v} = \\mathbf{v}^* \\lambda \\mathbf v = \\lambda \\mathbf{v}^* \\mathbf v$\nFirstly, note that both $\\mathbf{v}^* \\mathbf{A v}$ and $\\mathbf{v}^* \\mathbf{v}$ are $1 \\times 1$-matrices.\nNow observe that, using Conjugate Transpose of Matrix Product: General Case:\n:$\\left({\\mathbf{v}^* \\mathbf{A v}}\\right)^* = \\mathbf{v}^* \\mathbf{A}^* \\left({\\mathbf{v}^*}\\right)^*$\nAs $\\mathbf A$ is Hermitian, and $\\left({\\mathbf{v}^*}\\right)^* = \\mathbf v$ by Double Conjugate Transpose is Itself, it follows that:\n:$\\mathbf{v}^* \\mathbf{A}^* \\left({\\mathbf{v}^*}\\right)^* = \\mathbf{v}^* \\mathbf{A v}$\nThat is, $\\mathbf{v}^* \\mathbf{A v}$ is also Hermitian.\nBy Product with Conjugate Transpose Matrix is Hermitian, $\\mathbf{v}^* \\mathbf v$ is Hermitian. That is:\n:$ (\\mathbf v^*\\mathbf v)^*=\\mathbf v^*(\\mathbf v^*)^*=\\mathbf v^*\\mathbf v $\nSo both $\\mathbf v^*A\\mathbf v$ and $\\mathbf v^*\\mathbf v$ are Hermitian $1 \\times 1$ matrices. If we let ''a'' be the entry in $\\mathbf v^*A\\mathbf v$ and ''b'' the entry in $\\mathbf v^*\\mathbf v$:\nBy definition of Hermitian Matrices, $a=\\bar{a}$ and $b=\\bar{b}$ which can only be true if they are real entries.\nFrom above, we have $\\mathbf v^*A\\mathbf v=\\lambda\\mathbf v^*\\mathbf v$. This means that $\\lambda$ must also be real.\nTherefore, Hermitian matrices have real eigenvalues.\n{{qed}}\n\\end{proof}\n\n"}}, "14717": {"score": 0.8437617421150208, "content": {"text": "\\section{Eigenvalues of Normal Operator have Orthogonal Eigenspaces}\nTags: Linear Transformations on Hilbert Spaces\n\n\\begin{theorem}\nLet $\\HH$ be a Hilbert space.\nLet $\\mathbf T: \\HH \\to \\HH$ be a normal operator.\nLet $\\lambda_1, \\lambda_2$ be distinct eigenvalues of $\\mathbf T$.\nThen:\n:$\\map \\ker {\\mathbf T - \\lambda_1} \\perp \\map \\ker {\\mathbf T - \\lambda_2}$\nwhere:\n:$\\ker$ denotes kernel\n:$\\perp$ denotes orthogonality.\n\\end{theorem}\n\n\\begin{proof}\n{{improve}}\nRequisite knowledge: $\\mathbf T^*$ is the adjoint of $\\mathbf T$ and is defined by the fact that for any $\\mathbf u, \\mathbf w \\in \\HH$, we have\n:$\\innerprod {\\mathbf {T u} } {\\mathbf w} = \\innerprod {\\mathbf u} {\\mathbf T^* \\mathbf w}$\nIt is important to note the existence and uniqueness of adjoint operators.\n{{explain|Link to the required proofs and/or definitions as given above.}}\n'''Claim''': We know that for $\\mathbf v \\in \\HH$:\n:$\\mathbf {T v} = \\lambda \\mathbf v \\iff \\mathbf T^* \\mathbf v = \\overline \\lambda \\mathbf v$\nThis is true because for all normal operators, by definition:\n:$\\mathbf T^* \\mathbf T = \\mathbf T {\\mathbf T^*}$\nand so:\n{{begin-eqn}}\n{{eqn | l = \\norm {\\mathbf {T v} }^2\n      | r = \\innerprod {\\mathbf{T v} } {\\mathbf{T v} }\n      | c = {{Defof|Inner Product Norm}}\n}}\n{{eqn | r = \\innerprod {\\mathbf T^* \\mathbf {T v} } {\\mathbf v}\n}}\n{{eqn | r = \\innerprod {\\mathbf T \\mathbf T^* \\mathbf v} {\\mathbf v}\n      | c = {{Defof|Normal Operator}}\n}}\n{{eqn | r = \\innerprod {\\mathbf T^* \\mathbf v} {\\mathbf T^* \\mathbf v}\n}}\n{{eqn | r = \\norm {\\mathbf T^* \\mathbf v}^2\n      | c = {{Defof|Inner Product Norm}}\n}}\n{{end-eqn}}\nSince $\\mathbf T$ is normal, $\\mathbf T - \\lambda \\mathbf I$ is also normal.\n{{explain|Link to a page demonstrating the above}}\nThus:\n{{begin-eqn}}\n{{eqn | l = \\mathbf {T v}\n      | r = \\lambda \\mathbf v\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\bszero\n      | r = \\norm {\\paren {\\mathbf T - \\lambda \\mathbf I} \\mathbf v}\n}}\n{{eqn | r = \\norm {\\paren {\\mathbf T - \\lambda \\mathbf I}^* \\mathbf v}\n}}\n{{eqn | r = \\norm {\\mathbf T^* \\mathbf v - \\overline \\lambda \\mathbf v}\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\mathbf T^* \\mathbf v\n      | r = \\overline \\lambda \\mathbf v\n}}\n{{end-eqn}}\nLet $\\mathbf v_1$ and $\\mathbf v_2$ be non-zero eigenvectors of $\\mathbf T$ with corresponding eigenvalues $\\lambda_1$ and $\\lambda_2$, respectively.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\lambda_1 \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n      | r = \\innerprod {\\lambda_1 \\mathbf v_1} {\\mathbf v_2}\n}}\n{{eqn | r = \\innerprod {\\mathbf T \\mathbf v_1} {\\mathbf v_2}\n}}\n{{eqn | r = \\innerprod {\\mathbf v_1} {\\mathbf T^* \\mathbf v_2}\n}}\n{{eqn | r = \\innerprod {\\mathbf v_1} {\\overline{\\lambda_2} \\mathbf v_2}\n}}\n{{eqn | r = \\lambda_2 \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\bszero\n      | r = \\paren {\\lambda_1 - \\lambda_2} \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n}}\n{{end-eqn}}\nSince $\\lambda_1 \\ne \\lambda_2$, this is only possible if $\\innerprod {\\mathbf v_1} {\\mathbf v_2} = 0$, which means the eigenvectors of our normal operator are orthogonal.\n{{qed}}\n\\end{proof}\n\n"}}, "2724": {"score": 0.948101282119751, "content": {"text": "\\begin{definition}[Definition:Eigenvector/Real Square Matrix]\nLet $\\mathbf A$ be a square matrix of order $n$ over $\\R$. \nLet $\\lambda \\in \\R$ be an eigenvalue of $\\mathbf A$. \nA non-zero vector $\\mathbf v \\in \\R^n$ is an '''eigenvector corresponding to $\\lambda$''' {{iff}}: \n:$\\mathbf A \\mathbf v = \\lambda \\mathbf v$\n\\end{definition}"}}, "2720": {"score": 0.9189104437828064, "content": {"text": "\\begin{definition}[Definition:Eigenvalue/Linear Operator]\nLet $K$ be a field.\nLet $V$ be a vector space over $K$. \nLet $A : V \\to V$ be a linear operator.\n$\\lambda \\in K$ is an '''eigenvalue''' of $A$ {{iff}}:\n:$\\map \\ker {A - \\lambda I} \\ne \\set {0_V}$\nwhere: \n:$0_V$ is the zero vector of $V$\n:$I : V \\to V$ is the identity mapping on $V$\n:$\\map \\ker {A - \\lambda I}$ denotes the kernel of $A - \\lambda I$.\nThat is, $\\lambda \\in K$ is an '''eigenvalue of $A$''' {{iff}} the kernel of $A$ is non-trivial.\n{{explain|Link to \"non-trivial\"}}\n\\end{definition}"}}, "2721": {"score": 0.9519184231758118, "content": {"text": "\\begin{definition}[Definition:Eigenvalue/Real Square Matrix]\nLet $\\mathbf A$ be a square matrix of order $n$ over $\\R$. \nLet $\\lambda \\in \\R$. \n$\\lambda$ is an '''eigenvalue''' of $A$ if there exists a non-zero vector $\\mathbf v \\in \\R^n$ such that: \n:$\\mathbf A \\mathbf v = \\lambda \\mathbf v$\n\\end{definition}"}}, "14713": {"score": 0.9243267774581909, "content": {"text": "\\section{Eigenvalue of Matrix Powers}\nTags: Matrix Algebra\n\n\\begin{theorem}\nLet $A$ be a square matrix.\nLet $\\lambda$ be an eigenvalue of $A$ and $\\mathbf v$ be the corresponding eigenvector.\nThen:\n:$A^n \\mathbf v = \\lambda^n \\mathbf v$\nholds for each positive integer $n$.\n\\end{theorem}\n\n\\begin{proof}\nProof by induction:\nFor all $n \\in \\N_{> 0}$, let $\\map P n$ be the proposition:\n:$A^n \\mathbf v = \\lambda^n \\mathbf v$\n\\end{proof}\n\n"}}, "2723": {"score": 0.9280818104743958, "content": {"text": "\\begin{definition}[Definition:Eigenvector/Linear Operator]\nLet $K$ be a field.\nLet $V$ be a vector space over $K$. \nLet $A : V \\to V$ be a linear operator.\nLet $\\lambda \\in K$ be an eigenvalue of $A$.\nA non-zero vector $v \\in V$ is an '''eigenvector corresponding to $\\lambda$''' {{iff}}:\n:$v \\in \\map \\ker {A - \\lambda I}$\nwhere: \n:$I : V \\to V$ is the identity mapping on $V$\n:$\\map \\ker {A - \\lambda I}$ denotes the kernel of $A - \\lambda I$.\nThat is, {{iff}}: \n:$A v = \\lambda v$\n\\end{definition}"}}}}, "TheoremQA_jianyu_xu/Multinomial_2.json": {"gold": {"10463": 1, "10462": 1}, "retrieved": {"17942": {"score": 0.8444912433624268, "content": {"text": "\\section{Number of Permutations with Repetition}\nTags: Number of Permutations with Repetition, Combinatorics\n\n\\begin{theorem}\nSet $S$ be a set of $n$ elements.\nLet $\\sequence T_m$ be a sequence of $m$ terms of $S$.\nThen there are $n^m$ different instances of $\\sequence T_m$.\n\\end{theorem}\n\n\\begin{proof}\nLet $N_m$ denote the set $\\set {1, 2, \\ldots, m}$.\nLet $f: N_m \\to S$ be the mapping defined as:\n:$\\forall k \\in N_m: \\map f t = t_m$\nBy definition, $f$ corresponds to one of the specific instances of $\\sequence T_m$.\nHence the number of different instances of $\\sequence T_m$ is found from Cardinality of Set of All Mappings:\n:$\\card S^{\\card {N_m} }$\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "6877": {"score": 0.8446542024612427, "content": {"text": "\\begin{definition}[Definition:Permutation/Ordered Selection/Notation]\nThe number of $r$-permutations from a set of cardinality $n$ is denoted variously:\n:$P_{n r}$\n:${}^r P_n$\n:${}_r P_n$\n:${}_n P_r$ (extra confusingly)\nThere is little consistency in the literature).\nOn {{ProofWiki}} the notation of choice is ${}^r P_n$.\nCategory:Definitions/Permutation Theory\n\\end{definition}"}}, "1337": {"score": 0.851832389831543, "content": {"text": "\\begin{definition}[Definition:Combinatorics]\n'''Combinatorics''' is that branch of mathematics concerned with counting things.\n'''Combinatorial''' problems are so named because they are exercises in counting the number of combinations of various objects.\nIt has been stated that it is the core of the discipline of discrete mathematics.\n\\end{definition}"}}, "17941": {"score": 0.8485265970230103, "content": {"text": "\\section{Number of Permutations}\nTags: Permutations, Permutation Theory, Number of Permutations, Combinatorics\n\n\\begin{theorem}\nLet $S$ be a set of $n$ elements.\nLet $r \\in \\N: r \\le n$.\nThen the number of $r$-permutations of $S$ is:\n:${}^r P_n = \\dfrac {n!} {\\paren {n - r}!}$\nWhen $r = n$, this becomes:\n:${}^n P_n = \\dfrac {n!} {\\paren {n - n}!} = n!$\nUsing the falling factorial symbol, this can also be expressed:\n:${}^r P_n = n^{\\underline r}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition, an <math>r</math>-permutations of <math>S</math> is an ordered selection of <math>r</math> elements of <math>S</math>.\nIt can be seen that an <math>r</math>-permutation is an injection from a subset of <math>S</math> into <math>S</math>.\nFrom Cardinality of Set of Injections, we see that the number of <math>r</math>-permutations <math>{}^r P_n</math> on a set of <math>n</math> elements is given by:\n:<math>{}^r P_n = \\frac {n!} {\\left({n-r}\\right)!}</math>\nFrom this definition, it can be seen that a bijection <math>f: S \\to S</math> (as defined above) is an '''<math>n</math>-permutation'''.\nHence the number of <math>r</math>-permutations on a set of <math>n</math> elements is <math>{}^n P_n = \\frac {n!} {\\left({n-n}\\right)!} = n!</math>.\n{{Qed}}\nCategory:Combinatorics\n24405\n24403\n2010-01-14T06:55:12Z\nPrime.mover\n59\n24405\nwikitext\ntext/x-wiki\n\\end{proof}\n\n"}}, "15526": {"score": 0.8469609618186951, "content": {"text": "\\section{Count of All Permutations on n Objects}\nTags: Permutation Theory, Count of All Permutations on n Objects\n\n\\begin{theorem}\nLet $S$ be a set of $n$ objects.\nLet $N$ be the number of permutations of $r$ objects from $S$, where $1 \\le r \\le N$.\nThen:\n:$\\ds N = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}$\n\\end{theorem}\n\n\\begin{proof}\nThe number of permutations on $k$ objects, from $n$ is denoted ${}^k P_{10}$.\nFrom Number of Permutations:\n:${}^k P_n = \\dfrac {n!} {\\paren {n - k}!}$\nHence:\n{{begin-eqn}}\n{{eqn | q = \n      | l = N\n      | r = \\sum_{k \\mathop = 1}^n {}^k P_n\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {\\paren {n - k}!}\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {k!}\n      | c = \n}}\n{{eqn | r = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Permutation Theory\nCategory:Count of All Permutations on n Objects\n\\end{proof}\n\n"}}, "16939": {"score": 0.8902059197425842, "content": {"text": "\\section{Cardinality of Set of Combinations with Repetition}\nTags: Combinations with Repetition\n\n\\begin{theorem}\nLet $S$ be a finite set with $n$ elements\nThe number of $k$-combinations of $S$ with repetition is given by:\n:$N = \\dbinom {n + k - 1} k$\n\\end{theorem}\n\n\\begin{proof}\nLet the elements of $S$ be (totally) ordered in some way, by assigning an index to each element.\nThus let $S = \\left\\{ {a_1, a_2, a_3, \\ldots, a_n}\\right\\}$.\nThus each $k$-combination of $S$ with repetition can be expressed as:\n:$\\left\\{ {\\left\\{ {a_{r_1}, a_{r_1}, \\ldots, a_{r_k} }\\right\\} }\\right\\}$\nwhere:\n:$r_1, r_2, \\ldots, r_k$ are all elements of $\\left\\{ {1, 2, \\ldots, n}\\right\\}$\nand such that:\n:$r_1 \\le r_2 \\le \\cdots \\le r_k$\nHence the problem reduces to the number of integer solutions $\\left({r_1, r_2, \\ldots, r_k}\\right)$ such that $1 \\le r_1 \\le r_2 \\le \\cdots \\le r_k \\le n$.\nThis is the same as the number of solutions to:\n:$0 < r_1 < r_2 + 1 < \\cdots < r_k + k - 1 < n + k$\nwhich is the same as the number of solutions to:\n:$0 < s_1 < s_2 < \\cdots < s_k < n + k$\nwhich is the number of ways to choose $k$ elements from $n + k - 1$.\nThis is the same as the number of subsets as a set with $n + k - 1$ elements.\nFrom Cardinality of Set of Subsets:\n:$N = \\dbinom {n + k - 1} k$\n{{qed}}\n\\end{proof}\n\n"}}, "17224": {"score": 0.8619405031204224, "content": {"text": "\\section{Birthday Paradox/General/3}\nTags: Birthday Paradox\n\n\\begin{theorem}\nLet $n$ be a set of people.\nLet the probability that at least $3$ of them have the same birthday be greater than $50 \\%$.\nThen $n \\ge 88$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map F {r, n}$ be the number of ways to distribute $r$ objects into $n$ cells such that there are no more than $2$ objects in each cell.\nLet there be $d$ cells which are each occupied by $2$ objects.\nThese can be chosen in $\\dbinom n d$ ways.\nThere remain $s = r - 2 d$ objects which can then be distributed among $n - d$ cells in $\\dbinom {n - d} s$ ways.\nIn each such arrangement, the $r$ objects may be permuted in:\n:$\\dbinom r 2 \\dbinom {r - 2} 2 \\cdots \\dbinom {r - 2 d + 2} 2 \\paren {r - 2 d}! = \\dfrac {r!} {2^d}$\ndifferent ways.\nHence:\n:$\\map F {r, n} = \\dbinom n d \\dbinom {n - d} s \\dfrac {r!} {2^d}$\nSo the probability of exactly $d$ pairs and $s$ singletons, where $d - s \\le n$, is given by:\n:$\\dfrac {\\map F {r, n} } {n^r}$\nIf we assume a $365$-day year, we have that the probability that at least $3$ of them have the same birthday is given by:\n:$\\map \\Pr r = 1 - \\ds \\sum_{d \\mathop = 0}^{\\floor {r / 2} } \\dfrac {n! \\, r!} {n^r 2^d d! \\paren {r - 2 d}! \\paren {n + d - r}!}$\nwhere $n = 365$.\nWe require the smallest $r$ for which $\\map \\Pr r > \\dfrac 1 2$.\nThe result yields to calculation.\n{{qed}}\n\\end{proof}\n\n"}}, "1332": {"score": 0.8968130946159363, "content": {"text": "\\begin{definition}[Definition:Combination]\nLet $S$ be a set containing $n$ elements.\nAn '''$r$-combination of $S$''' is a subset of $S$ which has $r$ elements.\n\\end{definition}"}}, "1333": {"score": 0.878016471862793, "content": {"text": "\\begin{definition}[Definition:Combination with Repetition]\nLet $S$ be a (finite) set with $n$ elements.\nA '''$k$ combination of $S$ with repetition''' is a multiset with $k$ elements selected from $S$.\n\\end{definition}"}}, "17192": {"score": 0.8811016082763672, "content": {"text": "\\section{Binomial Coefficient/Examples/Number of Bridge Hands}\nTags: Binomial Coefficients, Examples of Binomial Coefficients\n\n\\begin{theorem}\nThe total number $N$ of possible different hands for a game of [https://en.wikipedia.org/wiki/Contract_bridge bridge] is:\n:$N = \\dfrac {52!} {13! \\, 39!} = 635 \\ 013 \\ 559 \\ 600$\n\\end{theorem}\n\n\\begin{proof}\nThe total number of cards in a standard deck is $52$.\nThe number of cards in a single bridge hand is $13$.\nThus $N$ is equal to the number of ways $13$ things can be chosen from $52$.\nThus:\n{{begin-eqn}}\n{{eqn | l = N\n      | r = \\dbinom {52} {23}\n      | c = Cardinality of Set of Subsets\n}}\n{{eqn | r = \\frac {52!} {13! \\left({52 - 13}\\right)!}\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\frac {52!} {13! \\, 39!}\n      | c = \n}}\n{{eqn | r = 635 \\ 013 \\ 559 \\ 600\n      | c = after calculation\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Cayley_3.json": {"gold": {"16790": 1, "17301": 1}, "retrieved": {"15412": {"score": 0.8732702136039734, "content": {"text": "\\section{Cycle Graph is Connected}\nTags: Cycle Graphs, Connected Graphs\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a cycle graph.\nThen $G$ is connected.\n\\end{theorem}\n\n\\begin{proof}\nA cycle graph is defined as a (simple) graph which consists of a single cycle.\nSo a cycle graph consists of just one component, and hence is connected.\n{{qed}}\nCategory:Cycle Graphs\nCategory:Connected Graphs\n\\end{proof}\n\n"}}, "18607": {"score": 0.8771011233329773, "content": {"text": "\\section{Path in Tree is Unique/Sufficient Condition}\nTags: Tree Theory, Graph Theory, Trees\n\n\\begin{theorem}\nLet $T$ be a graph.\nLet $T$ be such that between any two vertices there is exactly one path.\nThen $T$ is a tree.\n\\end{theorem}\n\n\\begin{proof}\nLet $T$ be such that between any two vertices there is exactly one path.\nThen for a start $T$ is by definition connected.\nSuppose $T$ had a circuit, say $\\left({u, u_1, u_2, \\ldots, u_n, v, u}\\right)$.\nThen there are two paths from $u$ to $v$:\n: $\\left({u, u_1, u_2, \\ldots, u_n, v}\\right)$\nand\n: $\\left({u, v}\\right)$.\nHence, by Modus Tollendo Tollens, $T$ can have no circuits.\nThat is, by definition, $T$ is a tree.\n{{qed}}\n\\end{proof}\n\n"}}, "18606": {"score": 0.8818441033363342, "content": {"text": "\\section{Path in Tree is Unique/Necessary Condition}\nTags: Tree Theory, Graph Theory, Trees, Proofs by Contraposition\n\n\\begin{theorem}\nLet $T$ be a tree.\nThen there is exactly one path between any two vertices.\n\\end{theorem}\n\n\\begin{proof}\nLet $T$ be a tree.\n{{AimForCont}} there exists a pair of vertices $u$ and $v$ in $T$ such that there is not exactly one path between them.\nIf there is no path between $u$ and $v$, $T$ is not connected.\nIn this case, $T$ is certainly not a tree.\nSo, in keeping with our supposition, there is more than one path between $u$ and $v$.\nLet two of these paths be:\n:$P_1 = \\tuple {u, u_1, \\ldots, u_i, r_1, r_2, \\ldots, r_{j - 1}, r_j, u_{i + 1}, \\ldots, v}$\n:$P_2 = \\tuple {u, u_1, \\ldots, u_i, s_1, s_2, \\ldots, s_{k - 1}, s_k, u_{i + 1}, \\ldots, v}$\nNow consider the path:\n:$P_3 = \\tuple {u_i, r_1, r_2, \\ldots, r_{j - 1}, r_j, u_{i + 1}, s_k, s_{k - 1}, \\ldots, s_2, s_1, u_i}$\nIt can be seen that $P_3$ is a circuit.\nThus by definition $T$ can not be a tree.\nFrom Proof by Contradiction it follows that there is exactly one path between any pair of vertices.\n{{qed}}\n\\end{proof}\n\n"}}, "21475": {"score": 0.8805125951766968, "content": {"text": "\\section{Size of Tree is One Less than Order/Sufficient Condition}\nTags: Size of Tree is One Less than Order, Tree Theory, Trees\n\n\\begin{theorem}\nLet $T$ be a connected simple graph of order $n$.\nLet the size of $T$ be $n-1$.\nThen $T$ is a tree.\n\\end{theorem}\n\n\\begin{proof}\nBy definition, the order of a tree is how many nodes it has, and its size is how many edges it has.\nSuppose $T$ is a connected simple graph of order $n$ with $n - 1$ edges.\nWe need to show that $T$ is a tree.\n{{AimForCont}} $T$ is not a tree.\nThen it contains a circuit.\nIt follows from Condition for Edge to be Bridge that there is at least one edge in $T$ which is not a bridge.\nSo we can remove this edge and obtain a graph $T'$ which is connected and has $n$ nodes and $n - 2$ edges.\nLet us try and construct a connected graph with $n$ nodes and $n - 2$ edges.\nWe start with the edgeless graph $N_n$, and add edges till the graph is connected.\nWe pick any two vertices of $N_n$, label them $u_1$ and $u_2$ for convenience, and use one edge to connect them, labelling that edge $e_1$.\nWe pick any other vertex, label it $u_3$, and use one edge to connect it to either $u_1$ or $u_2$, labelling that edge $e_2$.\nWe pick any other vertex, label it $u_4$, and use one edge to connect it to either $u_1, u_2$ or $u_3$, labelling that edge $e_3$.\nWe continue in this way, until we pick a vertex, label it $u_{n - 1}$, and use one edge to connect it to either $u_1, u_2, \\ldots, u_{n - 2}$, labelling that edge $e_{n - 2}$.\nThat was the last of our edges, and the last vertex still has not been connected.\nTherefore a graph with $n$ vertices and $n-2$ edges that such a graph ''cannot'' be connected.\nTherefore we cannot remove any edge from $T$ without leaving it disconnected.\nTherefore all the edges in $T$ are bridges.\nHence $T$ can contain no circuits.\nHence, by Proof by Contradiction, $T$ must be a tree.\n{{qed}}\n\\end{proof}\n\n"}}, "3489": {"score": 0.8773112893104553, "content": {"text": "\\begin{definition}[Definition:Forest]\nA '''forest''' is a simple graph whose components are all trees.\nA '''connected forest''' is just a single tree.\n\\end{definition}"}}, "9568": {"score": 0.8910118937492371, "content": {"text": "\\begin{definition}[Definition:Tree (Graph Theory)/Definition 1]\nA '''tree''' is a simple connected graph with no circuits.\n:300px\n\\end{definition}"}}, "14704": {"score": 0.8819646239280701, "content": {"text": "\\section{Edgeless Graph of Order 1 is Tree}\nTags: Tree Theory, Edgeless Graphs\n\n\\begin{theorem}\nLet $N_1$ denote the edgeless graph with $1$ vertex.\nThen $N_1$ is a tree.\n\\end{theorem}\n\n\\begin{proof}\nBy definition, a tree is a simple connected graph with no circuits.\n$N_1$ is trivially connected graph.\nAs $N_1$ is edgeless, it has no edges.\nBut a circuit is a closed trail with at least one edge.\nHence the result.\n{{qed}}\nCategory:Edgeless Graphs\nCategory:Tree Theory\n\\end{proof}\n\n"}}, "14217": {"score": 0.9005665183067322, "content": {"text": "\\section{Equivalent Definitions for Finite Tree}\nTags: Tree Theory, Graph Theory, Trees\n\n\\begin{theorem}\nLet $T$ be a finite tree of order $n$.\nThe following statements are equivalent:\n: $(1): \\quad T$ is connected and has no circuits.\n: $(2): \\quad T$ has $n-1$ edges and has no circuits.\n: $(3): \\quad T$ is connected and has $n-1$ edges.\n: $(4): \\quad T$ is connected, and the removal of any one edge renders $T$ disconnected.\n: $(5): \\quad$ Any two vertices of $T$ are connected by exactly one path.\n: $(6): \\quad T$ has no circuits, but adding one edge creates a cycle.\n\\end{theorem}\n\n\\begin{proof}\nStatement $1$ is the usual definition of a tree.\n\\end{proof}\n\n"}}, "17644": {"score": 0.8838852047920227, "content": {"text": "\\section{Adding Edge to Tree Creates One Cycle}\nTags: Tree Theory, Graph Theory, Trees, Tree Treory\n\n\\begin{theorem}\nAdding a new edge to a tree can create no more than one cycle.\n\\end{theorem}\n\n\\begin{proof}\nFrom Equivalent Definitions for Finite Tree, adding an edge creates at least one cycle.\nSuppose that adding an edge $\\tuple {u, v}$ to a tree $T$ creates two or more cycles.\nLet two such cycles be $\\tuple {u, v, \\ldots, u_1, u_2, \\ldots, u}$ and $\\tuple {u, v, \\ldots, v_1, v_2, \\ldots, u}$.\nBy removing the edge $\\tuple {u, v}$ from this cycle, we have two paths from $v$ to $u$:\n:$\\tuple {v, \\ldots, u_1, u_2, \\ldots, u}$\n:$\\tuple {v, \\ldots, v_1, v_2, \\ldots, u}$.\nBut that means $T$ has more than one path between two nodes.\nFrom Path in Tree is Unique, that means $T$ can not be a tree.\nHence the result.\n{{qed}}\nCategory:Tree Theory\n\\end{proof}\n\n"}}, "9569": {"score": 0.8889775276184082, "content": {"text": "\\begin{definition}[Definition:Tree (Graph Theory)/Definition 2]\nA '''tree''' is a simple connected graph with no cycles.\n\\end{definition}"}}}}, "TheoremQA_elainewan/econ_micro_7.json": {"gold": {"7665": 1}, "retrieved": {"9047": {"score": 0.7893478870391846, "content": {"text": "\\begin{definition}[Definition:Subjective Probability Distribution]\nA '''subjective probability distribution''' is a probability distribution based on the beliefs of a rational decision-maker about all relevant unknown factors concerning a game.\nAs new information becomes available, the '''subjective probabilities''' can then revised according to Bayes' Formula.\n\\end{definition}"}}, "6279": {"score": 0.8001505732536316, "content": {"text": "\\begin{definition}[Definition:Objective Unknown]\nAn '''objective unknown''' is an event which has a well-defined objective probability of occurring.\nTwo '''objective unknowns''' with the same probability are equivalent in the field of decision theory.\nAn '''objective unknown''' is appropriately modelled by means of a probability model.\n\\end{definition}"}}, "5359": {"score": 0.8204938173294067, "content": {"text": "\\begin{definition}[Definition:Lottery]\nA '''lottery''' is a game in which the consequence of each move is determined by the realization of a random variable.\nLet $X$ denote the set of prizes which a player may receive.\nLet $\\Omega$ denote the set of possible states.\nA '''lottery''' is a mapping $f: X \\times \\Omega \\to \\R_{\\ge 0}$ such that:\n:$\\forall t \\in \\Omega: \\ds \\sum_{x \\mathop \\in X} \\map f {x, t} = 1$\n\\end{definition}"}}, "540": {"score": 0.8119609951972961, "content": {"text": "\\begin{definition}[Definition:Bayesian Decision Theory]\n'''Bayesian decision theory''' is a branch of decision theory which is informed by Bayesian probability.\nIt is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs.\n\\end{definition}"}}, "13874": {"score": 0.8037316799163818, "content": {"text": "\\section{Expectation of Bernoulli Distribution}\nTags: Expectation, Expectation of Bernoulli Distribution, Bernoulli Distribution\n\n\\begin{theorem}\nLet $X$ be a discrete random variable with a Bernoulli distribution with parameter $p$.\nThen the expectation of $X$ is given by:\n:$\\expect X = p$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of expectation:\n:<math>E \\left({X}\\right) = \\sum_{x \\in \\operatorname{Im} \\left({X}\\right)} x \\Pr \\left({X = x}\\right)</math>\nBy definition of Bernoulli distribution:\n:<math>E \\left({X}\\right) = 1 \\times p + 0 \\times \\left({1-p}\\right)</math>\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "5361": {"score": 0.8424725532531738, "content": {"text": "\\begin{definition}[Definition:Lottery/State-Variable Model]\nA '''state-variable model''' is a technique to model decisions under uncertainty.\nIn a '''state-variable model''', a lottery is defined as a mapping from a set of possible states into a set of prizes.\n\\end{definition}"}}, "5362": {"score": 0.826159656047821, "content": {"text": "\\begin{definition}[Definition:Lottery Induced by Preference Relation]\nLet $G$ be a game.\nLet $N$ be the set of players of $G$.\nLet $A$ be the set of moves available to player $i \\in N$.\nLet $C$ be the set of consequences of those moves.\nLet the consequences of those moves be affected by a random variable on a probability space $\\Omega$ whose realization is not known to the players before they make their moves.\nLet $g: A \\times \\Omega \\to C$ be the consequence function for player $i$.\nThen the '''lottery on $C$ induced by the profile of preference relations over $C$''' is defined by:\n:$\\forall a, b \\in A: \\left({a \\succsim_i b}\\right) \\iff \\left({g \\left({a, \\omega_a}\\right) \\succsim_i^* g \\left({b, \\omega_b}\\right)}\\right)$\nwhere $\\succsim_i$ is the preference relation for player $i$.\n\\end{definition}"}}, "5360": {"score": 0.8451226353645325, "content": {"text": "\\begin{definition}[Definition:Lottery/Probability Model]\nA '''probability model''' is a technique to model decisions under uncertainty.\nIn a '''probability model''', a lottery is a probability distribution over a set of prizes.\n\\end{definition}"}}, "7322": {"score": 0.8290425539016724, "content": {"text": "\\begin{definition}[Definition:Prize]\nLet $G$ be a game.\nA prize in $G$ is a payoff whose utility value is strictly positive.\nThe term is usually seen in the context of a lottery.\n\\end{definition}"}}, "1585": {"score": 0.8412764668464661, "content": {"text": "\\begin{definition}[Definition:Conditional Preference]\nLet $G$ be a lottery.\nLet $P$ be a player of $G$.\nLet $X$ denote the set of prizes which $P$ may receive.\nLet $\\Omega$ denote the set of possible states of $G$.\nLet $\\Xi$ be the event space of $G$.\nLet $L$ be the set of all plays of $G$.\nLet $f$ and $g$ be two lotteries in $G$.\nLet $S \\subseteq \\Xi$ be an event.\nA '''conditional preference''' is a preference relation $\\succsim_S$ such that:\n:$f \\succsim_S g$ {{iff}} $f$ would be at least as desirable to $P$ as $g$, if $P$ was aware that the true state of the world was $S$.\nThat is, $f \\succsim_S g$ {{iff}} $P$ prefers $f$ to $g$ and he knows only that $S$ has occurred.\nThe notation $a \\sim_S b$ is defined as:\n:$a \\sim_S b$ {{iff}} $a \\succsim_S b$ and $b \\succsim_S a$\nThe notation $a \\succ_S b$ is defined as:\n:$a \\succ_S b$ {{iff}} $a \\succsim_S b$ and $a \\not \\sim_S a$\nWhen no conditioning event $S$ is mentioned, the notation $a \\succsim_\\Omega b$, $a \\succ_\\Omega b$ and $a \\sim_\\Omega b$ can be used, which mean the same as $a \\succsim b$, $a \\succ b$ and $a \\sim b$.\n{{handwaving|Myerson is as lax as all the other game theory writers when it comes to defining rigorous concepts. I am going to have to abandon this field of study until I really understand exactly what the underlying mathematical objects are.}}\n\\end{definition}"}}}}, "TheoremQA_xinyi/dag_2.json": {"gold": {"3778": 1, "23699": 1}, "retrieved": {"305": {"score": 0.842637836933136, "content": {"text": "\\begin{definition}[Definition:Arborescence/Definition 2]\nLet $G = \\left({V, A}\\right)$ be a directed graph.\nLet $r \\in V$.\n$G$ is an '''arborescence of root $r$''' {{iff}}:\n: $(1): \\quad$ $G$ is an orientation of a tree\n: $(2): \\quad$ For each $v \\in V$, $v$ is reachable from $r$.\n\\end{definition}"}}, "306": {"score": 0.851134181022644, "content": {"text": "\\begin{definition}[Definition:Arborescence/Definition 3]\nLet $G = \\struct {V, A}$ be a directed graph.\nLet $r \\in V$.\n$G$ is an '''arborescence of root $r$''' {{iff}}:\n: $(1): \\quad$ Each vertex $v \\ne r$ is the final vertex of exactly one arc.\n: $(2): \\quad$ $r$ is not the final vertex of any arc.\n: $(3): \\quad$ For each $v \\in V$ such that $v \\ne r$ there is a directed walk from $r$ to $v$.\n\\end{definition}"}}, "302": {"score": 0.8581588864326477, "content": {"text": "\\begin{definition}[Definition:Arborescence/Also defined as]\nSome sources, for example {{BookReference|The Art of Computer Programming: Volume 1: Fundamental Algorithms||Donald E. Knuth}}, define an arborescence of root $r$ so as to reverse the orientation of $G$, so that the arcs are all directed toward the root rather than away from it.\nCategory:Definitions/Arborescences\n\\end{definition}"}}, "2455": {"score": 0.8539993762969971, "content": {"text": "\\begin{definition}[Definition:Directed Graph/Arc]\nLet $G = \\struct {V, E}$ be a digraph.\nThe '''arcs''' are the elements of $E$.\nInformally, an '''arc''' is a line that joins one vertex to another.\nIf $e \\in E$ is an '''arc''' joining the vertex $u$ to the vertex $v$, it is denoted $u v$.\n\\end{definition}"}}, "307": {"score": 0.8523369431495667, "content": {"text": "\\begin{definition}[Definition:Arborescence/Root]\nLet $G = \\left({V, A}\\right)$ be a directed graph.\nLet $r \\in V$.\nLet $G$ be an arborescence of $r$.\nThe vertex $r$ of $G$ is known as the '''root of (the arborescence) $G$'''.\nCategory:Definitions/Arborescences\n\\end{definition}"}}, "2454": {"score": 0.8657076954841614, "content": {"text": "\\begin{definition}[Definition:Directed Graph]\nA '''directed graph''' or '''digraph''' is a graph each of whose edges has a '''direction''':\n310px\nIn the above graph, the vertices are $v_1, v_2, v_3$ and $v_4$.\n\\end{definition}"}}, "303": {"score": 0.8603408336639404, "content": {"text": "\\begin{definition}[Definition:Arborescence/Also known as]\nAn arborescence of root $r$ can be referred to as an an '''$r$-arborescence''', or just an '''arborescence'''.\nSome sources, for example {{BookReference|The Art of Computer Programming: Volume 1: Fundamental Algorithms||Donald E. Knuth}}, call an arborescence an '''oriented tree'''.\nCategory:Definitions/Arborescences\n\\end{definition}"}}, "20765": {"score": 0.8744655847549438, "content": {"text": "\\section{Rooted Tree Corresponds to Arborescence}\nTags: Rooted Trees, Graph Theory\n\n\\begin{theorem}\nLet $T = \\struct {V, E}$ be a rooted tree with root $r$.\nThen there is a unique orientation of $T$ which is an $r$-arborescence.\n\\end{theorem}\n\n\\begin{proof}\nRecall that a tree is connected and has no cycles.\nThus there is exactly one path from each node of $T$ to each other node of $T$.\n{{explain|This is in fact a result that already exists and can be quoted directly.}}\nLet $A$ be the set of all ordered pairs $x, y \\in V$ such that:\n:$\\tuple {x, y} \\in E$ and\n:The unique path from $r$ to $y$ passes through $x$.\n{{finish}}\n\\end{proof}\n\n"}}, "2459": {"score": 0.8608877658843994, "content": {"text": "\\begin{definition}[Definition:Directed Graph/Formal Definition]\n310px\nA '''directed graph''' or '''digraph''' $D$ is a non-empty set $V$ together with an antireflexive relation $E$ on $V$.\nThe elements of $E$ are the arcs.\nThus the above '''digraph''' can be defined as:\n:$D = \\struct {V, E}:$\n::$V = \\set {v_1, v_2, v_3, v_4}$\n::$E = \\set {\\tuple {v_1, v_2}, \\tuple {v_2, v_4}, \\tuple {v_4, v_3}, \\tuple {v_4, v_1}, \\tuple {v_1, v_4} }$\n\\end{definition}"}}, "6025": {"score": 0.8609598278999329, "content": {"text": "\\begin{definition}[Definition:Network/Directed]\nA '''directed network''' is a network whose underlying graph is a digraph:\n:320px\n\\end{definition}"}}}}, "TheoremQA_jianyu_xu/Stirling_number_second_kind_5.json": {"gold": {"8903": 1}, "retrieved": {"19708": {"score": 0.8203122615814209, "content": {"text": "\\section{Product Rule for Counting}\nTags: Product Rule for Counting, Counting Arguments, Combinatorics, combinatorics\n\n\\begin{theorem}\nLet it be possible to choose an element $\\alpha$ from a given set $S$ in $m$ different ways.\nLet it be possible to choose an element $\\beta$ from a given set $T$ in $n$ different ways.\nThen the ordered pair $\\tuple {\\alpha, \\beta}$ can be chosen from the cartesian product $S \\times T$ in $m n$ different ways.\n\\end{theorem}\n\n\\begin{proof}\n{{handwaving}}\nThe validity of this rule follows directly from the definition of multiplication of integers.\nThe product $a b$ (for $a, b \\in \\N_{>0}$) is the number of sequences $\\sequence {A, B}$, where $A$ can be any one of $a$ items and $B$ can be any one of $b$ items.\n{{qed}}\n\\end{proof}\n\n"}}, "17192": {"score": 0.8203513622283936, "content": {"text": "\\section{Binomial Coefficient/Examples/Number of Bridge Hands}\nTags: Binomial Coefficients, Examples of Binomial Coefficients\n\n\\begin{theorem}\nThe total number $N$ of possible different hands for a game of [https://en.wikipedia.org/wiki/Contract_bridge bridge] is:\n:$N = \\dfrac {52!} {13! \\, 39!} = 635 \\ 013 \\ 559 \\ 600$\n\\end{theorem}\n\n\\begin{proof}\nThe total number of cards in a standard deck is $52$.\nThe number of cards in a single bridge hand is $13$.\nThus $N$ is equal to the number of ways $13$ things can be chosen from $52$.\nThus:\n{{begin-eqn}}\n{{eqn | l = N\n      | r = \\dbinom {52} {23}\n      | c = Cardinality of Set of Subsets\n}}\n{{eqn | r = \\frac {52!} {13! \\left({52 - 13}\\right)!}\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\frac {52!} {13! \\, 39!}\n      | c = \n}}\n{{eqn | r = 635 \\ 013 \\ 559 \\ 600\n      | c = after calculation\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "16939": {"score": 0.8287886381149292, "content": {"text": "\\section{Cardinality of Set of Combinations with Repetition}\nTags: Combinations with Repetition\n\n\\begin{theorem}\nLet $S$ be a finite set with $n$ elements\nThe number of $k$-combinations of $S$ with repetition is given by:\n:$N = \\dbinom {n + k - 1} k$\n\\end{theorem}\n\n\\begin{proof}\nLet the elements of $S$ be (totally) ordered in some way, by assigning an index to each element.\nThus let $S = \\left\\{ {a_1, a_2, a_3, \\ldots, a_n}\\right\\}$.\nThus each $k$-combination of $S$ with repetition can be expressed as:\n:$\\left\\{ {\\left\\{ {a_{r_1}, a_{r_1}, \\ldots, a_{r_k} }\\right\\} }\\right\\}$\nwhere:\n:$r_1, r_2, \\ldots, r_k$ are all elements of $\\left\\{ {1, 2, \\ldots, n}\\right\\}$\nand such that:\n:$r_1 \\le r_2 \\le \\cdots \\le r_k$\nHence the problem reduces to the number of integer solutions $\\left({r_1, r_2, \\ldots, r_k}\\right)$ such that $1 \\le r_1 \\le r_2 \\le \\cdots \\le r_k \\le n$.\nThis is the same as the number of solutions to:\n:$0 < r_1 < r_2 + 1 < \\cdots < r_k + k - 1 < n + k$\nwhich is the same as the number of solutions to:\n:$0 < s_1 < s_2 < \\cdots < s_k < n + k$\nwhich is the number of ways to choose $k$ elements from $n + k - 1$.\nThis is the same as the number of subsets as a set with $n + k - 1$ elements.\nFrom Cardinality of Set of Subsets:\n:$N = \\dbinom {n + k - 1} k$\n{{qed}}\n\\end{proof}\n\n"}}, "17224": {"score": 0.8281657099723816, "content": {"text": "\\section{Birthday Paradox/General/3}\nTags: Birthday Paradox\n\n\\begin{theorem}\nLet $n$ be a set of people.\nLet the probability that at least $3$ of them have the same birthday be greater than $50 \\%$.\nThen $n \\ge 88$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map F {r, n}$ be the number of ways to distribute $r$ objects into $n$ cells such that there are no more than $2$ objects in each cell.\nLet there be $d$ cells which are each occupied by $2$ objects.\nThese can be chosen in $\\dbinom n d$ ways.\nThere remain $s = r - 2 d$ objects which can then be distributed among $n - d$ cells in $\\dbinom {n - d} s$ ways.\nIn each such arrangement, the $r$ objects may be permuted in:\n:$\\dbinom r 2 \\dbinom {r - 2} 2 \\cdots \\dbinom {r - 2 d + 2} 2 \\paren {r - 2 d}! = \\dfrac {r!} {2^d}$\ndifferent ways.\nHence:\n:$\\map F {r, n} = \\dbinom n d \\dbinom {n - d} s \\dfrac {r!} {2^d}$\nSo the probability of exactly $d$ pairs and $s$ singletons, where $d - s \\le n$, is given by:\n:$\\dfrac {\\map F {r, n} } {n^r}$\nIf we assume a $365$-day year, we have that the probability that at least $3$ of them have the same birthday is given by:\n:$\\map \\Pr r = 1 - \\ds \\sum_{d \\mathop = 0}^{\\floor {r / 2} } \\dfrac {n! \\, r!} {n^r 2^d d! \\paren {r - 2 d}! \\paren {n + d - r}!}$\nwhere $n = 365$.\nWe require the smallest $r$ for which $\\map \\Pr r > \\dfrac 1 2$.\nThe result yields to calculation.\n{{qed}}\n\\end{proof}\n\n"}}, "22117": {"score": 0.8224775195121765, "content": {"text": "\\section{Sum Rule for Counting}\nTags: combinatorics, Counting Arguments, counting arguments, Combinatorics\n\n\\begin{theorem}\nLet there be:\n:$r_1$ different objects in the set $S_1$\n:$r_2$ different objects in the set $S_2$\n:$\\ldots$\n:$r_m$ different objects in the set $S_m$.\nLet $\\ds \\bigcap_{i \\mathop = 1}^m S_i = \\O$.\nThen the number of ways to select an object from one of the $m$ sets is $\\ds \\sum_{i \\mathop = 1}^m r_i$.\n\\end{theorem}\n\n\\begin{proof}\nA direct application of Cardinality of Set Union.\n{{qed}}\n\\end{proof}\n\n"}}, "17948": {"score": 0.8410674929618835, "content": {"text": "\\section{Number of Set Partitions by Number of Components}\nTags: Set Partitions, Number of Set Partitions, Stirling Numbers, Combinatorics, Number of Set Partitions by Number of Components\n\n\\begin{theorem}\nLet $S$ be a (finite) set whose cardinality is $n$.\nLet $\\map f {n, k}$ denote the number of different ways $S$ can be partitioned into $k$ components.\nThen:\n:$\\ds \\map f {n, k} = {n \\brace k}$\nwhere $\\ds {n \\brace k}$ denotes a Stirling number of the second kind.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction on $n$.\nFor all $n \\in \\Z_{\\ge 0}$, let $\\map P n$ be the proposition:\n:$\\ds \\map f {n, k} = {n \\brace k}$\n$\\map P 0$ is the degenerate case:\n:$\\ds \\map f {0, k} = \\delta_{0 k} = {0 \\brace k}$\nThat is: the empty set can be partitioned one and only one way: into $0$ subsets.\nThus $\\map P 0$ is seen to hold.\nThe remainder of the proof considers $n \\in \\Z_{> 0}$.\nFirst we note that when $k < 1$ or $k > n$:\n:$\\ds \\map f {n, k} = 0 = {n \\brace k}$\nHence, throughout, we consider only such $k$ as $1 \\le k \\le n$.\nWe define the representative set of cardinality $n$ to be:\n:$S_n := \\set {1, 2, \\ldots, n}$\n\\end{proof}\n\n"}}, "6334": {"score": 0.8303124904632568, "content": {"text": "\\begin{definition}[Definition:One Hundred Fowls Problem]\n'''One Hundred Fowls problems''' are variants of the classic problem:\n{{:One Hundred Fowls}}\n\\end{definition}"}}, "17901": {"score": 0.8474652171134949, "content": {"text": "\\section{Number of Compositions}\nTags: Combinatorics\n\n\\begin{theorem}\nA $k$-composition of a positive integer $n$ is an ordered $k$-tuple: $c = \\tuple {c_1, c_2, \\ldots, c_k}$ such that $c_1 + c_2 + \\cdots + c_k = n$ and $c_i $ are strictly positive integers.\nThe number of $k$-composition of $n$ is $\\dbinom {n - 1} {k - 1}$ and the total number of compositions of $n$ is $2^{n - 1}$ (that is for $k = 1, 2, 3, \\ldots, n$).\n\\end{theorem}\n\n\\begin{proof}\nConsider the following array consisting of $n$ ones and $n - 1$ blanks:\n:$\\begin{bmatrix} 1 \\ \\_ \\ 1 \\ \\_ \\ \\cdots \\ \\_ \\ 1 \\ \\_ \\ 1 \\end{bmatrix}$\nIn each blank we can either put a comma or a plus sign.\nEach way of choosing $,$ or $+$ will give a composition of $n$ with the commas separating the individual $c_i$'s.\nIt follows easily that there are $2^{n-1}$ ways of doing this, since there are two choices for each of $n-1$ blanks.\nThe result follows from the Product Rule for Counting.\nSimilarly if we want specifically $k$ different $c_i$'s then we are left with choosing $k - 1$ out of $n - 1$ blanks to place the $k - 1$ commas.\nThe number of ways of doing so is $\\dbinom {n - 1} {k - 1}$ from the Binomial Theorem.\n{{qed}}\nCategory:Combinatorics\n\\end{proof}\n\n"}}, "20393": {"score": 0.8316447734832764, "content": {"text": "\\section{Recurrence Relation for Bell Numbers}\nTags: Bell Numbers\n\n\\begin{theorem}\nLet $B_n$ be the Bell number for $n \\in \\Z_{\\ge 0}$.\nThen:\n:$B_{n + 1} = \\ds \\sum_{k \\mathop = 0}^n \\dbinom n k B_k$\nwhere $\\dbinom n k$ are binomial coefficients.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of Bell numbers:\n:$B_{n + 1}$ is the number of partitions of a (finite) set whose cardinality is $n + 1$.\nLet $k \\in \\set {k \\in \\Z: 0 \\le k \\le n}$.\nLet us form a partition of a (finite) set $S$ with cardinality $n + 1$ such that one component has $n + 1 - k > 0$ elements.\nWe can do this by first choosing $1$ element from $S$. We put this element into that single component.\nThen choose $k$ more elements from $S$, and let the remaining $n - k$ elements be put into the same component as the first element.\nFrom Cardinality of Set of Subsets and the definition of binomial coefficient, there are $\\dbinom n k$ ways to do this.\nFor the chosen $k$ elements, there are $B_k$ ways to partition them.\nThus there are $\\dbinom n k B_k$ possible partitions for $S$:\n:$\\dbinom n k$ of selecting $n - k$ elements to form one component with the one singled-out element\n:for each of these, $B_k$ ways to partition the remaining $k$ elements.\nSumming the number of ways to do this over all possible $k$:\n:$\\ds B_{n + 1} = \\sum_{k \\mathop = 0}^n \\dbinom n k B_k$\n{{qed}}\nCategory:Bell Numbers\n\\end{proof}\n\n"}}, "17916": {"score": 0.8366665840148926, "content": {"text": "\\section{Number of Elements in Partition}\nTags: Combinatorics\n\n\\begin{theorem}\nLet $S$ be a set.\nLet there be a partition on $S$ of $n$ subsets, each of which has $m$ elements.\nThen:\n:$\\card S = n m$\n\\end{theorem}\n\n\\begin{proof}\nLet the partition of $S$ be $S_1, S_2, \\ldots, S_n$.\nThen:\n:$\\forall k \\in \\set {1, 2, \\ldots, n}: \\card {S_k} = m$\nBy definition of multiplication:\n:$\\ds \\sum_{k \\mathop = 1}^n \\card {S_k} = n m$\nand the result follows from the Fundamental Principle of Counting.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Ramsey_3.json": {"gold": {"20117": 1, "7627": 1}, "retrieved": {"18389": {"score": 0.8601967692375183, "content": {"text": "\\section{Ore Graph is Connected}\nTags: Graph Theory, Ore Graphs\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be an Ore graph.\nThen $G$ is connected.\n\\end{theorem}\n\n\\begin{proof}\nLet $G$ be an Ore graph.\n{{AimForCont}} $G$ is not connected.\nThen it has at least two components.\nCall these components $C_1$ and $C_2$.\nThus, there exist non-adjacent vertices $u$ and $v$ such that $u$ is in $C_1$ and $v$ is in $C_2$.\nLet $m_1$ and $m_2$ be the number of vertices in $C_1$ and $C_2$ respectively.\nIt is clear that:\n:$m_1 + m_2 \\le n$\nBy definition of Ore graph, $G$ is simple.\nThus it follows that:\n:$\\map {\\deg_G} u \\le m_1 - 1$ and $\\map {\\deg_G} v \\le m_2 - 1$\nThus:\n{{begin-eqn}}\n{{eqn | l = \\map {\\deg_G} u + \\map {\\deg_G} v\n      | o = \\le\n      | r = m_1 - 1 + m_2 - 1\n      | c = \n}}\n{{eqn | r = m_1 + m_2 - 2\n      | c = \n}}\n{{eqn | o = <\n      | r = m_1 + m_2\n      | c = \n}}\n{{eqn | o = \\le\n      | r = n\n      | c = \n}}\n{{end-eqn}}\nThat is:\n:$\\map {\\deg_G} u + \\map {\\deg_G} v < n$\nBut by definition of Ore graph:\n:$\\map {\\deg_G} u + \\map {\\deg_G} v \\ge n$\nBy Proof by Contradiction, it follows that our assumption that $G$ is not connected was false.\nHence the result.\n{{Qed}}\n\\end{proof}\n\n"}}, "15419": {"score": 0.8644265532493591, "content": {"text": "\\section{Cycle does not Contain Subcycles}\nTags: Graph Theory\n\n\\begin{theorem}\nLet $G$ be a cycle graph.\nThen the only cycle graph that is a subgraph of $G$ is $G$ itself.\n\\end{theorem}\n\n\\begin{proof}\n{{AimForCont}} that $G$ contains a subgraph $C$ such that:\n:$C$ is a cycle graph\n:$C \\ne G$ is non-empty.\nThen there exists some vertex $v$ that is not in $C$.\nLet $u$ be any vertex of $C$.\nSince $G$ is a cycle graph, it is connected.\nTherefore there is a walk from $u$ to $v$ in $G$.\nThere must be some vertex $x$ that is the last vertex in $C$ along that walk.\nTherefore, $x$ is adjacent to a vertex not in $C$.\nThus it has a degree of at least $3$.\nBut $G$ is a cycle graph and every vertex in a cycle graph has degree $2$.\nThe result follows by Proof by Contradiction.\n{{Qed}}\nCategory:Graph Theory\n\\end{proof}\n\n"}}, "21375": {"score": 0.8775275945663452, "content": {"text": "\\section{Simple Graph whose Vertices Incident to All Edges}\nTags: Graph Theory\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph whose vertices are incident to all its edges.\nThen $G$ is either:\n:the star graph $S_2$, which is also the complete graph $K_2$\n:an edgeless graph of any order.\n\\end{theorem}\n\n\\begin{proof}\nIf $G$ has no edges, then all the vertices are incident to all the edges vacuously.\nSo any of the edgeless graphs $N_n$ for order $n \\in \\Z_{\\ge 0}$ fulfils the criterion.\nSuppose $G$ has more than $2$ vertices $v_1, v_2, v_3$ and at least one edge.\n{{WLOG}}, let one edge be $v_1 v_2$.\nBut $v_3$ cannot be incident to edge $v_1 v_2$.\nSo $G$ can have no more than $2$ vertices.\nFurthermore, there can be only one edge joining those two vertices.\nThe result follows from inspection of $K_2$ and $S_2$.\n{{qed}}\n\\end{proof}\n\n"}}, "9598": {"score": 0.8752860426902771, "content": {"text": "\\begin{definition}[Definition:Triangle (Graph Theory)]\nThe complete graph $K_3$ of order $3$ is called a '''triangle'''.\n:250px\nCategory:Definitions/Graph Theory\n\\end{definition}"}}, "1433": {"score": 0.8659524917602539, "content": {"text": "\\begin{definition}[Definition:Complete Graph]\nLet $G = \\struct {V, E}$ be a simple graph such that every vertex is adjacent to every other vertex.\nThen $G$ is called '''complete'''.\nThe '''complete graph''' of order $p$ is denoted $K_p$.\n\\end{definition}"}}, "15417": {"score": 0.8937743902206421, "content": {"text": "\\section{Cycle Graph of Order 3 is Complete Graph}\nTags: Examples of Cycle Graphs, Examples of Complete Graphs\n\n\\begin{theorem}\nLet $C_3$ denote the cycle graph of order $2$.\nThen $C_3$ is the complete graph of of order $3$.\n\\end{theorem}\n\n\\begin{proof}\nLet the vertex set of $C_3$ is $\\set {v_1, v_2, v_3}$.\nBy definition of cycle graph, $C_3$ consists of the cycle $v_1 v_2 v_3 v_1$.\nIt is seen by inspection that:\n:$v_1$ is adjacent to $v_2$ and $v_3$\n:$v_2$ is adjacent to $v_1$ and $v_3$\n:$v_3$ is adjacent to $v_1$ and $v_2$.\nHence the result by definition of complete graph.\n{{qed}}\nCategory:Examples of Cycle Graphs\nCategory:Examples of Complete Graphs\n\\end{proof}\n\n"}}, "21376": {"score": 0.8803563714027405, "content": {"text": "\\section{Simple Graph whose Vertices all Incident but Edges not Adjacent}\nTags: Simple Graphs\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph such that:\n:every vertex is incident with at least one edge\n:no two edges are adjacent to each other.\nThen $G$ has an even number of vertices.\n\\end{theorem}\n\n\\begin{proof}\nSuppose there exists a set of $3$ vertices that are connected.\nThen at least one of these vertices has at least $2$ edges.\nThat would mean that at least $2$ edges were incident with the same vertex.\nThat is, that at least $2$ edges were adjacent to each other.\nSo, for a simple graph to fulfil the conditions, vertices can exist only in $2$-vertex components.\nSo such a simple graph must have an even number of vertices.\n{{qed}}\n\\end{proof}\n\n"}}, "21374": {"score": 0.9196874499320984, "content": {"text": "\\section{Simple Graph where All Vertices and All Edges are Adjacent}\nTags: Simple Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph in which:\n:every vertex is adjacent to every other vertex\nand:\n:every edge is adjacent to every other edge.\nThen $G$ is of order no greater than $3$.\n\\end{theorem}\n\n\\begin{proof}\nIt is seen that examples exist of simple graphs which fulfil the criteria where the order of $G$ is no greater than $3$:\n:400px\nThe cases where the order of $G$ is $1$ or $2$ are trivial.\nWhen the order of $G$ is $3$, the criteria can be verified by inspection.\nLet the order of $G = \\struct {V, E}$ be $4$ or more.\nLet $v_1, v_2, v_3, v_4 \\in V$.\nSuppose every vertex is adjacent to every other vertex.\nAs $v_1$ is adjacent to $v_2$, there exists the edge $v_1 v_2$.\nAs $v_3$ is adjacent to $v_4$, there exists the edge $v_3 v_4$.\nBut $v_1 v_2$ and $v_3 v_4$ both join two distinct pairs of vertices.\nThus $v_1 v_2$ and $v_3 v_4$ are not adjacent, by definition.\nSo when there are $4$ or more vertices in $G$, it cannot fulfil the criteria.\n{{qed}}\n\\end{proof}\n\n"}}, "11719": {"score": 0.8827263116836548, "content": {"text": "\\section{Isomorphism Classes for Order 4 Size 3 Simple Graphs}\nTags: Graph Isomorphisms\n\n\\begin{theorem}\nThere are $3$ equivalence classes for simple graphs of order $4$ and size $3$ under isomorphism:\n:400px\n\\end{theorem}\n\n\\begin{proof}\n{{tidy}}\n{{MissingLinks}}\nThe fact that the $3$ graphs given are not isomorphic follows from Vertex Condition for Isomorphic Graphs.\nThe vertices have degrees as follows:\n:Graph $1$: $2, 2, 1, 1$\n:Graph $2$: $3, 1, 1, 1$\n:Graph $3$: $2, 2, 2, 0$\nThe fact that there are no more isomorphism classes of such graphs can be proved constructively.\nLet the $4$ vertices be named $A, B, C$ and $D$.\nLemma: There must be intersections among the edges.\nProof: If there were no intersection at all, it requires at least $3 \\times 2 = 6$ vertices.\n{{WLOG}}, let $2$ of the edges be $AB$ and $AC$.\nTo place the last edge, there are $\\dbinom 4 2 = 6$ potential choices:\n:$AB$: this makes the graph not simple\n:$AC$: this makes the graph not simple\n:$AD$: this is isomorphic to graph $2$\n:$BC$: this is isomorphic to graph $3$\n:$BD$: this is isomorphic to graph $1$\n:$CD$: this is isomorphic to graph $1$.\nHence, by Proof by Cases, these $3$ are the only isomorphism classes.\n{{qed}}\n\\end{proof}\n\n"}}, "23824": {"score": 0.8897897601127625, "content": {"text": "\\section{No Simple Graph is Perfect}\nTags: Simple Graphs, Graph Theory, Perfect Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph whose order is $2$ or greater.\nThen $G$ is not perfect.\n\\end{theorem}\n\n\\begin{proof}\nRecall that a perfect graph is one where each vertex is of different degree.\nWe note in passing that the simple graph consisting of one vertex trivially fulfils the condition for perfection.\n{{AimForCont}} $G$ is a simple graph of order $n$ where $n \\ge 2$ such that $G$ is perfect.\nFirst, suppose that $G$ has no isolated vertices.\nBy the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n$.\nThat means it must connect to at least $n$ other vertices.\nBut there are only $n - 1$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\nNow suppose $G$ has an isolated vertex.\nThere can be only one, otherwise there would be two vertices of degree zero, and so $G$ would not be perfect.\nAgain by the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n - 1$.\nBut of the remaining $n - 1$ vertices, one of them is of degree zero.\nSo it cannot be adjacent to any vertex.\nSo there are only $n - 2$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_panlu/gravitational_force1.json": {"gold": {"3830": 1}, "retrieved": {"3539": {"score": 0.825421154499054, "content": {"text": "\\begin{definition}[Definition:Free Fall]\nA body $B$ influenced by a gravitational field $M$ is in '''free fall''' {{iff}} the force on it caused by $M$ is the only force on $B$.\n\\end{definition}"}}, "10100": {"score": 0.8318219184875488, "content": {"text": "\\begin{definition}[Definition:Weight (Physics)/Warning]\nThere is a certain amount of confusion in the common mind between weight and mass.\nThe latter is usually determined by measuring its weight.\nBut while the mass of a body is (under normal circumstances) constant, its weight varies according to its position relative to the gravitational field it is in, and so is not a constant property of that body.\nHowever, under usual terrestrial conditions the gravitational field is more or less constant (any differences being detectable only by instruments).\nThis means that the weight and mass of a body are commonly considered \"the same\".\nThus a weighing machine, while indicating the mass of a body, does so by measuring its weight.\n\\end{definition}"}}, "23170": {"score": 0.8463640213012695, "content": {"text": "\\section{Unlike Electric Charges Attract}\nTags: Electric Charge\n\n\\begin{theorem}\nLet $a$ and $b$ be stationary particles, each carrying an electric charge of $q_a$ and $q_b$ respectively.\nLet $q_a$ and $q_b$ be of the opposite sign.\nThat is, let $q_a$ and $q_b$ be unlike charges.\nThen the forces exerted by $a$ on $b$, and by $b$ on $a$, are such as to cause $a$ and $b$ to attract each other.\n\\end{theorem}\n\n\\begin{proof}\nBy Coulomb's Law of Electrostatics:\n:$\\mathbf F_{a b} \\propto \\dfrac {q_a q_b {\\mathbf r_{a b} } } {r^3}$\nwhere:\n:$\\mathbf F_{a b}$ is the force exerted on $b$ by the electric charge on $a$\n:$\\mathbf r_{a b}$ is the displacement vector from $a$ to $b$\n:$r$ is the distance between $a$ and $b$.\n{{WLOG}}, let $q_a$ be positive and $q_b$ be negative.\nThen $q_a q_b$ is a positive number multiplied by a negative number. \nThus $q_a q_b$ is a negative number.\nHence $\\mathbf F_{a b}$ is in the opposite direction to the displacement vector from $a$ to $b$.\nThat is, the force exerted on $b$ by the electric charge on $a$ is in the direction towards $a$.\nThe same applies to the force exerted on $a$ by the electric charge on $b$.\nThat is, the force exerted on $b$ by the electric charge on $a$ is in the direction towards $b$.\nThe effect of these forces is to cause $a$ and $b$ to pull together, that is, to attract each other.\n\\end{proof}\n\n"}}, "22706": {"score": 0.8393028974533081, "content": {"text": "\\section{Total Force on Charged Particle from 2 Charged Particles}\nTags: Electric Charge\n\n\\begin{theorem}\nLet $p_1$, $p_2$ and $p_3$ be charged particles.\nLet $q_1$, $q_2$ and $q_3$ be the electric charges on $p_1$, $p_2$ and $p_3$ respectively.\nLet $\\mathbf F_{ij}$ denote the force exerted on $q_j$ by $q_i$.\nLet $\\mathbf F_i$ denote the force exerted on $q_i$ by the combined action of the other two charged particles.\nThen the force $\\mathbf F_1$ exerted on $q_1$ by the combined action of $q_2$ and $q_3$ is given by:\n{{begin-eqn}}\n{{eqn | l = \\mathbf F_1\n      | r = \\mathbf F_{21} + \\mathbf F_{31}\n      | c = \n}}\n{{eqn | r = \\dfrac {q_2 q_1} {4 \\pi \\varepsilon_0 r_{2 1}^3} \\mathbf r_{2 1} + \\dfrac {q_3 q_1} {4 \\pi \\varepsilon_0 r_{3 1}^3} \\mathbf r_{3 1}\n      | c = \n}}\n{{end-eqn}}\nwhere:\n:$\\mathbf F_{21} + \\mathbf F_{31}$ denotes the vector sum of $\\mathbf F_{21}$ and $\\mathbf F_{31}$\n:$\\mathbf r_{ij}$ denotes the displacement from $p_i$ to $p_j$\n:$r_{ij}$ denotes the distance between $p_i$ and $p_j$\n:$\\varepsilon_0$ denotes the vacuum permittivity.\n\\end{theorem}\n\n\\begin{proof}\n:500px\nBy definition, the force $\\mathbf F_{21}$ and $\\mathbf F_{31}$ are vector quantities.\nHence their resultant can be found by using the Parallelogram Law.\nThe result follows from Coulomb's Law of Electrostatics.\n{{qed}}\n\\end{proof}\n\n"}}, "22707": {"score": 0.8319579362869263, "content": {"text": "\\section{Total Force on Charged Particle from Multiple Charged Particles}\nTags: Electric Charge\n\n\\begin{theorem}\nLet $p_1, p_2, \\ldots, p_n$ be charged particles.\nLet $q_1, q_2, \\ldots, q_n$ be the electric charges on $p_1, p_2, \\ldots, p_n$ respectively.\nFor all $i$ in $\\set {1, 2, \\ldots, n}$ where $i \\ne j$, let $\\mathbf F_{i j}$ denote the force exerted on $q_j$ by $q_i$.\nFor all $i$ in $\\set {1, 2, \\ldots, n}$, let $\\mathbf F_i$ denote the force exerted on $q_i$ by the combined action of all the other charged particles.\nThen the force $\\mathbf F_i$ exerted on $q_i$ by the combined action of all the other charged particles is given by:\n{{begin-eqn}}\n{{eqn | l = \\mathbf F_i\n      | r = \\sum_{\\substack {1 \\mathop \\le j \\mathop \\le n \\\\ i \\mathop \\ne j} } \\mathbf F_{j i}\n      | c = \n}}\n{{eqn | r = \\dfrac 1 {4 \\pi \\varepsilon_0} \\sum_{\\substack {1 \\mathop \\le j \\mathop \\le n \\\\ i \\mathop \\ne j} } \\dfrac {q_i q_j} {r_{j i}^3} \\mathbf r_{j i}\n      | c = \n}}\n{{end-eqn}}\nwhere:\n:the summation denotes the vector sum of $\\mathbf F_{21}$ and $\\mathbf F_{31}$\n:$\\mathbf r_{ij}$ denotes the displacement from $p_i$ to $p_j$\n:$r_{ij}$ denotes the distance between $p_i$ and $p_j$\n:$\\varepsilon_0$ denotes the vacuum permittivity.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction.\nFor all $n \\in \\Z_{\\ge 2}$, let $\\map P n$ be the proposition:\n:$\\ds \\mathbf F_i = \\dfrac 1 {4 \\pi \\varepsilon_0} \\sum_{\\substack {1 \\mathop \\le j \\mathop \\le n \\\\ i \\mathop \\ne j} } \\dfrac {q_i q_j} {r_{j i}^3} \\mathbf r_{j i}$\n$\\map P 2$ is the case:\n$\\mathbf F_1 = \\dfrac 1 {4 \\pi \\varepsilon_0} \\dfrac {q_1 q_2} {r_{2 1}^3} \\mathbf r_{2 1}$\nwhich is Coulomb's Law of Electrostatics.\nThus $\\map P 0$ is seen to hold.\n\\end{proof}\n\n"}}, "3830": {"score": 0.8822574019432068, "content": {"text": "\\begin{definition}[Definition:Gravity/Gravitational Force]\nThe '''gravitational force''' on a body $B$ is the force which is exerted on $B$ as a result of the gravitational field whose influence it is under.\n\\end{definition}"}}, "10098": {"score": 0.8668794631958008, "content": {"text": "\\begin{definition}[Definition:Weight (Physics)]\nThe '''weight''' of a body is the magnitude of the force exerted on it by the influence of a gravitational field.\nThe context is that the gravitational field in question is usually that of the Earth.\n\\end{definition}"}}, "3484": {"score": 0.8880054950714111, "content": {"text": "\\begin{definition}[Definition:Force of Gravity]\nThe '''force of gravity''' or '''gravitational force''' is the force on a body as a result of Newton's Law of Universal Gravitation.\nWhen used in an unqualified sense, it is usual for this to mean the force on a body at the surface of the Earth.\nFrom Gravity at Earth's Surface, this is approximately $9.8 \\ \\mathrm N \\ \\mathrm{kg}^{-1}$\nThe force of gravity varies across the earth's surface, and therefore it makes little sense to use it as a standard.\nHowever, the {{WP|General_Conference_on_Weights_and_Measures|CGPM}} adopted a standard acceleration of gravity of $9.806 \\, 65  \\ \\mathrm N \\ \\mathrm{kg}^{-1}$ in 1901.\nCategory:Definitions/Gravity\n\\end{definition}"}}, "3829": {"score": 0.8750180602073669, "content": {"text": "\\begin{definition}[Definition:Gravity]\n'''Gravity''' is the tendency of bodies with mass to attract each other.\n\\end{definition}"}}, "23422": {"score": 0.877535879611969, "content": {"text": "\\section{Weight of Body at Earth's Surface}\nTags: Weight (Physics), Weight\n\n\\begin{theorem}\nLet $B$ be a body of mass $m$ situated at (or near) the surface of Earth.\nThen the weight of $B$ is given by:\n:$W = m g$\nwhere $g$ is the value of the acceleration due to gravity at the surface of Earth.\n\\end{theorem}\n\n\\begin{proof}\nThe weight of $B$ is the magnitude of the force exerted on it by the influence of the gravitational field it is in.\nBy Newton's Second Law of Motion, that force is given by:\n:$\\mathbf W = -m g \\mathbf k$\nwhere:\n:$g$ is the value of the acceleration due to gravity at the surface of Earth\n:$\\mathbf k$ is a unit vector directed vertically upwards.\nHence the magnitude of $\\mathbf W$ is given by:\n:$W = \\size {-m g \\mathbf k} = m g$\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_elainewan/math_calculus_2_7.json": {"gold": {"21758": 1, "11333": 1, "21762": 1, "10521": 1}, "retrieved": {"2273": {"score": 0.8537259697914124, "content": {"text": "\\begin{definition}[Definition:Derivative]\nInformally, a '''derivative''' is the rate of change of one variable with respect to another.\n\\end{definition}"}}, "2284": {"score": 0.8550147414207458, "content": {"text": "\\begin{definition}[Definition:Derivative/Real Function/Derivative at Point]\nLet $I$ be an open real interval.\nLet $f: I \\to \\R$ be a real function defined on $I$.\nLet $\\xi \\in I$ be a point in $I$.\nLet $f$ be differentiable at the point $\\xi$.\n\\end{definition}"}}, "2288": {"score": 0.8616178631782532, "content": {"text": "\\begin{definition}[Definition:Derivative/Real Function/With Respect To]\nLet $f$ be a real function which is differentiable on an open interval $I$.\nLet $f$ be defined as an equation: $y = \\map f x$.\nThen the '''derivative of $y$ with respect to $x$''' is defined as:\n:$\\ds y^\\prime = \\lim_{h \\mathop \\to 0} \\frac {\\map f {x + h} - \\map f x} h = D_x \\, \\map f x$\nThis is frequently abbreviated as '''derivative of $y$''' '''WRT''' or '''w.r.t.''' '''$x$''', and often pronounced something like '''wurt'''.\nWe introduce the quantity $\\delta y = \\map f {x + \\delta x} - \\map f x$.\nThis is often referred to as '''the small change in $y$ consequent on the small change in $x$'''.\nHence the motivation behind the popular and commonly-seen notation:\n:$\\ds \\dfrac {\\d y} {\\d x} := \\lim_{\\delta x \\mathop \\to 0} \\dfrac {\\map f {x + \\delta x} - \\map f x} {\\delta x} = \\lim_{\\delta x \\mathop \\to 0} \\dfrac {\\delta y} {\\delta x}$\nHence the notation $\\map {f^\\prime} x = \\dfrac {\\d y} {\\d x}$.\nThis notation is useful and powerful, and emphasizes the concept of a derivative as being the limit of a ratio of very small changes.\nHowever, it has the disadvantage that the variable $x$ is used ambiguously: both as the point at which the derivative is calculated and as the variable with respect to which the derivation is done.\nFor practical applications, however, this is not usually a problem.\n\\end{definition}"}}, "11318": {"score": 0.8609035611152649, "content": {"text": "\\section{Limit of (Cosine (X) - 1) over X at Zero}\nTags: Cosine Function, Limits of Real Functions, Limit of (Cosine (X) - 1) over X, Limits of Functions, Analysis, Differential Calculus, Examples of Limits of Real Functions, Limit of (Cosine (X) - 1) over X at Zero\n\n\\begin{theorem}\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\cos x - 1} x = 0$\n\\end{theorem}\n\n\\begin{proof}\nThis proof works directly from the definition of the cosine function:\n{{begin-eqn}}\n{{eqn | l=\\cos x\n      | r=\\sum_{n=0}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n      | c=By the definition of the cosine function\n}}\n{{eqn | r=(-1)^0 \\cdot \\frac{x^{2\\cdot0} }{(2\\cdot0)!}+\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n}}\n{{eqn | r=1 + \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n      | c=From the definition of $0!$ and the definition of $a^0$\n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l=\\lim_{x \\to 0} \\ \\frac{\\cos (x) - 1} x\n      | r=\\lim_{x \\to 0} \\ \\frac{1 + \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!} - 1} x\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\frac{\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!} } x\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\frac{\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n-1} } {\\left({2n-1}\\right)!} } 1\n      | c=by Power Series Differentiable on Interval of Convergence and L'H\u00f4pital's Rule\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n-1} }{\\left({2n-1}\\right)!}\n}}\n{{eqn | r=\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {0^{2n-1} }{\\left({2n-1}\\right)!}\n      | c=by Polynomial is Continuous\n}}\n{{eqn | r=0\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "17361": {"score": 0.8580655455589294, "content": {"text": "\\section{Behaviour of Function Near Limit}\nTags: Limits of Real Functions, Limits of Functions, Limits\n\n\\begin{theorem}\nLet $f$ be a real function.\nLet $f \\left({x}\\right) \\to l$ as $x \\to \\xi$.\nThen:\n:If $l > 0$, then $\\exists h > 0: \\forall x: \\xi - h < x < \\xi + h, x \\ne \\xi: \\map f x > 0$\n:If $l < 0$, then $\\exists h > 0: \\forall x: \\xi - h < x < \\xi + h, x \\ne \\xi: \\map f x < 0$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of limit of a function:\n:$\\forall \\epsilon > 0: \\exists \\delta > 0: 0 < \\size {x - \\xi} < \\delta \\implies \\size {\\map f x - l} < \\epsilon$\nLet $l > 0$.\nSince this is true for all $\\epsilon > 0$, it is also true for $\\epsilon = l$.\nSo let the value of $\\delta$, for the above to be true, labelled $h$.\nThen:\n:$0 < \\size {x - \\xi} < h \\implies \\size {\\map f x - l} < l$\nThat is:\n:$\\xi - h < x < \\xi + h, x \\ne \\xi \\implies 0 = l - l < \\map f x < l + l = 2 l$\nHence:\n:$\\forall x: \\xi - h < x < \\xi + h, x \\ne \\xi: 0 < \\map f x$\nNow let $l < 0$, and consider $\\epsilon = -l$.\nA similar thread of reasoning leads us to:\n:$\\xi - h < x < \\xi + h, x \\ne \\xi \\implies -2 l < \\map f x < 0$\nand hence the second result.\n{{qed}}\n\\end{proof}\n\n"}}, "2286": {"score": 0.8717171549797058, "content": {"text": "\\begin{definition}[Definition:Derivative/Real Function/Derivative at Point/Definition 2]\nLet $I$ be an open real interval.\nLet $f: I \\to \\R$ be a real function defined on $I$.\nLet $\\xi \\in I$ be a point in $I$.\nLet $f$ be differentiable at the point $\\xi$.\nThat is, suppose the limit $\\ds \\lim_{h \\mathop \\to 0} \\frac {\\map f {\\xi + h} - \\map f \\xi} h$ exists.\nThen this limit is called the '''derivative of $f$ at the point $\\xi$'''.\n\\end{definition}"}}, "15227": {"score": 0.8626498579978943, "content": {"text": "\\section{Derivative of Exponential at Zero}\nTags: Differential Calculus, Exponential Function, Derivative of Exponential at Zero, Derivatives involving Exponential Function\n\n\\begin{theorem}\nLet $\\exp x$ be the exponential of $x$ for real $x$.\nThen:\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\exp x - 1} x = 1$\n\\end{theorem}\n\n\\begin{proof}\nFor all $x \\in \\R$, we have the following:\n* $\\exp 0 - 1 = 0$ from Exponential of Zero and One\n* $D_x \\left({\\exp x - 1}\\right) = \\exp x$ from Sum Rule for Derivatives\n* $D_x x = 1$ from Differentiation of the Identity Function.\nHaving verified its prerequisites, Corollary 1 to L'H\u00f4pital's rule yields immediately:\n:$\\displaystyle \\lim_{x \\to 0} \\frac {\\exp x - 1} {x} = \\lim_{x \\to 0} \\frac {\\exp x} {1} = \\exp 0 = 1$\n{{qed}}\n\\end{proof}\n\n"}}, "11319": {"score": 0.8770548701286316, "content": {"text": "\\section{Limit of Absolute Value}\nTags: Limits, Examples of Limits of Real Functions, Limits of Functions, Absolute Value Function\n\n\\begin{theorem}\nLet $x, \\xi \\in \\R$ be real numbers.\nThen:\n:$\\size {x - \\xi} \\to 0$ as $x \\to \\xi$\nwhere $\\size {x - \\xi}$ denotes the Absolute Value.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\epsilon > 0$.\nLet $\\delta = \\epsilon$.\nFrom the definition of a limit of a function, we need to show that $\\size {\\map f x - 0} < \\epsilon$ provided that $0 < \\size {x - \\xi} < \\delta$, where $\\map f x = \\size {x - \\xi}$.\nThus, provided $0 < \\size {x - \\xi} < \\delta$, we have:\n{{begin-eqn}}\n{{eqn | l = \\size {x - \\xi} - 0\n      | r = \\size {x - \\xi}\n      | c = \n}}\n{{eqn | o = <\n      | r = \\delta\n      | c = \n}}\n{{eqn | r = \\epsilon\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "15118": {"score": 0.8682057857513428, "content": {"text": "\\section{Derivative of Logarithm at One}\nTags: Differential Calculus, Derivatives, Logarithms, Derivative of Logarithm at One\n\n\\begin{theorem}\nLet $\\ln x$ be the natural logarithm of $x$ for real $x$ where $x > 0$.\nThen:\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\map \\ln {1 + x} } x = 1$\n\\end{theorem}\n\n\\begin{proof}\nL'H\u00f4pital's rule gives:\n:$\\displaystyle \\lim_{x \\to c} \\frac {f \\left({x}\\right)} {g \\left({x}\\right)} = \\lim_{x \\to c} \\frac {f^{\\prime} \\left({x}\\right)} {g^{\\prime} \\left({x}\\right)}$\n(provided the appropriate conditions are fulfilled).\nHere we have:\n* $\\ln \\left({1 + 0}\\right) = 0$\n* $D_x \\left({\\ln \\left({1 + x}\\right)}\\right) = \\dfrac 1 {1 + x}$ from the Chain Rule\n* $D_x x = 1$ from Differentiation of the Identity Function.\nThus:\n:$\\displaystyle \\lim_{x \\to 0} \\frac {\\ln \\left({1 + x}\\right)} {x} = \\lim_{x \\to 0} \\frac {\\left({1 + x}\\right)^{-1}} {1} = \\frac 1 {1 + 0} = 1$\n{{qed}}\n\\end{proof}\n\n"}}, "2285": {"score": 0.8705860376358032, "content": {"text": "\\begin{definition}[Definition:Derivative/Real Function/Derivative at Point/Definition 1]\nLet $I$ be an open real interval.\nLet $f: I \\to \\R$ be a real function defined on $I$.\nLet $\\xi \\in I$ be a point in $I$.\nLet $f$ be differentiable at the point $\\xi$.\nThat is, suppose the limit $\\ds \\lim_{x \\mathop \\to \\xi} \\frac {\\map f x - \\map f \\xi} {x - \\xi}$ exists.\nThen this limit is called the '''derivative of $f$ at the point $\\xi$'''.\n\\end{definition}"}}}}, "TheoremQA_elainewan/math_algebra_4_3.json": {"gold": {"597": 1}, "retrieved": {"21142": {"score": 0.8884444832801819, "content": {"text": "\\section{Set of Affine Mappings on Real Line under Composition forms Group}\nTags: Affine Geometry\n\n\\begin{theorem}\nLet $S$ be the set of all real functions $f: \\R \\to \\R$ of the form:\n:$\\forall x \\in \\R: \\map f x = r x + s$\nwhere $r \\in \\R_{\\ne 0}$ and $s \\in \\R$\nLet $\\struct {S, \\circ}$ be the algebraic structure formed from $S$ and the composition operation $\\circ$.\nThen $\\struct {S, \\circ}$ is a group.\n\\end{theorem}\n\n\\begin{proof}\nWe note that $S$ is a subset of the set of all real functions on $\\R$.\nFrom Set of all Self-Maps under Composition forms Semigroup, we have that $\\circ$ is associative.\nConsider the real function $I: \\R \\to \\R$ defined as:\n:$\\forall x \\in \\R: \\map I x = 1 \\times x + 0$\nWe have that:\n:$I \\in S$\n:$I$ is the identity mapping.\nSo $S$ is not empty.\nThen we note:\nLet $f, g \\in S$ such that:\n:$\\map f x = r_1 x + s_1$\n:$\\map g x = r_2 x + s_2$\nFor all $x \\in \\R$, we have:\n{{begin-eqn}}\n{{eqn | l = \\map {g \\circ f} x\n      | r = \\map g {\\map f x}\n      | c = {{Defof|Composition of Mappings}}\n}}\n{{eqn | r = \\map g {r_1 x + s_1}\n      | c = Definition of $f$\n}}\n{{eqn | r = r_2 {r_1 x + s_1} + s_2\n      | c = Definition of $f$\n}}\n{{eqn | r = \\paren {r_2 r_1} x + \\paren {r_2 s_1 + s_2}\n      | c = \n}}\n{{eqn | r = r x + s\n      | c = where $r = r_2 r_2$ and $s = r_2 s_1 + s_2$\n}}\n{{end-eqn}}\nThis demonstrates that $\\struct {S, \\circ}$ is closed.\nLet $\\phi: \\R \\to \\R$ be the real function defined as:\n:$\\forall x \\in \\R: \\map \\phi x = p x + q$\nwhere $p \\in \\R_{\\ne 0}$ and $q \\in \\R$.\nThen:\n{{begin-eqn}}\n{{eqn | l = y\n      | r = p x + q\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = x\n      | r = \\dfrac {y - q} p\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = x\n      | r = \\dfrac 1 p y + \\paren {-\\dfrac q p}\n      | c = \n}}\n{{end-eqn}}\nThus we define $\\psi: \\R \\to \\R$ be the real function defined as:\n:$\\forall y \\in \\R: \\map \\psi y = \\dfrac 1 p y + \\paren {-\\dfrac q p}$\nwhere $\\dfrac 1 p \\in \\R_{\\ne 0}$ and $-\\dfrac q p \\in \\R$.\nIt is seen that $\\psi \\in S$.\nFor all $x \\in \\R$, we have:\n{{begin-eqn}}\n{{eqn | l = \\map {\\phi \\circ \\psi} x\n      | r = \\map \\phi {\\map \\psi x}\n      | c = {{Defof|Composition of Mappings}}\n}}\n{{eqn | r = \\map \\phi {\\dfrac 1 p x - \\dfrac q p}\n      | c = Definition of $\\psi$\n}}\n{{eqn | r = p {\\dfrac 1 p x -\\dfrac q p} + q\n      | c = Definition of $\\phi$\n}}\n{{eqn | r = \\dfrac p p x - \\dfrac {p q} p + q\n      | c = \n}}\n{{eqn | r = x\n      | c = \n}}\n{{end-eqn}}\nHence:\n:$\\phi \\circ \\psi = I$\nand so $\\psi$ is the inverse of $\\phi$.\nWe have then that $\\struct {S, \\circ}$ is closed under inversion.\nThe result follows by the Two-Step Subgroup Test.\n{{qed}}\n\\end{proof}\n\n"}}, "16105": {"score": 0.8897131085395813, "content": {"text": "\\section{Composite of Bijection with Inverse is Identity Mapping}\nTags: Bijections, Identity Mappings, Mappings, Inverse Mappings\n\n\\begin{theorem}\nLet $f: S \\to T$ be a bijection.\nThen:\n{{begin-eqn}}\n{{eqn | l = f^{-1} \\circ f\n      | r = I_S\n}}\n{{eqn | l = f \\circ f^{-1}\n      | r = I_T\n}}\n{{end-eqn}}\nwhere $I_S$ and $I_T$ are the identity mappings on $S$ and $T$ respectively.\n\\end{theorem}\n\n\\begin{proof}\nLet $f: S \\to T$ be a bijection.\nFrom Inverse of Bijection is Bijection, $f^{-1}$ is also a bijection.\nLet $x \\in S$.\nFrom Inverse Element of Bijection:\n:$\\exists y \\in T: y = \\map f x \\implies x = \\map {f^{-1} } y$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map {f^{-1} \\circ f} x\n      | r = \\map {f^{-1} } {\\map f x}\n      | c = {{Defof|Composition of Mappings}}\n}}\n{{eqn | r = \\map {f^{-1} } y\n      | c = {{hypothesis}}\n}}\n{{eqn | r = x\n      | c = {{hypothesis}}\n}}\n{{eqn | r = \\map {I_S} x\n      | c = {{Defof|Identity Mapping}}\n}}\n{{end-eqn}}\nFrom Domain of Composite Relation and Codomain of Composite Relation, the domain and codomain of $f^{-1} \\circ f$ are both $S$, and so are those of $I_S$ by definition.\nSo all the criteria for Equality of Mappings are met and thus $f^{-1} \\circ f = I_S$.\nLet $y \\in T$.\nFrom Inverse Element of Bijection:\n:$\\exists x \\in S: x = \\map {f^{-1} } y \\implies y = \\map f x$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map {f \\circ f^{-1} } y\n      | r = \\map f {\\map {f^{-1} } y}\n      | c = {{Defof|Composition of Mappings}}\n}}\n{{eqn | r = \\map f x\n      | c = {{hypothesis}}\n}}\n{{eqn | r = y\n      | c = {{hypothesis}}\n}}\n{{eqn | r = \\map {I_T} y\n      | c = {{Defof|Identity Mapping}}\n}}\n{{end-eqn}}\nFrom Domain of Composite Relation and Codomain of Composite Relation, the domain and codomain of $f \\circ f^{-1}$ are both $T$, and so are those of $I_T$ by definition.\nSo all the criteria for Equality of Mappings are met and thus $f \\circ f^{-1} = I_T$.\n{{Qed}}\n\\end{proof}\n\n"}}, "11834": {"score": 0.89163738489151, "content": {"text": "\\section{Inverse of Composite Bijection}\nTags: Mappings, Inverse of Composite Bijection, Composite Mappings, Bijections, Inverse Mappings\n\n\\begin{theorem}\nLet $f$ and $g$ be bijections such that $\\Dom g = \\Cdm f$.\nThen:\n:$\\paren {g \\circ f}^{-1} = f^{-1} \\circ g^{-1}$\nand $f^{-1} \\circ g^{-1}$ is itself a bijection.\n\\end{theorem}\n\n\\begin{proof}\n$\\left({g \\circ f}\\right)^{-1} = f^{-1} \\circ g^{-1}$ is a specific example of Inverse of Composite Relation.\nAs $f$ and $g$ are bijections then by Bijection iff Inverse is Bijection, so are both $f^{-1}$ and $g^{-1}$.\nBy Composite of Bijections, it follows that $f^{-1} \\circ g^{-1}$ is a bijection.\n{{qed}}\n\\end{proof}\n\n"}}, "23127": {"score": 0.8914749026298523, "content": {"text": "\\section{Unit of Ring of Mappings iff Image is Subset of Ring Units/Image is Subset of Ring Units implies Unit of Ring of Mappings}\nTags: Unit of Ring of Mappings iff Image is Subset of Ring Units, Mapping Theory, Ring Theory, Abstract Algebra\n\n\\begin{theorem}\nLet $\\struct {R, +, \\circ}$ be a ring with unity $1_R$.\nLet $U_R$ be the set of units in $R$.\nLet $S$ be a set.\nLet $\\struct {R^S, +', \\circ'}$ be the ring of mappings on the set of mappings $R^S$.\nLet $\\Img f \\subseteq U_R$ where $\\Img f$ is the image of $f$.\nThen:\n:$f \\in R^S$ is a unit of $R^S$\nand the inverse of $f$ is the mapping $f^{-1} : S \\to U_R$ defined by:\n:$\\forall x \\in S : \\map {f^{-1}} {x} = \\map f x^{-1}$\n\\end{theorem}\n\n\\begin{proof}\nFrom Structure Induced by Ring with Unity Operations is Ring with Unity, $\\struct {R^S, +', \\circ'}$ has a unity $f_{1_R}$ defined by:\n:$\\forall x \\in S: \\map {f_{1_R}} x = 1_R$\nBy assumption: \n:$\\forall x \\in S: \\exists \\map f x^{-1} : \\map f x \\circ \\map f x^{-1} = \\map f x^{-1} \\circ \\map f x  = 1_R$\nLet $f^{-1} : S \\to U_R$ be defined by:\n:$\\forall x \\in S : \\map {f^{-1}} {x} = \\map f x^{-1}$\nConsider the mapping $f \\circ\u2019 f^{-1}$.\nFor all $x \\in S$: \n{{begin-eqn}}\n{{eqn | l = \\map {\\paren {f \\circ\u2019 f^{-1} } } x\n      | r = \\map f x \\circ\u2019 \\map {f^{-1} } x\n}}\n{{eqn | r = \\map f x \\circ \\map f x^{-1}\n}}\n{{eqn | r = 1_R\n}}\n{{end-eqn}}\nHence $f \\circ\u2019 f^{-1} = f_{1_R}$.\nSimilarly, $f^{-1} \\circ\u2019 f = f_{1_R}$.\nHence $f$ is a unit of $\\struct {R^S, +', \\circ'}$ and the inverse of $f$ is the mapping $f^{-1}$.\n{{qed}}\nCategory:Unit of Ring of Mappings iff Image is Subset of Ring Units\n\\end{proof}\n\n"}}, "11751": {"score": 0.8909139633178711, "content": {"text": "\\section{Inverse of Permutation is Permutation}\nTags: Mappings, Permutation Theory, Permutations\n\n\\begin{theorem}\nIf $f$ is a permutation of $S$, then so is its inverse $f^{-1}$.\n\\end{theorem}\n\n\\begin{proof}\nLet $f: S \\to S$ is a permutation of $S$.\nBy definition, a permutation is a bijection such that the domain and codomain are the same set.\nFrom Bijection iff Inverse is Bijection, it follows $f^{-1}$ is a bijection.\nFrom the definition of inverse relation, the domain of a relation is the codomain of its inverse and vice versa.\nThus the domain and codomain of $f^{-1}$ are both $S$ and it follows that $f^{-1}$ is a permutation.\n{{Qed}}\n\\end{proof}\n\n"}}, "11761": {"score": 0.8962521553039551, "content": {"text": "\\section{Inverse of Similarity Mapping}\nTags: Similarity Mappings, Linear Algebra\n\n\\begin{theorem}\nLet $G$ be a vector space over a field $K$.\nLet $\\beta \\in K$ such that $\\beta \\ne 0$.\nLet $s_\\beta: G \\to G$ be the similarity on $G$ defined as:\n:$\\forall \\mathbf x \\in G: \\map {s_\\beta} {\\mathbf x} = \\beta \\mathbf x$\nLet $\\paren {s_\\beta}^{-1}$ denote the inverse of $s_\\beta$.\nThen:\n:$\\paren {s_\\beta}^{-1} = s_{\\beta^{-1} }$\nwhere $\\beta^{-1}$ is the multiplicative inverse in $K$ of $\\beta$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Similarity Mapping is Automorphism, $s_\\beta$ is an automorphism of $G$.\nHence $s_\\beta$ is an vector space isomorphism from $G$ to $G$ itself.\nSo by definition $s_\\beta$ is a bijection.\nHence the existence of this inverse $\\paren {s_\\beta}^{-1}$ follows from Bijection iff Left and Right Inverse.\nBy {{Field-axiom|M4}}, we have that there exists a multiplicative inverse $\\beta^{-1}$ for $\\beta$ such that:\n:$\\beta \\beta^{-1} = 1_K = \\beta^{-1} \\beta$\nwhere $1_K$ is the multiplicative identity of $K$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map {\\paren {s_\\beta}^{-1} } {\\map {s_\\beta} {\\beta^{-1} \\, \\mathbf x} }\n      | r = \\beta^{-1} \\mathbf x\n      | c = {{Defof|Inverse Mapping}}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map {\\paren {s_\\beta}^{-1} } {\\beta \\beta^{-1} \\, \\mathbf x}\n      | r = \\beta^{-1} \\, \\mathbf x\n      | c = Definition of $s_\\beta$\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map {\\paren {s_\\beta}^{-1} } {1_K \\, \\mathbf x}\n      | r = \\beta^{-1} \\, \\mathbf x\n      | c = {{Field-axiom|M4}}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map {\\paren {s_\\beta}^{-1} } {\\mathbf x}\n      | r = \\beta^{-1} \\, \\mathbf x\n      | c = {{Field-axiom|M3}}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map {\\paren {s_\\beta}^{-1} } {\\mathbf x}\n      | r = \\map {s_{\\beta^{-1} } } {\\mathbf x}\n      | c = Definition of $s_{\\beta^{-1} }$\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "16041": {"score": 0.8935442566871643, "content": {"text": "\\section{Composition of Repeated Compositions of Injections}\nTags: Injections, Composite Mappings\n\n\\begin{theorem}\nLet $S$ be a set.\nLet $f: S \\to S$ be an injection.\nLet the sequence of mappings:\n:$f^0, f^1, f^2, \\ldots, f^n, \\ldots$\nbe defined as:\n:$\\forall n \\in \\N: f^n \\left({x}\\right) = \\begin{cases}\nx & : n = 0 \\\\\nf \\left({x}\\right) & : n = 1 \\\\\nf \\left({f^{n-1} \\left({x}\\right)}\\right) & : n > 1\n\\end{cases}$\nThen:\n:$\\forall m, n \\in \\Z_{\\ge 0}: f^n \\circ f^m = f^{m + n}$\nwhere $f^n \\circ f^m$ denotes composition of mappings.\n\\end{theorem}\n\n\\begin{proof}\nProof by induction:\nLet $m \\in \\Z_{\\ge 0}$ be given.\nFor all $n \\in \\Z_{\\ge 0}$, let $P \\left({n}\\right)$ be the proposition:\n:$f^n \\circ f^m = f^{m + n}$\n$P \\left({0}\\right)$ is true, as this is the case:\n{{begin-eqn}}\n{{eqn | l = f^0 \\circ f^m\n      | r = I_S \\circ f^m\n      | c = where $I_S$ is the identity mapping\n}}\n{{eqn | r = f^m\n      | c = Identity Mapping is Left Identity\n}}\n{{end-eqn}}\n\\end{proof}\n\n"}}, "11835": {"score": 0.8979021310806274, "content": {"text": "\\section{Inverse of Composite Bijection/Proof 2}\nTags: Bijections, Inverse of Composite Bijection, Composite Mappings\n\n\\begin{theorem}\nLet $f$ and $g$ be bijections.\nThen:\n:$\\paren {g \\circ f}^{-1} = f^{-1} \\circ g^{-1}$\nand $f^{-1} \\circ g^{-1}$ is itself a bijection.\n\\end{theorem}\n\n\\begin{proof}\nLet $f: X \\to Y$ and $g: Y \\to Z$ be bijections.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\paren {g \\circ f} \\circ \\paren {f^{-1} \\circ g^{-1} }\n      | r = g \\circ \\paren {\\paren {f \\circ f^{-1} } \\circ g^{-1} }\n      | c = Composition of Mappings is Associative\n}}\n{{eqn | r = g \\circ \\paren {I_Y \\circ g^{-1} }\n      | c = Composite of Bijection with Inverse is Identity Mapping\n}}\n{{eqn | r = g \\circ g^{-1}\n      | c = Identity Mapping is Left Identity\n}}\n{{eqn | r = I_Z\n      | c = Composite of Bijection with Inverse is Identity Mapping\n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l = \\paren {f^{-1} \\circ g^{-1} } \\circ \\paren {g \\circ f}\n      | r = \\paren {f^{-1} \\circ \\paren {g^{-1} \\circ g} } \\circ f\n      | c = Composition of Mappings is Associative\n}}\n{{eqn | r = \\paren {f^{-1} \\circ I_Y} \\circ f\n      | c = Composite of Bijection with Inverse is Identity Mapping\n}}\n{{eqn | r = f^{-1} \\circ f\n      | c = Identity Mapping is Right Identity\n}}\n{{eqn | r = I_X\n      | c = Composite of Bijection with Inverse is Identity Mapping\n}}\n{{end-eqn}}\nHence the result.\n\\end{proof}\n\n"}}, "16051": {"score": 0.8936381340026855, "content": {"text": "\\section{Composition of Three Mappings which form Identity Mapping}\nTags: Bijections, Composite Mappings\n\n\\begin{theorem}\nLet $A$, $B$ and $C$ be non-empty sets.\nLet $f: A \\to B$, $g: B \\to C$ and $h: C \\to A$ be mappings.\nLet the following hold:\n{{begin-eqn}}\n{{eqn | l = h \\circ g \\circ f\n      | r = I_A\n}}\n{{eqn | l = f \\circ h \\circ g\n      | r = I_B\n}}\n{{eqn | l = g \\circ f \\circ h\n      | r = I_C\n}}\n{{end-eqn}}\nwhere:\n:$g \\circ f$ (and so on) denote composition of mappings\n:$I_A$ (and so on) denote the identity mappings.\nThen each of $f$, $g$ and $h$ are bijections, and:\n{{begin-eqn}}\n{{eqn | l = f^{-1}\n      | r = h \\circ g\n}}\n{{eqn | l = g^{-1}\n      | r = f \\circ h\n}}\n{{eqn | l = h^{-1}\n      | r = g \\circ f\n}}\n{{end-eqn}}\nwhere $f^{-1}$ (and so on) denote the inverse mappings.\n\\end{theorem}\n\n\\begin{proof}\nFirst note that from Composition of Mappings is Associative:\n:$\\paren {h \\circ g} \\circ f = h \\circ \\paren {g \\circ f}$\nand so on.\nHowever, while there is no need to use parenthesis to establish the order of composition of mappings, in the following the technique will be used in order to clarify what is being done.\nWe have that:\n:$\\paren {h \\circ g} \\circ f = I_A$\nIt follows from Injection iff Left Inverse that $f$ is an injection.\nWe also have that:\n:$f \\circ \\paren {h \\circ g} = I_B$\nIt follows from Surjection iff Right Inverse that $f$ is a surjection.\nSo $f$ is a bijection.\nIt follows from the corollary to Bijection iff Left and Right Inverse that $h \\circ g$ is also a bijection.\nThus we have that $h \\circ g$ is both a left inverse and a right inverse of $f$.\nIt follows by definition that $h \\circ g$ is the inverse of $f$:\n:$f^{-1} = h \\circ g$\nThe same argument, mutatis mutandis, can be used to show that:\n:$g$ and $h$ are bijections\n:the inverse of $g$ is $f \\circ h$\n:the inverse of $h$ is $g \\circ f$.\n{{qed}}\n\\end{proof}\n\n"}}, "13936": {"score": 0.8946696519851685, "content": {"text": "\\section{Existence of One-Sided Inverses on Natural Numbers whose Composition is Identity Mapping}\nTags: Natural Numbers, Inverse Mappings\n\n\\begin{theorem}\nConsider the set of natural numbers $\\N$.\nThere exist mappings $f: \\N \\to \\N$ and $g: \\N \\to \\N$ such that:\n:$g \\circ f = I_\\N$\nwhere:\n:$\\circ$ denotes composition of mappings\n:$I_\\N$ denotes the identity mapping on $\\N$\nsuch that neither $f$ nor $g$ are permutations on $\\N$.\n\\end{theorem}\n\n\\begin{proof}\nLet $f: \\N \\to \\N$ be the mapping defined as:\n:$\\forall x \\in \\N: \\map f x = x + 1$\nLet $g: \\N \\to \\N$ be the mapping defined as:\n:$\\forall x \\in \\N: \\map g x = \\begin {cases} x - 1 & : x > 0 \\\\ 0 & : x = 0 \\end {cases}$\nIt is apparent by inspection that:\n:$f$ is injective\nbut\n:$f$ is not surjective, as there exists no $x \\in \\N$ such that $x + 1 = 0$\nIt is also apparent by inspection that:\n:$g$ is surjective\nbut\n:$g$ is not injective, as $\\map g 0 = \\map g 1 = 0$\nThus neither $f$ nor $g$ are bijective, and hence not permutations on $\\N$.\nHowever, it is also apparent by inspection that:\n:$g \\circ f = I_\\N$\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Ramsey_4.json": {"gold": {"12234": 1, "20117": 1, "7627": 1}, "retrieved": {"21466": {"score": 0.8503908514976501, "content": {"text": "\\section{Size of Complete Graph}\nTags: Proofs by Induction, Complete Graphs\n\n\\begin{theorem}\nLet $K_n$ denote the complete graph of order $n$ where $n \\ge 0$.\nThe size of $K_n$ is given by:\n:$\\size {K_n} = \\dfrac {n \\paren {n - 1} } 2$\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction.\nFor all $n \\in \\Z_{\\ge 0}$, let $\\map P n$ be the proposition:\n:$\\size {K_n} = \\dfrac {n \\paren {n - 1} } 2$\nFirst we explore the degenerate case $\\map P 0$:\n{{begin-eqn}}\n{{eqn | l = \\size {K_0}\n      | r = 0\n      | c = as $K_0$ is the null graph\n}}\n{{eqn | r = \\dfrac {0 \\paren {0 - 1} } 2\n      | c = \n}}\n{{end-eqn}}\nThus $\\map P 0$ is seen to hold.\n\\end{proof}\n\n"}}, "21376": {"score": 0.8594642877578735, "content": {"text": "\\section{Simple Graph whose Vertices all Incident but Edges not Adjacent}\nTags: Simple Graphs\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph such that:\n:every vertex is incident with at least one edge\n:no two edges are adjacent to each other.\nThen $G$ has an even number of vertices.\n\\end{theorem}\n\n\\begin{proof}\nSuppose there exists a set of $3$ vertices that are connected.\nThen at least one of these vertices has at least $2$ edges.\nThat would mean that at least $2$ edges were incident with the same vertex.\nThat is, that at least $2$ edges were adjacent to each other.\nSo, for a simple graph to fulfil the conditions, vertices can exist only in $2$-vertex components.\nSo such a simple graph must have an even number of vertices.\n{{qed}}\n\\end{proof}\n\n"}}, "7627": {"score": 0.8745722770690918, "content": {"text": "\\begin{definition}[Definition:Ramsey Number]\nRamsey's Theorem states that in any coloring of the edges of a sufficiently large complete graph, one will find monochromatic complete subgraphs.\nMore precisely, for any given number of colors $c$, and any given integers $n_1, \\ldots, n_c$, there is a number $\\map R {n_1, \\ldots, n_c}$ such that:\n:if the edges of a complete graph of order $\\map R {n_1, \\ldots, n_c}$ are colored with $c$ different colours, then for some $i$ between $1$ and $c$, it must contain a complete subgraph of order $n_i$ whose edges are all color $i$.\nThis number $\\map R {n_1, \\ldots, n_c}$ is called the '''Ramsey number''' for $n_1, \\ldots, n_c$.\n{{NamedforDef|Frank Plumpton Ramsey|cat = Ramsey}}\nCategory:Definitions/Ramsey Theory\n\\end{definition}"}}, "21375": {"score": 0.8649275898933411, "content": {"text": "\\section{Simple Graph whose Vertices Incident to All Edges}\nTags: Graph Theory\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph whose vertices are incident to all its edges.\nThen $G$ is either:\n:the star graph $S_2$, which is also the complete graph $K_2$\n:an edgeless graph of any order.\n\\end{theorem}\n\n\\begin{proof}\nIf $G$ has no edges, then all the vertices are incident to all the edges vacuously.\nSo any of the edgeless graphs $N_n$ for order $n \\in \\Z_{\\ge 0}$ fulfils the criterion.\nSuppose $G$ has more than $2$ vertices $v_1, v_2, v_3$ and at least one edge.\n{{WLOG}}, let one edge be $v_1 v_2$.\nBut $v_3$ cannot be incident to edge $v_1 v_2$.\nSo $G$ can have no more than $2$ vertices.\nFurthermore, there can be only one edge joining those two vertices.\nThe result follows from inspection of $K_2$ and $S_2$.\n{{qed}}\n\\end{proof}\n\n"}}, "5791": {"score": 0.8610689640045166, "content": {"text": "\\begin{definition}[Definition:Monochromatic Graph]\nA '''monochromatic graph''' is a colored graph (either vertex-colored or edge-colored, depending on the context) in which each of the vertices or edges is assigned the same color.\nCategory:Definitions/Graph Theory\nCategory:Definitions/Ramsey Theory\n\\end{definition}"}}, "20117": {"score": 0.8821319937705994, "content": {"text": "\\section{Ramsey's Theorem}\nTags: Ramsey Theory, Named Theorems, Combinatorics\n\n\\begin{theorem}\nIn any coloring of the edges of a sufficiently large complete graph, one will find monochromatic complete subgraphs.\nFor 2 colors, Ramsey's theorem states that for any pair of positive integers $\\tuple {r, s}$, there exists a least positive integer $\\map R {r, s}$ such that for any complete graph on $\\map R {r, s}$ vertices, whose edges are colored red or blue, there exists either a complete subgraph on $r$ vertices which is entirely red, or a complete subgraph on $s$ vertices which is entirely blue.\nMore generally, for any given number of colors $c$, and any given integers $n_1, \\ldots, n_c$, there is a number $\\map R {n_1, \\ldots, n_c}$ such that:\n:if the edges of a complete graph of order $\\map R {n_1, \\ldots, n_c}$ are colored with $c$ different colours, then for some $i$ between $1$ and $c$, it must contain a complete subgraph of order $n_i$ whose edges are all color $i$.\nThis number $\\map R {n_1, \\ldots, n_c}$ is called the Ramsey number for $n_1, \\ldots, n_c$.\nThe special case above has $c = 2$ (and $n_1 = r$ and $n_2 = s$).\nHere $\\map R {r, s}$ signifies an integer that depends on both $r$ and $s$. It is understood to represent the smallest integer for which the theorem holds.\n\\end{theorem}\n\n\\begin{proof}\nFirst we prove the theorem for the 2-color case, by induction on $r + s$.\nIt is clear from the definition that\n:$\\forall n \\in \\N: \\map R {n, 1} = \\map R {1, n} = 1$\nbecause the complete graph on one node has no edges.\nThis is the base case.\nWe prove that $R \\left({r, s}\\right)$ exists by finding an explicit bound for it.\nBy the inductive hypothesis, $\\map R {r - 1, s}$ and $\\map R {r, s - 1}$ exist.\n\\end{proof}\n\n"}}, "20864": {"score": 0.875238835811615, "content": {"text": "\\section{Schur's Theorem (Ramsey Theory)}\nTags: Ramsey Theory, Named Theorems, Combinatorics\n\n\\begin{theorem}\nLet $r$ be a positive integer.\nThen there exists a positive integer $S$ such that:\n:for every partition of the integers $\\set {1, \\ldots, S}$ into $r$ parts, one of the parts contains integers $x$, $y$ and $z$ such that:\n::$x + y = z$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$n = \\map R {3, \\ldots, 3}$\nwhere $\\map R {3, \\ldots, 3}$ denotes the Ramsey number on $r$ colors.\nTake $S$ to be $n$.\n{{refactor|Extract the below process of \"coloring\" a partition into its own page}}\npartition the integers $\\set {1, \\ldots, n}$ into $r$ parts, which we denote by '''colors'''.\nThat is:\n:the integers in the first part are said to be '''colored''' $c_1$\n:the integers in the second part are said to be colored $c_2$ \nand so on till color $c_r$.\nThus $\\set {1, \\ldots, S}$ has been '''$r$-colored'''.\n(This terminology is common in Ramsey theory.)\nNow consider the complete graph $K_n$.\nNow color the edges of $K_n$ as follows:\n:An edge $xy$ is given color $c$ if $\\size {x - y}$ was colored $c$ in the partitioning.\n{{explain|When the page defining a \"coloring\" of a partition is written, make sure that the links are assigned appropriately from the two difference senses of \"coloring\" in the above.}}\nFrom the definition of $\\map R {3, \\ldots, 3}$ and Ramsey's Theorem, $K_n$ will definitely contain a monochromatic triangle, say built out of the vertices $i > j > k$.\nLet the triangle be colored $c_m$.\nNow $i - j$, $i - k$ and $j - k$ will also be colored $c_m$.\nThat is, $i - j$, $i - k$ and $j - k$ will belong to the same part in the partition.\nIt only remains to take $x = i - j$, $y = j - k$ and $z = i - k$ to complete the proof.\n{{qed}}\n\\end{proof}\n\n"}}, "21374": {"score": 0.8936582803726196, "content": {"text": "\\section{Simple Graph where All Vertices and All Edges are Adjacent}\nTags: Simple Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph in which:\n:every vertex is adjacent to every other vertex\nand:\n:every edge is adjacent to every other edge.\nThen $G$ is of order no greater than $3$.\n\\end{theorem}\n\n\\begin{proof}\nIt is seen that examples exist of simple graphs which fulfil the criteria where the order of $G$ is no greater than $3$:\n:400px\nThe cases where the order of $G$ is $1$ or $2$ are trivial.\nWhen the order of $G$ is $3$, the criteria can be verified by inspection.\nLet the order of $G = \\struct {V, E}$ be $4$ or more.\nLet $v_1, v_2, v_3, v_4 \\in V$.\nSuppose every vertex is adjacent to every other vertex.\nAs $v_1$ is adjacent to $v_2$, there exists the edge $v_1 v_2$.\nAs $v_3$ is adjacent to $v_4$, there exists the edge $v_3 v_4$.\nBut $v_1 v_2$ and $v_3 v_4$ both join two distinct pairs of vertices.\nThus $v_1 v_2$ and $v_3 v_4$ are not adjacent, by definition.\nSo when there are $4$ or more vertices in $G$, it cannot fulfil the criteria.\n{{qed}}\n\\end{proof}\n\n"}}, "23824": {"score": 0.875508189201355, "content": {"text": "\\section{No Simple Graph is Perfect}\nTags: Simple Graphs, Graph Theory, Perfect Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph whose order is $2$ or greater.\nThen $G$ is not perfect.\n\\end{theorem}\n\n\\begin{proof}\nRecall that a perfect graph is one where each vertex is of different degree.\nWe note in passing that the simple graph consisting of one vertex trivially fulfils the condition for perfection.\n{{AimForCont}} $G$ is a simple graph of order $n$ where $n \\ge 2$ such that $G$ is perfect.\nFirst, suppose that $G$ has no isolated vertices.\nBy the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n$.\nThat means it must connect to at least $n$ other vertices.\nBut there are only $n - 1$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\nNow suppose $G$ has an isolated vertex.\nThere can be only one, otherwise there would be two vertices of degree zero, and so $G$ would not be perfect.\nAgain by the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n - 1$.\nBut of the remaining $n - 1$ vertices, one of them is of degree zero.\nSo it cannot be adjacent to any vertex.\nSo there are only $n - 2$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\n{{qed}}\n\\end{proof}\n\n"}}, "11719": {"score": 0.8813105225563049, "content": {"text": "\\section{Isomorphism Classes for Order 4 Size 3 Simple Graphs}\nTags: Graph Isomorphisms\n\n\\begin{theorem}\nThere are $3$ equivalence classes for simple graphs of order $4$ and size $3$ under isomorphism:\n:400px\n\\end{theorem}\n\n\\begin{proof}\n{{tidy}}\n{{MissingLinks}}\nThe fact that the $3$ graphs given are not isomorphic follows from Vertex Condition for Isomorphic Graphs.\nThe vertices have degrees as follows:\n:Graph $1$: $2, 2, 1, 1$\n:Graph $2$: $3, 1, 1, 1$\n:Graph $3$: $2, 2, 2, 0$\nThe fact that there are no more isomorphism classes of such graphs can be proved constructively.\nLet the $4$ vertices be named $A, B, C$ and $D$.\nLemma: There must be intersections among the edges.\nProof: If there were no intersection at all, it requires at least $3 \\times 2 = 6$ vertices.\n{{WLOG}}, let $2$ of the edges be $AB$ and $AC$.\nTo place the last edge, there are $\\dbinom 4 2 = 6$ potential choices:\n:$AB$: this makes the graph not simple\n:$AC$: this makes the graph not simple\n:$AD$: this is isomorphic to graph $2$\n:$BC$: this is isomorphic to graph $3$\n:$BD$: this is isomorphic to graph $1$\n:$CD$: this is isomorphic to graph $1$.\nHence, by Proof by Cases, these $3$ are the only isomorphism classes.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/pigeonhole_1.json": {"gold": {"18695": 1}, "retrieved": {"17920": {"score": 0.8340919613838196, "content": {"text": "\\section{Number of Friday 13ths in a Year}\nTags: Calendars\n\n\\begin{theorem}\nIn any given year, there are between $1$ and $3$ (inclusive) months in which the $13$th falls on a Friday.\n\\end{theorem}\n\n\\begin{proof}\nThe day of the week on which the $13$th falls is directy dependent upon the day of the week that the $1$st of the month falls.\nFrom Months that Start on the Same Day of the Week, the months can be grouped into equivalence classes according to which day the month starts:\nFor a non-leap year, the set of equivalence classes is:\n:$\\set {\\set {\\text {January}, \\text {October} }, \\set {\\text {February}, \\text {March}, \\text {November} }, \\set {\\text {April}, \\text {July} }, \\set {\\text {May} }, \\set {\\text {June} }, \\set {\\text {August} }, \\set {\\text {September}, \\text {December} } }$\nFor a leap year, the set of equivalence classes is:\n:$\\set {\\set {\\text {January}, \\text {April}, \\text {July} }, \\set {\\text {February}, \\text {August} }, \\set {\\text {March}, \\text {November} }, \\set {\\text {May} }, \\set {\\text {June} }, \\set {\\text {September}, \\text {December} }, \\set {\\text {October} } }$\nIt can be seen that none of these equivalence classes has fewer than $1$ day, and none has more than $3$.\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "13864": {"score": 0.8363110423088074, "content": {"text": "\\section{Exists Divisor in Set of n+1 Natural Numbers no greater than 2n}\nTags: Divisibility\n\n\\begin{theorem}\nLet $S$ be a set of $n + 1$ non-non-zero natural numbers all less than or equal to $2 n$.\nThen there exists $a, b \\in S$ such that\n:$a \\divides b$\nwhere $\\divides$ denotes divisibility.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\alpha$ denote the relation defined on the natural numbers $\\N$ by:\n:$\\forall x, y \\in \\N: x \\mathrel \\alpha y \\iff \\exists n \\in \\Z: x = 2^n y$\nFrom Equivalence Relation on Natural Numbers such that Quotient is Power of Two, $\\alpha$ is an equivalence relation.\nFrom Equivalence Class under $\\alpha$ Contains $1$ Odd Number, each odd integer between $1$ and $2 n$ is in its own equivalence class.\nEach even integer $m$ between $1$ and $2 n$ is equal to an odd integer $p$ strictly less than $m$ multiplied by a power of $2$.\nHence each such $m$ is an element of one of the equivalence classes of one of the $n$ odd integers between $1$ and $2 n$.\nThus each element of $S$ is an element of one of exactly $n$ equivalence classes of $\\alpha$.\nBut there are $n + 1$ elements of $S$.\nSo by the Pigeonhole Principle, at least $2$ elements of $S$ are in the same equivalence class.\nThus let $a, b \\in S: a \\mathrel \\alpha b$.\nFrom One of Pair of Equivalent Elements is Divisor of the Other, either $a \\divides b$ or $b \\divides a$.\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "2565": {"score": 0.8494032621383667, "content": {"text": "\\begin{definition}[Definition:Distinct/Plural/Pairwise Distinct]\nA set $S$ of objects is '''pairwise distinct''' {{iff}}:\n:for each pair $\\set {x, y} \\subseteq S$ of elements of $S$, $x$ and $y$ are distinct.\nCategory:Definitions/Set Theory\nCategory:Definitions/Distinct\n\\end{definition}"}}, "4003": {"score": 0.8445658683776855, "content": {"text": "\\begin{definition}[Definition:Hexad]\nA '''hexad''' is a set or tuple which has exactly $6$ elements.\n\\end{definition}"}}, "2663": {"score": 0.8393025398254395, "content": {"text": "\\begin{definition}[Definition:Doubleton]\nA '''doubleton''' is a set that contains exactly two elements.\nThe '''doubleton''' containing the distinct elements $a$ and $b$ can be written $\\set {a, b}$.\nThe set $\\set {a, b}$ is known as '''the doubleton of $a$ and $b$'''.\n\\end{definition}"}}, "19695": {"score": 0.8885658979415894, "content": {"text": "\\section{Probability of no 2 People out of 53 Sharing the Same Birthday}\nTags: 53, Probability Theory\n\n\\begin{theorem}\nLet there be $53$ people in a room.\nThe probability that no $2$ of them have the same birthday is approximately $\\dfrac 1 {53}$.\n\\end{theorem}\n\n\\begin{proof}\n{{Refactor|The analyis has already been performed in Birthday Paradox. Extract that general case and make it a theorem, and then introduce Birthday Paradox and this page as examples.}}\nLet there be $n$ people in the room.\nLet $\\map p n$ be the probability that no two people in the room have the same birthday.\nFor simplicity, let us ignore leap years and assume there are $365$ days in the year.\nLet the birthday of person $1$ be established.\nThe probability that person $2$ shares person $1$'s birthday is $\\dfrac 1 {365}$.\nThus, the probability that person $2$ does not share person $1$'s birthday is $\\dfrac {364} {365}$.\nSimilarly, the probability that person $3$ does not share the birthday of either person $1$ or person $2$ is $\\dfrac {363} {365}$.\nAnd further, the probability that person $n$ does not share the birthday of any of the people indexed $1$ to $n - 1$ is $\\dfrac {365 - \\paren {n - 1} } {365}$.\nHence the total probability that none of the $n$ people share a birthday is given by:\n:$\\map p n = \\dfrac {364} {365} \\dfrac {363} {365} \\dfrac {362} {365} \\cdots \\dfrac {365 - n + 1} {365}$\n{{begin-eqn}}\n{{eqn | l = \\map p n\n      | r = \\dfrac {364} {365} \\dfrac {363} {365} \\dfrac {362} {365} \\cdots \\dfrac {365 - n + 1} {365}\n      | c = \n}}\n{{eqn | r = \\dfrac {365!} {365^n} \\binom {365} n\n      | c = \n}}\n{{end-eqn}}\nSetting $n = 53$ and evaluating the above gives:\n:$\\map p {53} \\approx 0.01887$\nor:\n:$\\map p {53} \\approx \\dfrac 1 {53.01697}$\n{{qed}}\n\\end{proof}\n\n"}}, "20864": {"score": 0.8516885042190552, "content": {"text": "\\section{Schur's Theorem (Ramsey Theory)}\nTags: Ramsey Theory, Named Theorems, Combinatorics\n\n\\begin{theorem}\nLet $r$ be a positive integer.\nThen there exists a positive integer $S$ such that:\n:for every partition of the integers $\\set {1, \\ldots, S}$ into $r$ parts, one of the parts contains integers $x$, $y$ and $z$ such that:\n::$x + y = z$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$n = \\map R {3, \\ldots, 3}$\nwhere $\\map R {3, \\ldots, 3}$ denotes the Ramsey number on $r$ colors.\nTake $S$ to be $n$.\n{{refactor|Extract the below process of \"coloring\" a partition into its own page}}\npartition the integers $\\set {1, \\ldots, n}$ into $r$ parts, which we denote by '''colors'''.\nThat is:\n:the integers in the first part are said to be '''colored''' $c_1$\n:the integers in the second part are said to be colored $c_2$ \nand so on till color $c_r$.\nThus $\\set {1, \\ldots, S}$ has been '''$r$-colored'''.\n(This terminology is common in Ramsey theory.)\nNow consider the complete graph $K_n$.\nNow color the edges of $K_n$ as follows:\n:An edge $xy$ is given color $c$ if $\\size {x - y}$ was colored $c$ in the partitioning.\n{{explain|When the page defining a \"coloring\" of a partition is written, make sure that the links are assigned appropriately from the two difference senses of \"coloring\" in the above.}}\nFrom the definition of $\\map R {3, \\ldots, 3}$ and Ramsey's Theorem, $K_n$ will definitely contain a monochromatic triangle, say built out of the vertices $i > j > k$.\nLet the triangle be colored $c_m$.\nNow $i - j$, $i - k$ and $j - k$ will also be colored $c_m$.\nThat is, $i - j$, $i - k$ and $j - k$ will belong to the same part in the partition.\nIt only remains to take $x = i - j$, $y = j - k$ and $z = i - k$ to complete the proof.\n{{qed}}\n\\end{proof}\n\n"}}, "17224": {"score": 0.9113463163375854, "content": {"text": "\\section{Birthday Paradox/General/3}\nTags: Birthday Paradox\n\n\\begin{theorem}\nLet $n$ be a set of people.\nLet the probability that at least $3$ of them have the same birthday be greater than $50 \\%$.\nThen $n \\ge 88$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map F {r, n}$ be the number of ways to distribute $r$ objects into $n$ cells such that there are no more than $2$ objects in each cell.\nLet there be $d$ cells which are each occupied by $2$ objects.\nThese can be chosen in $\\dbinom n d$ ways.\nThere remain $s = r - 2 d$ objects which can then be distributed among $n - d$ cells in $\\dbinom {n - d} s$ ways.\nIn each such arrangement, the $r$ objects may be permuted in:\n:$\\dbinom r 2 \\dbinom {r - 2} 2 \\cdots \\dbinom {r - 2 d + 2} 2 \\paren {r - 2 d}! = \\dfrac {r!} {2^d}$\ndifferent ways.\nHence:\n:$\\map F {r, n} = \\dbinom n d \\dbinom {n - d} s \\dfrac {r!} {2^d}$\nSo the probability of exactly $d$ pairs and $s$ singletons, where $d - s \\le n$, is given by:\n:$\\dfrac {\\map F {r, n} } {n^r}$\nIf we assume a $365$-day year, we have that the probability that at least $3$ of them have the same birthday is given by:\n:$\\map \\Pr r = 1 - \\ds \\sum_{d \\mathop = 0}^{\\floor {r / 2} } \\dfrac {n! \\, r!} {n^r 2^d d! \\paren {r - 2 d}! \\paren {n + d - r}!}$\nwhere $n = 365$.\nWe require the smallest $r$ for which $\\map \\Pr r > \\dfrac 1 2$.\nThe result yields to calculation.\n{{qed}}\n\\end{proof}\n\n"}}, "22993": {"score": 0.8574153184890747, "content": {"text": "\\section{Union of Blocks is Set of Points}\nTags: Design Theory, Union of Blocks is Set of Points\n\n\\begin{theorem}\nLet $\\struct {X, \\BB}$ be a pairwise balanced design.\nThat is, let $\\struct {X, \\BB}$ be a design, with $\\size X \\ge 2$, and the number of occurrences of each pair of distinct points in $\\BB$ be $\\lambda$ for some $\\lambda > 0$ constant.\nThen the set union of all the subset elements in $\\BB$ is precisely $X$.\n\\end{theorem}\n\n\\begin{proof}\nLet $X = \\set {x_1, x_2, \\ldots, x_v}$.\nLet $\\BB = \\multiset {y_1, y_2,\\ldots, y_b}$, where the notation denotes a multiset.\nLet $Y = \\ds \\bigcup_{i \\mathop = 1}^b y_i$.\nWe shall show that $Y \\subseteq X$ and $X \\subseteq Y$.\n\\end{proof}\n\n"}}, "18695": {"score": 0.8752503991127014, "content": {"text": "\\section{Pigeonhole Principle}\nTags: Pigeonhole Principle, Named Theorems, Combinatorics\n\n\\begin{theorem}\nLet $S$ be a finite set whose cardinality is $n$.\nLet $S_1, S_2, \\ldots, S_k$ be a partition of $S$ into $k$ subsets.\nThen:\n:at least one subset $S_i$ of $S$ contains at least $\\ceiling {\\dfrac n k}$ elements\nwhere $\\ceiling {\\, \\cdot \\,}$ denotes the ceiling function.\n\\end{theorem}\n\n\\begin{proof}\n{{AimForCont}} no subset $S_i$ of $S$ has as many as $\\ceiling {\\dfrac n k}$ elements.\nThen the maximum number of elements of any $S_i$ would be $\\ceiling {\\dfrac n k} - 1$.\nSo the total number of elements of $S$ would be no more than $k \\paren {\\ceiling {\\dfrac n k} - 1} = k \\ceiling {\\dfrac n k} - k$.\nThere are two cases:\n:$n$ is divisible by $k$\n:$n$ is not divisible by $k$.\nSuppose $k \\divides n$.\nThen $\\ceiling {\\dfrac n k} = \\dfrac n k$ is an integer and:\n:$k \\ceiling {\\dfrac n k} - k = n - k$\nThus:\n:$\\ds \\card S = \\sum_{i \\mathop = 1}^k \\card {S_i} \\le n - k < n$\nThis contradicts the fact that $\\card S = n$.\nHence our assumption that no subset $S_i$ of $S$ has as many as $\\ceiling {\\dfrac n k}$ elements was false.\nNext, suppose that $k \\nmid n$.\nThen:\n:$\\card S = k \\ceiling {\\dfrac n k} - k < \\dfrac {k \\paren {n + k} } k - k = n$\nand again this contradicts the fact that $\\card S = n$.\nIn the same way, our assumption that no subset $S_i$ of $S$ has as many as $\\ceiling {\\dfrac n k}$ elements was false.\nHence, by Proof by Contradiction, there has to be at least $\\ceiling {\\dfrac n k}$ elements in at least one $S_i \\subseteq S$.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/cauchy_integral1.json": {"gold": {"16899": 1}, "retrieved": {"16899": {"score": 0.8445484042167664, "content": {"text": "\\section{Cauchy-Goursat Theorem}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $D$ be a simply connected open subset of the complex plane $\\C$.\nLet $\\partial D$ denote the closed contour bounding $D$.\nLet $f: D \\to \\C$ be holomorphic everywhere in $D$.\nThen:\n:$\\ds \\oint_{\\partial D} \\map f z \\rd z = 0$\n\\end{theorem}\n\n\\begin{proof}\nBegin by rewriting the function $f$ and differential $\\rd z$ in terms of their real and complex parts:\n:$f = u + iv$\n:$\\d z = \\d x + i \\rd y$\nThen we have:\n:$\\ds \\oint_{\\partial D} \\map f z \\rd z = \\oint_{\\partial D} \\paren {u + iv} \\paren {\\d x + i \\rd y}$\nExpanding the result and again separating into real and complex parts yields two integrals of real variables:\n:$\\ds \\oint_{\\partial D} \\paren {u \\rd x - v \\rd y} + i \\oint_{\\partial D} \\paren {v \\rd x + u \\rd y}$\nWe next apply Green's Theorem to each integral term to convert the contour integrals to surface integrals over $D$:\n:$\\ds \\iint_D \\paren {-\\dfrac {\\partial v} {\\partial x} - \\dfrac {\\partial u} {\\partial y} } \\rd x \\rd y + \\iint_D \\paren {\\dfrac {\\partial u} {\\partial x} - \\dfrac {\\partial v} {\\partial y} } \\rd x \\rd y$\nBy the assumption that $f$ is holomorphic, it satisfies the Cauchy-Riemann Equations\n:$\\dfrac {\\partial v} {\\partial x} + \\dfrac {\\partial u} {\\partial y} = 0$\n:$\\dfrac {\\partial u} {\\partial x} - \\dfrac {\\partial v} {\\partial y} = 0$\nThe integrands are therefore zero and hence the integral is zero.\n{{qed}}\n\\end{proof}\n\n"}}, "15356": {"score": 0.8452781438827515, "content": {"text": "\\section{Definite Integral from 0 to 2 Pi of Reciprocal of a plus b Cosine x}\nTags: Definite Integral from 0 to 2 Pi of Reciprocal of a plus b Cosine x, Definite Integral involving Cosine Function, Definite Integrals involving Cosine Function\n\n\\begin{theorem}\n:$\\ds \\int_0^{2 \\pi} \\frac {\\d x} {a + b \\cos x} = \\frac {2 \\pi} {\\sqrt {a^2 - b^2} }$\nwhere $a$ and $b$ are real numbers with $a > b > 0$.\n\\end{theorem}\n\n\\begin{proof}\nLet $C$ be the unit open disk centred at $0$. \nThe boundary of $C$, $\\partial C$, can be parameterized by: \n:$\\map \\gamma \\theta = e^{i \\theta}$\nfor $0 \\le \\theta \\le 2 \\pi$. \nWe have:\n{{begin-eqn}}\n{{eqn\t| l = \\int_0^{2 \\pi} \\frac {\\d x} {a + b \\cos x}\n\t| r = \\int_0^{2 \\pi} \\frac {\\d x} {a + \\frac b 2 \\paren {e^{i x} + e^{-i x} } }\n\t| c = Cosine Exponential Formulation\n}}\n{{eqn\t| r = 2 \\int_0^{2 \\pi} \\frac {e^{i x} } {2 a e^{i x} + b e^{2 i x} + b} \\rd x\n}}\n{{eqn\t| r = \\frac 2 {i b} \\int_{\\partial C} \\frac {\\d z} {z^2 + \\frac {2 a} b z + 1}\n\t| c = {{Defof|Contour Integral}}, Derivative of Exponential Function\n}}\n{{eqn\t| r = \\frac 2 {i b} \\int_{\\partial C} \\frac {\\d z} {\\paren {z + \\frac a b}^2 - \\frac {a^2} {b^2} + 1}\n}}\n{{end-eqn}}\nThe integrand has poles where: \n:$\\displaystyle \\paren {z + \\frac a b}^2 - \\frac {a^2} {b^2} + 1 = 0$\nThat is, where: \n:$\\displaystyle \\size {z + \\frac a b} = \\frac {\\sqrt {a^2 - b^2} } b$\nSo: \n:$z_1 = \\dfrac {-a + \\sqrt {a^2 - b^2} } b$\nand:\n:$z_2 = \\dfrac {-a - \\sqrt {a^2 - b^2} } b$\nare the poles of the integrand.  \nWe have: \n{{begin-eqn}}\n{{eqn\t| l = \\size {z_2}\n\t| r = \\size {\\frac {-a - \\sqrt {a^2 - b^2} } b}\n}}\n{{eqn\t| r = \\frac {a + \\sqrt {a^2 - b^2} } b\n\t| c = as $a > b > 0$\n}}\n{{eqn\t| r = \\frac a b \\paren {1 + \\sqrt {1 - \\frac {b^2} {a^2} } }\n}}\n{{eqn\t| o = >\n\t| r = \\frac a b\n\t| c = as $\\sqrt {1 - \\dfrac {b^2} {a^2} } > 0$\n}}\n{{eqn\t| o = >\n\t| r = 1\n}}\n{{end-eqn}} \nSo $z_2$ lies outside the closed disk $\\size z \\le 1$ for all real $a > b > 0$, so is of no concern. \nWe now establish a bound on $z_1$. \nAs $a > b > 0$, we  have that: \n:$a^2 > a^2 - b^2 > 0$\nSo, from Square Root is Strictly Increasing: \n:$a > \\sqrt {a^2 - b^2} > 0$\nMultiplying through $-2 \\sqrt {a^2 - b^2} < 0$: \n:$-2 a \\sqrt{a^2 - b^2} < 2 \\paren {b^2 - a^2}$\nThis can be rewritten as: \n{{begin-eqn}}\n{{eqn\t| l = a^2 - 2 a \\sqrt {a^2 - b^2} + a^2 - b^2 \n\t| r = a^2 - 2 a \\sqrt {a^2 - b^2} + \\paren {\\sqrt {a^2 - b^2} }^2\n}}\n{{eqn\t| r = \\paren {-a + \\sqrt {a^2 - b^2} }^2\n\t| c = Square of Sum\n}}\n{{eqn\t| o = <\n\t| r = b^2\n}}\n{{end-eqn}}\ngiving: \n:$\\size {-a + \\sqrt {a^2 - b^2} } < b$\nTherefore: \n:$\\size {z_1} = \\size {\\dfrac {-a + \\sqrt {a^2 - b^2} } b} < 1$\nSo $z_1$ is the only pole of the integrand lying within $C$. \nTherefore: \n{{begin-eqn}}\n{{eqn\t| l = \\frac 2 {i b} \\int_C \\frac {\\d z} {z^2 + \\frac {2 a} z + 1}\n\t| r = \\frac {4 \\pi i} {i b} \\Res {\\frac 1 {z^2 + \\frac {2 a} b z + 1} } {z_1}\n\t| c = Residue Theorem\n}}\n{{eqn\t| r = \\frac {4 \\pi} b \\sqbrk {\\frac 1 {2 z + \\frac {2 a} b} }_{z = z_1}\n\t| c = Residue at Simple Pole\n}}\n{{eqn\t| r = \\frac {2 \\pi} b \\paren {\\frac 1 {-\\frac a b + \\frac {\\sqrt {a^2 - b^2} } b + \\frac a b} }\n}}\n{{eqn\t| r = \\frac {2 \\pi} {\\sqrt {a^2 - b^2} }\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "13149": {"score": 0.860416054725647, "content": {"text": "\\section{Fundamental Theorem of Calculus for Contour Integrals}\nTags: Complex Analysis, Contour Integration\n\n\\begin{theorem}\nLet $F, f: D \\to \\C$ be complex functions, where $D$ is a connected domain.\nLet $C$ be a contour that is a concatenation of the directed smooth curves $C_1, \\ldots, C_n$.\nLet $C_k$ be parameterized by the smooth path $\\gamma_k: \\closedint {a_k} {b_k} \\to D$ for all $k \\in \\set {1, \\ldots, n}$.\nSuppose that $F$ is a primitive of $f$.\nIf $C$ has start point $z$ and end point $w$, then:\n:$\\ds \\int_C \\map f z \\rd z = \\map F w - \\map F z$\nIf $C$ is a closed contour, then:\n:$\\ds \\oint_C \\map f z \\rd z = 0$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\int_C \\map f z\n      | r = \\sum_{k \\mathop = 1}^n \\int_{a_k}^{b_k} \\map f {\\map {\\gamma_k} t} \\map {\\gamma_k'} t \\rd t\n      | c = {{Defof|Complex Contour Integral}}\n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\int_{a_k}^{b_k} \\paren {\\dfrac \\rd {\\rd t} \\map F {\\map {\\gamma_k} t} } \\rd t\n      | c = Derivative of Complex Composite Function\n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\paren {\\map F {\\map {\\gamma_k} {b_k} } - \\map F {\\map {\\gamma_k} {a_k} } }\n      | c = Fundamental Theorem of Calculus for Complex Riemann Integrals\n}}\n{{eqn | r = \\map F {\\map {\\gamma_n} {b_n} } - \\map F {\\map {\\gamma_1} {a_1} }\n      | c = the sum is telescoping\n}}\n{{eqn | r = \\map F w - \\map F z\n      | c = {{Defof|Endpoints of Contour (Complex Plane)}}\n}}\n{{end-eqn}}\nIf $C$ is a closed contour, then $z = w$.\nIt follows that:\n:$\\map F w - \\map F z = 0$\n{{qed}}\n\\end{proof}\n\n"}}, "11386": {"score": 0.8513138294219971, "content": {"text": "\\section{Length of Perimeter of Cardioid}\nTags: Length of Perimeter of Cardioid, Cardioids\n\n\\begin{theorem}\nConsider the cardioid $C$ embedded in a polar plane given by its polar equation:\n:$r = 2 a \\paren {1 + \\cos \\theta}$\nwhere $a > 0$.\nThe length of the perimeter of $C$ is $16 a$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\mathcal L$ denote the length of the perimeter of $C$.\nThe boundary of the $C$ is traced out where $-\\pi \\le \\theta \\le \\pi$.\nFrom Arc Length for Parametric Equations:\n:$\\displaystyle \\mathcal L = \\int_{\\theta \\mathop = -\\pi}^{\\theta \\mathop = \\pi} \\sqrt {\\paren {\\frac {\\d x} {\\d \\theta} }^2 + \\paren {\\frac {\\d y} {\\d \\theta} }^2} \\rd \\theta$\nwhere, from Equation of Cardioid:\n:$\\begin {cases}\nx & = 2 a \\cos \\theta \\paren {1 + \\cos \\theta} \\\\\ny & = 2 a \\sin \\theta \\paren {1 + \\cos \\theta}\n\\end {cases}$\nWe have:\n{{begin-eqn}}\n{{eqn | l = \\frac {\\d x} {\\d \\theta}\n      | r = 2 a \\map {\\frac \\d {\\d \\theta} } {\\cos \\theta + \\cos^2 \\theta}\n      | c = rearranging\n}}\n{{eqn | r = -2 a \\paren {\\sin \\theta + 2 \\cos \\theta \\sin \\theta}\n      | c = Derivative of Cosine Function, Chain Rule\n}}\n{{eqn | r = -2 a \\paren {\\sin \\theta + \\sin 2 \\theta}\n      | c = Double Angle Formula for Sine\n}}\n{{eqn | l = \\frac {\\d y} {\\d \\theta}\n      | r = 2 a \\map {\\frac \\d {\\d \\theta} } {\\sin \\theta + \\sin \\theta \\cos \\theta}\n      | c = \n}}\n{{eqn | r = 2 a \\paren {\\cos \\theta + \\cos^2 \\theta - \\sin^2 \\theta}\n      | c = Derivative of Sine Function, Product Rule\n}}\n{{eqn | r = 2 a \\paren {\\cos \\theta + \\cos 2 \\theta}\n      | c = Double Angle Formula for Cosine\n}}\n{{end-eqn}}\nThus:\n{{begin-eqn}}\n{{eqn | l = \\sqrt {\\paren {\\frac {\\d x} {\\d \\theta} }^2 + \\paren {\\frac {\\d y} {\\d \\theta} }^2}\n      | r = \\sqrt {4 a^2 \\paren {\\paren {\\sin \\theta + \\sin 2 \\theta}^2 + \\paren {\\cos \\theta + \\cos 2 \\theta}^2} }\n      | c = \n}}\n{{eqn | r = 2 a \\sqrt {\\sin^2 \\theta + 2 \\sin \\theta \\sin 2 \\theta + \\sin^2 2 \\theta + \\cos^2 \\theta + 2 \\cos \\theta \\cos 2 \\theta + \\cos^2 2 \\theta}\n      | c = \n}}\n{{eqn | r = 2 a \\sqrt {2 + 2 \\sin \\theta \\sin 2 \\theta + 2 \\cos \\theta \\cos 2 \\theta}\n      | c = Sum of Squares of Sine and Cosine in $2$ instances\n}}\n{{eqn | r = 2 a \\sqrt {2 + 2 \\sin \\theta \\paren {2 \\sin \\theta \\cos \\theta} + 2 \\cos \\theta \\paren {\\cos^2 \\theta - \\sin^2 \\theta} }\n      | c = Double Angle Formulas\n}}\n{{eqn | r = 2 a \\sqrt {2 + 4 \\sin^2 \\theta \\cos \\theta + 2 \\cos^3 \\theta - 2 \\sin^2 \\theta \\cos \\theta}\n      | c = \n}}\n{{eqn | r = 2 a \\sqrt {2 + 2 \\sin^2 \\theta \\cos \\theta + 2 \\cos^3 \\theta}\n      | c = \n}}\n{{eqn | r = 2 a \\sqrt {2 + 2 \\cos \\theta \\paren {\\sin^2 \\theta + \\cos^2 \\theta} }\n      | c = \n}}\n{{eqn | r = 4 a \\sqrt {\\dfrac {1 + \\cos \\theta} 2}\n      | c = Sum of Squares of Sine and Cosine and extracting factor\n}}\n{{eqn | n = 1\n      | r = 4 a \\cos \\dfrac \\theta 2\n      | c = Half Angle Formula for Cosine\n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l = \\mathcal L\n      | r = \\int_{\\theta \\mathop = -\\pi}^{\\theta \\mathop = \\pi} \\sqrt {\\paren {\\frac {\\d x} {\\d \\theta} }^2 + \\paren {\\frac {\\d y} {\\d \\theta} }^2} \\rd \\theta\n      | c = Area between Radii and Curve in Polar Coordinates\n}}\n{{eqn | r = \\int_{-\\pi}^\\pi 4 a \\cos \\dfrac \\theta 2 \\rd \\theta\n      | c = from $(1)$\n}}\n{{eqn | r = 4 a \\intlimits {2 \\sin \\dfrac \\theta 2} {-\\pi} \\pi\n      | c = Primitive of $\\cos a x$\n}}\n{{eqn | r = 8 a \\paren {\\sin \\dfrac \\pi 2 - \\sin \\dfrac {-\\pi} 2}\n      | c = evaluation between $-\\pi$ and $\\pi$\n}}\n{{eqn | r = 8 a \\paren {1 - \\paren {-1} }\n      | c = Sine of Right Angle, Sine Function is Odd\n}}\n{{eqn | r = 16 a\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "12974": {"score": 0.8474324941635132, "content": {"text": "\\section{Green's Theorem}\nTags: Integral Calculus\n\n\\begin{theorem}\nLet $\\Gamma$ be a positively oriented piecewise smooth simple closed curve in  $\\R^2$.\nLet $U = \\Int \\Gamma$, that is, the interior of $\\Gamma$.\nLet $A$ and $B$ be functions of $\\tuple {x, y}$ defined on an open region containing $U$ and have continuous partial derivatives in such a set.\nThen:\n:$\\ds \\oint_\\Gamma \\paren {A \\rd x + B \\rd y} = \\iint_U \\paren {\\frac {\\partial B} {\\partial x} - \\frac {\\partial A} {\\partial y} } \\rd x \\rd y$\n\\end{theorem}\n\n\\begin{proof}\nIt suffices to demonstrate the theorem for rectangular regions in the $x y$-plane.\nThe Riemann-sum nature of the double integral will then guarantee the proof of the theorem for arbitrary regions, because a Riemann-sum is technically a summation of the areas of arbitrarily small rectangles. \nAs the proof is for a rectangle, the proof will work for arbitrary regions, which can be approximated by collections of ever smaller rectangles.\n{{handwaving|To do this formally involves the Monotone Convergence Theorem (Measure Theory) for $U$ and approximating $\\Gamma$ by means of a \"piecewise rectification\". Its arduous, yet necessary.}}\nLet $R = \\set {\\tuple {x, y}: a \\le x \\le b, c \\le y \\le d}$ be a rectangular region.\nLet the boundary $C$ of $R$ be oriented counterclockwise.\nWe break the boundary into $4$ pieces:\n:$C_1$, which runs from $\\tuple {a, c}$ to $\\tuple {b, c}$\n:$C_2$, which runs from $\\tuple {b, c}$ to $\\tuple {b, d}$\n:$C_3$, which runs from $\\tuple {b, d}$ to $\\tuple {a, d}$\n:$C_4$, which runs from $\\tuple {a, d}$ to $\\tuple {a, c}$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\iint_R \\frac {\\partial B} {\\partial x} \\rd x \\rd y\n      | r = \\int_c^d \\int_a^b \\frac {\\partial B} {\\partial x} \\rd x \\rd y\n      | c = \n}}\n{{eqn | r = \\int_c^d \\paren {\\map B {b, y} - \\map B {a, y} } \\rd y\n      | c = \n}}\n{{eqn | r = \\int_c^d  \\map B {b, y} \\rd y + \\int_d^c \\map B {a, y} \\rd y\n      | c = \n}}\n{{eqn | r = \\int_{C_2} B \\rd y + \\int_{C_4} B \\rd y\n      | c = \n}}\n{{end-eqn}}\nWe note that $y$ is constant along $C_1$ and $C_3$.\nSo:\n:$\\ds \\int_{C_1} B \\rd y = \\int_{C_3} B \\rd y = 0$\nHence:\n{{begin-eqn}}\n{{eqn | l = \\int_{C_2} B \\rd y + \\int_{C_4} B \\rd y\n      | r = \\int_{C_1} B \\rd y + \\int_{C_2} B \\rd y + \\int_{C_3} B \\rd y + \\int_{C_4} B \\rd y\n      | c = \n}}\n{{eqn | r = \\oint_C B \\rd y\n      | c = \n}}\n{{end-eqn}}\nA similar argument demonstrates that:\n:$\\ds \\iint_R \\frac {\\partial A} {\\partial y} \\rd x \\rd y = -\\oint_C A \\d x$\nand hence:\n:$\\ds \\oint_C \\paren {A \\rd x + B \\rd y} = \\iint_R \\paren {\\frac {\\partial B} {\\partial x} - \\frac {\\partial A} {\\partial y} } \\rd x \\rd y$\n{{qed}}\n\\end{proof}\n\n"}}, "17457": {"score": 0.8659444451332092, "content": {"text": "\\section{Area of Loop of Folium of Descartes}\nTags: Folium of Descartes\n\n\\begin{theorem}\nConsider the folium of Descartes $F$, given in parametric form as:\n:$\\begin {cases} x = \\dfrac {3 a t} {1 + t^3} \\\\ y = \\dfrac {3 a t^2} {1 + t^3} \\end {cases}$\nThe area $\\AA$ of the loop of $F$ is given as:\n:$\\AA = \\dfrac {3 a^2} 2$\n\\end{theorem}\n\n\\begin{proof}\nFrom Behaviour of Parametric Equations for Folium of Descartes according to Parameter we have that the loop is traversed for $0 \\le t < +\\infty$.\nWe convert the parametric equation to polar form:\n{{begin-eqn}}\n{{eqn | l = r^2\n      | r = x^2 + y^2\n      | c = \n}}\n{{eqn | r = \\dfrac {\\paren {3 a t}^2} {\\paren {1 + t^3}^2} + \\dfrac {\\paren {3 a t^2}^2} {\\paren {1 + t^3}^2}\n      | c = \n}}\n{{eqn | r = \\dfrac {\\paren {3 a t}^2 \\paren {1 + t^2} } {\\paren {1 + t^3}^2}\n      | c = \n}}\n{{eqn | l = \\tan \\theta\n      | r = \\dfrac y x\n      | c = \n}}\n{{eqn | r = \\dfrac {3 a t^2} {1 + t^3} \\dfrac {1 + t^3} {3 a t}\n      | c = \n}}\n{{eqn | r = t\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\theta\n      | r = \\arctan t\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\dfrac {\\d \\theta} {\\d t}\n      | r = \\dfrac 1 {1 + t^2}\n      | c = Derivative of Arctangent Function\n}}\n{{end-eqn}}\nThen we have:\n{{begin-eqn}}\n{{eqn | l = \\AA\n      | r = \\dfrac 1 2 \\int_{t \\mathop = 0}^{t \\mathop \\to \\infty} r^2 \\rd \\theta\n      | c = Area between Radii and Curve in Polar Coordinates\n}}\n{{eqn | r = \\dfrac 1 2 \\int_{t \\mathop = 0}^{t \\mathop \\to \\infty} \\dfrac {\\paren {3 a t}^2 \\paren {1 + t^2} } {\\paren {1 + t^3}^2} \\dfrac 1 {1 + t^2} \\rd t\n      | c = Integration by Substitution\n}}\n{{eqn | r = \\dfrac {3 a^2} 2 \\int_{t \\mathop = 0}^{t \\mathop \\to \\infty} \\dfrac {3 t^2 \\rd t} {\\paren {1 + t^3}^2}\n      | c = Integration by Substitution\n}}\n{{end-eqn}}\nThen:\n{{begin-eqn}}\n{{eqn | l = u\n      | r = 1 + t^3\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\dfrac {\\d u} {\\d t}\n      | r = 3 t^2\n      | c = \n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l = t\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = u\n      | r = 1\n      | c = \n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l = t\n      | o = \\to\n      | r = +\\infty\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = u\n      | o = \\to\n      | r = +\\infty\n      | c = \n}}\n{{end-eqn}}\nwhich leads us to:\n{{begin-eqn}}\n{{eqn | l = \\AA\n      | r = \\dfrac {3 a^2} 2 \\int_{t \\mathop = 0}^{t \\mathop \\to \\infty} \\dfrac {3 t^2 \\rd t} {\\paren {1 + t^3}^2}\n      | c = \n}}\n{{eqn | r = \\dfrac {3 a^2} 2 \\int_{u \\mathop = 1}^{u \\mathop \\to \\infty} \\dfrac {\\d u} {u^2} \n      | c = Integration by Substitution\n}}\n{{eqn | r = \\dfrac {3 a^2} 2 \\intlimits {-\\dfrac 1 u} 1 {+\\infty}\n      | c = Primitive of Power\n}}\n{{eqn | r = \\dfrac {3 a^2} 2 \\paren {-0 - \\paren {-1} }\n      | c = evaluating limits: $u \\to +\\infty \\implies \\dfrac 1 u \\to 0$\n}}\n{{eqn | r = \\dfrac {3 a^2} 2\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "17441": {"score": 0.8620398640632629, "content": {"text": "\\section{Area between Radii and Curve in Polar Coordinates}\nTags: Area Formulas\n\n\\begin{theorem}\nLet $C$ be a curve expressed in polar coordinates $\\polar {r, \\theta}$ as:\n:$r = \\map g \\theta$\nwhere $g$ is a real function.\nLet $\\theta = \\theta_a$ and $\\theta = \\theta_b$ be the two rays from the pole at angles $\\theta_a$ and $\\theta_b$ to the polar axis respectively.\nThen the area $\\AA$ between $\\theta_a$, $\\theta_b$ and $C$ is given by:\n:$\\ds \\AA = \\int \\limits_{\\theta \\mathop = \\theta_a}^{\\theta \\mathop = \\theta_b} \\frac {\\paren {\\map g \\theta}^2 \\rd \\theta} 2$\nas long as $\\paren {\\map g \\theta}^2$ is integrable.\n\\end{theorem}\n\n\\begin{proof}\n:600px\n{{ProofWanted}}\nCategory:Area Formulas\n\\end{proof}\n\n"}}, "17447": {"score": 0.8736535906791687, "content": {"text": "\\section{Area inside Cardioid}\nTags: Cardioids\n\n\\begin{theorem}\nConsider the cardioid $C$ embedded in a polar plane given by its polar equation:\n:$r = 2 a \\paren {1 + \\cos \\theta}$\nThe area inside $C$ is $6 \\pi a^2$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\AA$ denote the area inside $C$.\nThe boundary of $C$ is traced out where $-\\pi \\le \\theta \\le \\pi$.\nThus:\n{{begin-eqn}}\n{{eqn | l = \\AA\n      | r = \\int_{-\\pi}^\\pi \\dfrac {\\map {r^2} \\theta} 2 \\rd \\theta\n      | c = Area between Radii and Curve in Polar Coordinates\n}}\n{{eqn | r = \\int_{-\\pi}^\\pi \\dfrac {\\paren {2 a \\paren {1 + \\cos \\theta} }^2} 2 \\rd \\theta\n      | c = Equation of Cardioid\n}}\n{{eqn | r = 2 a^2 \\int_{-\\pi}^\\pi \\paren {1 + 2 \\cos \\theta + \\cos^2 \\theta} \\rd \\theta\n      | c = simplifying\n}}\n{{eqn | r = 2 a^2 \\intlimits {\\theta + 2 \\sin \\theta + \\frac \\theta 2 + \\frac {\\sin 2 \\theta} 4} {-\\pi} \\pi\n      | c = Primitive of $\\cos a x$, Primitive of Square of Cosine Function\n}}\n{{eqn | r = 2 a^2 \\paren {\\paren {\\pi + 2 \\sin \\pi + \\dfrac \\pi 2 + \\frac {\\sin 2 \\pi} 4} - \\paren {-\\pi + 2 \\, \\map \\sin {-\\pi} + \\dfrac {-\\pi} 2 + \\frac {\\map \\sin {-2 \\pi} } 4} }\n      | c = evaluation between $-\\pi$ and $\\pi$\n}}\n{{eqn | r = 2 a^2 \\paren {\\pi + \\dfrac \\pi 2 - \\paren {-\\pi} - \\paren {\\dfrac {-\\pi} 2} }\n      | c = Sine of Multiple of Pi\n}}\n{{eqn | r = 6 \\pi a^2\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "5146": {"score": 0.862779438495636, "content": {"text": "\\begin{definition}[Definition:Line Integral]\nLet $OA$ be a curve in a vector field $\\mathbf F$.\nLet $P$ be a point on $OA$.\nLet $\\d \\mathbf l$ be a small element of length of $OA$ at $P$.\nLet $\\mathbf v$ be the vector induced by $\\mathbf F$ on $P$.\nLet $\\mathbf v$ make an angle $\\theta$ with the tangent to $OA$ at $P$.\n:350px\nHence:\n:$\\mathbf v \\cdot \\d \\mathbf l = v \\cos \\theta \\rd l$\nwhere:\n:$\\cdot$ denotes dot product\n:$v$ and $\\d l$ denote the magnitude of $\\mathbf v$ and $\\d \\mathbf l$ respectively.\nThe '''line integral'''  of $\\mathbf v$ along $OA$ is therefore defined as:\n:$\\ds \\int_O^A \\mathbf v \\cdot \\d \\mathbf l = \\int_O^A v \\cos \\theta \\rd l$\n\\end{definition}"}}, "13150": {"score": 0.8628510236740112, "content": {"text": "\\section{Fundamental Theorem of Contour Integration}\nTags: Contour Integration, Complex Analysis, Fundamental Theorems\n\n\\begin{theorem}\nLet $D \\subseteq \\C$ be an open set.\nLet $f: D \\to \\C$ be a continuous function.\nSuppose that $F: D \\to \\C$ is an antiderivative of $f$.\nLet $\\gamma: \\closedint a b \\to D$ be a contour in $D$.\nThen the contour integral:\n:$\\ds \\int_\\gamma \\map f z \\rd z =  \\map F {\\map \\gamma b} - \\map F {\\map \\gamma a}$\n\\end{theorem}\n\n\\begin{proof}\nBy the chain rule:\n:$\\dfrac \\d {\\d t} \\map F {\\map \\gamma t} = \\map {F'} {\\map \\gamma t} \\map {\\gamma'} t = \\map f {\\map \\gamma t} \\map {\\gamma'} t$\nThus:\n{{begin-eqn}}\n{{eqn | l = \\int_\\gamma \\map f z \\rd z\n      | r = \\int_a^b \\map f {\\map \\gamma t} \\map {\\gamma'} t \\rd t\n      | c = {{Defof|Complex Primitive|Antiderivative}}\n}}\n{{eqn | r = \\int_a^b \\frac \\d {\\d t} \\map F {\\map \\gamma t} \\rd t\n      | c = \n}}\n{{eqn | r = \\map F {\\map \\gamma b} - \\map F {\\map \\gamma a}\n      | c = Fundamental Theorem of Calculus\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Complex Analysis\nCategory:Contour Integration\nCategory:Fundamental Theorems\n\\end{proof}\n\n"}}}}, "TheoremQA_elainewan/math_algebra_5.json": {"gold": {"18400": 1}, "retrieved": {"6597": {"score": 0.8176599144935608, "content": {"text": "\\begin{definition}[Definition:Orthogonal (Linear Algebra)/Sets]\nLet $\\struct {V, \\innerprod \\cdot \\cdot}$ be an inner product space.\nLet $A, B \\subseteq V$.\nWe say that $A$ and $B$ are '''orthogonal''' {{iff}}:\n:$\\forall a \\in A, b \\in B: a \\perp b$\nThat is, if $a$ and $b$ are orthogonal elements of $A$ and $B$ for all $a \\in A$ and $b \\in B$.\nWe write: \n:$A \\perp B$\n\\end{definition}"}}, "6594": {"score": 0.8233544826507568, "content": {"text": "\\begin{definition}[Definition:Orthogonal (Linear Algebra)/Orthogonal Complement]\nLet $\\mathbb K$ be a field.\nLet $V$ be a vector space over $\\mathbb K$.\nLet $\\struct {V, \\innerprod \\cdot \\cdot}$ be an inner product space.\nLet $S\\subseteq V$ be a subset.\nWe define the '''orthogonal complement of $S$ (with respect to $\\innerprod \\cdot \\cdot$)''', written $S^\\perp$ as the set of all $v \\in V$ which are orthogonal to all $s \\in S$.\nThat is: \n:$S^\\perp = \\set {v \\in V : \\innerprod v s = 0 \\text { for all } s \\in S}$\nIf $S = \\set v$ is a singleton, we may write $S^\\perp$ as $v^\\perp$.\n\\end{definition}"}}, "12940": {"score": 0.831339418888092, "content": {"text": "\\section{Gram-Schmidt Orthogonalization/Inner Product Space}\nTags: Linear Algebra, Vector Algebra, Gram-Schmidt Orthogonalization\n\n\\begin{theorem}\nLet $\\struct {V, \\innerprod \\cdot \\cdot}$ be an $n$-dimensional inner product space.\nLet $\\tuple {v_1, \\ldots, v_n}$ be any ordered basis for $V$.\nThen there is an orthonormal ordered basis $\\tuple {b_1, \\ldots, b_n}$ satisfying the following conditions:\n:$\\forall k \\in \\N_{\\mathop \\le n} : \\map \\span {b_1, \\ldots, b_k} = \\map \\span {v_1, \\ldots v_k}$\n\\end{theorem}\n\n\\begin{proof}\nLet $b_1 = \\dfrac {v_1} {\\size {v_1} }$ where $\\size {\\, \\cdot \\,}$ denotes the inner product norm.\nFor all $j \\in \\N$ such that $1 < j \\le n$ let:\n:$\\ds b_j = \\frac {\\ds v_j - \\sum_{i \\mathop = 1}^{j - 1} \\innerprod {v_j} {b_i} b_i} {\\ds \\size {v_j - \\sum_{i \\mathop = 1}^{j - 1} \\innerprod {v_j} {b_i} b_i} }$\nWe have that $v_1 \\ne 0$.\nFurthermore:\n:$\\forall j \\in \\N : 2 \\le j \\le n : v_j \\notin \\map \\span {b_1, \\ldots b_{j - 1} }$\nTake the inner product of $b_j$ and $b_k$:\n{{begin-eqn}}\n{{eqn | l = \\innerprod {b_j} {b_k}\n      | r = \\frac {\\ds \\innerprod {v_j - \\sum_{i \\mathop = 1}^{j - 1} \\innerprod {v_j} {b_i} b_i } {v_k - \\sum_{i \\mathop = 1}^{k - 1} \\innerprod {v_k} {b_i} b_i } } {\\ds \\size {v_j - \\sum_{i \\mathop = 1}^{j - 1} \\innerprod {v_j} {b_i} b_i } \\size {v_k - \\sum_{i \\mathop = 1}^{k - 1} \\innerprod {v_k} {b_i} b_i } }\n}}\n{{eqn | r = \\frac {\\ds \\innerprod {v_j} {v_k} - \\sum_{i \\mathop = 1}^{k - 1} \\innerprod {v_k} {b_i} \\innerprod {v_j} {b_i} - \\sum_{i \\mathop = 1}^{j - 1} \\innerprod {v_j} {b_i} \\innerprod {b_i} {v_k} + \\sum_{i \\mathop = 1}^{k - 1} \\sum_{h \\mathop = 1}^{j - 1} \\innerprod {v_k} {b_i} \\innerprod {v_j} {b_h} \\innerprod {b_i} {b_h} } {\\size {b_j} \\size {b_k} }\n}}\n{{end-eqn}}\n{{ProofWanted|finish the proof: orthogonality, span}}\n{{Namedfor|J\u00f8rgen Pedersen Gram|name2 = Erhard Schmidt|cat = Gram|cat2 = Schmidt}}\n\\end{proof}\n\n"}}, "16090": {"score": 0.8256486058235168, "content": {"text": "\\section{Component of Vector is Scalar Projection on Standard Ordered Basis Element}\nTags: Scalar Projections\n\n\\begin{theorem}\nLet $\\tuple {\\mathbf e_1, \\mathbf e_2, \\mathbf e_3}$ be the standard ordered basis of Cartesian $3$-space $S$.\nLet $\\mathbf a = a_1 \\mathbf e_1 + a_2 \\mathbf e_2 + a_3 \\mathbf e_3$ be a vector quantity in $S$.\nThen:\n:$\\mathbf a \\cdot \\mathbf e_i = a_i$\n\\end{theorem}\n\n\\begin{proof}\nUsing the Einstein summation convention\n{{begin-eqn}}\n{{eqn | l = \\mathbf a \\cdot \\mathbf e_i\n      | r = a_j \\cdot \\mathbf e_j \\cdot \\mathbf e_i\n      | c = \n}}\n{{eqn | r = a_j \\delta_{i j}\n      | c = Dot Product of Orthogonal Basis Vectors\n}}\n{{eqn | r = a_i\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "9982": {"score": 0.8246167302131653, "content": {"text": "\\begin{definition}[Definition:Vector Projection]\nLet $\\mathbf u$ and $\\mathbf v$ be vector quantities.\n\\end{definition}"}}, "9984": {"score": 0.8471890687942505, "content": {"text": "\\begin{definition}[Definition:Vector Projection/Definition 2]\nLet $\\mathbf u$ and $\\mathbf v$ be vector quantities.\nThe '''(vector) projection of $\\mathbf u$ onto $\\mathbf v$''' is defined and denoted:\n:$\\proj_\\mathbf v \\mathbf u = \\dfrac {\\mathbf u \\cdot \\mathbf v} {\\norm {\\mathbf v}^2} \\mathbf v$\nwhere:\n:$\\cdot$ denotes the dot product\n:$\\norm {\\mathbf v}$ denotes the magnitude of $\\mathbf v$.\n:300px\n\\end{definition}"}}, "6608": {"score": 0.8325310349464417, "content": {"text": "\\begin{definition}[Definition:Orthogonal Subspaces]\nLet $\\struct {V, \\innerprod \\cdot \\cdot}$ be an inner product space.\nLet $A$ and $B$ be closed linear subspaces of $V$.\nLet $A$ and $B$ be orthogonal in $V$.\nThen we say that $A$ and $B$ are '''orthogonal subspaces'''.\n\\end{definition}"}}, "18400": {"score": 0.8601915836334229, "content": {"text": "\\section{Orthogonal Projection onto Closed Linear Span}\nTags: Linear Transformations on Hilbert Spaces, Hilbert Spaces\n\n\\begin{theorem}\nLet $H$ be a Hilbert space with inner product $\\innerprod \\cdot \\cdot$ and inner product norm $\\norm \\cdot$. \nLet $E = \\set {e_1, \\ldots, e_n}$ be an orthonormal subset of $H$.\nLet $M = \\vee E$, where $\\vee E$ is the closed linear span of $E$. \nLet $P$ be the orthogonal projection onto $M$.\nThen:\n:$\\forall h \\in H: P h = \\ds \\sum_{k \\mathop = 1}^n \\innerprod h {e_k} e_k$\n\\end{theorem}\n\n\\begin{proof}\nLet $h \\in H$. \nLet: \n:$\\ds u = \\sum_{k \\mathop = 1}^n \\innerprod h {e_k} e_k$ \nWe have that:\n:$u \\in \\map \\span E$\nand from the definition of closed linear span:\n:$M = \\paren {\\map \\span E}^-$\nWe therefore have, by the definition of closure: \n:$u \\in M$ \nLet $v = h - u$ \nWe want to show that $v \\in M^\\bot$. \nFrom Intersection of Orthocomplements is Orthocomplement of Closed Linear Span, it suffices to show that: \n:$v \\in E^\\bot$\nNote that for each $l$ we have: \n:$\\innerprod v {e_l} = \\innerprod h {e_l} - \\innerprod u {e_l}$\nsince the inner product is linear in its first argument. \nWe have: \n{{begin-eqn}}\n{{eqn\t| l = \\innerprod u {e_l} \n\t| r = \\innerprod {\\sum_{k \\mathop = 1}^n \\innerprod h {e_k} e_k} {e_l}\n}}\n{{eqn\t| r = \\sum_{k \\mathop = 1}^n \\innerprod {\\innerprod h {e_k} e_k} {e_l}\n\t| c = linearity of inner product in first argument\n}}\n{{eqn\t| r = \\sum_{k \\mathop = 1}^n \\innerprod h {e_k} \\innerprod {e_k} {e_l}\n\t| c = linearity of inner product in first argument\n}}\n{{eqn\t| r = \\innerprod h {e_l} \\innerprod {e_l} {e_l}\n\t| c = {{Defof|Orthonormal Subset}}\n}}\n{{eqn\t| r = \\innerprod h {e_l} \\norm {e_l}^2\n\t| c = {{Defof|Inner Product Norm}}\n}}\n{{eqn\t| r = \\innerprod h {e_l}\n\t| c = since $\\norm {e_l} = 1$\n}}\n{{end-eqn}}\nso:\n:$\\innerprod v {e_l} = 0$ \nThat is: \n:$v \\in E^\\bot$\nso, by Intersection of Orthocomplements is Orthocomplement of Closed Linear Span, we have: \n:$v \\in M^\\bot$\nWe can therefore decompose each $h \\in H$ as: \n:$h = u + v$\nwith $u \\in M$ and $v \\in M^\\bot$. \nSo we have: \n{{begin-eqn}}\n{{eqn\t| l = P h \n\t| r = \\map P {u + v}\n}}\n{{eqn\t| r = \\map P u + \\map P v\n\t| c = Orthogonal Projection on Closed Linear Subspace of Hilbert Space is Linear Transformation\n}}\n{{eqn\t| r = v\n\t| c = Kernel of Orthogonal Projection on Closed Linear Subspace of Hilbert Space, Fixed Points of Orthogonal Projection on Closed Linear Subspace of Hilbert Space\n}}\n{{eqn\t| r = \\sum_{k \\mathop = 1}^n \\innerprod h {e_k} e_k\n}}\n{{end-eqn}}\nfor each $h \\in H$. \n{{qed}}\n\\end{proof}\n\n"}}, "9985": {"score": 0.8340150117874146, "content": {"text": "\\begin{definition}[Definition:Vector Projection/Definition 3]\nLet $\\mathbf u$ and $\\mathbf v$ be vector quantities.\nThe '''(vector) projection of $\\mathbf u$ onto $\\mathbf v$''' is defined and denoted:\n:$\\proj_\\mathbf v \\mathbf u = u_{\\parallel \\mathbf v} \\mathbf {\\hat v}$\nwhere:\n:$u_{\\parallel \\mathbf v}$ denotes the scalar projection of $\\mathbf u$ on $\\mathbf v$\n:$\\mathbf {\\hat v}$ denotes the unit vector in the direction of $\\mathbf v$.\n:300px\n\\end{definition}"}}, "9983": {"score": 0.840062141418457, "content": {"text": "\\begin{definition}[Definition:Vector Projection/Definition 1]\nLet $\\mathbf u$ and $\\mathbf v$ be vector quantities.\nThe '''(vector) projection of $\\mathbf u$ onto $\\mathbf v$''', denoted $\\proj_\\mathbf v \\mathbf u$, is the orthogonal projection of $\\mathbf u$ onto a straight line which is parallel to $\\mathbf v$.\nHence $\\proj_\\mathbf v \\mathbf u$ is a like vector to $\\mathbf v$ whose length is $\\norm {\\mathbf u} \\cos \\theta$, where:\n:$\\norm {\\mathbf u}$ is the magnitude of $\\mathbf u$\n:$\\cos \\theta$ is the angle between $\\mathbf u$ and $\\mathbf v$.\n:300px\n\\end{definition}"}}}}, "TheoremQA_jianyu_xu/Multinomial_4.json": {"gold": {"10463": 1}, "retrieved": {"6877": {"score": 0.8286389112472534, "content": {"text": "\\begin{definition}[Definition:Permutation/Ordered Selection/Notation]\nThe number of $r$-permutations from a set of cardinality $n$ is denoted variously:\n:$P_{n r}$\n:${}^r P_n$\n:${}_r P_n$\n:${}_n P_r$ (extra confusingly)\nThere is little consistency in the literature).\nOn {{ProofWiki}} the notation of choice is ${}^r P_n$.\nCategory:Definitions/Permutation Theory\n\\end{definition}"}}, "1332": {"score": 0.8324776291847229, "content": {"text": "\\begin{definition}[Definition:Combination]\nLet $S$ be a set containing $n$ elements.\nAn '''$r$-combination of $S$''' is a subset of $S$ which has $r$ elements.\n\\end{definition}"}}, "10462": {"score": 0.8382868766784668, "content": {"text": "\\section{Multinomial Coefficient expressed as Product of Binomial Coefficients}\nTags: Multinomial Coefficients, Binomial Coefficients\n\n\\begin{theorem}\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} = \\dbinom {k_1 + k_2} {k_1} \\dbinom {k_1 + k_2 + k_3} {k_1 + k_2} \\cdots \\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1 + k_2 + \\cdots + k_{m - 1} }$\nwhere:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m}$ denotes a multinomial coefficient\n:$\\dbinom {k_1 + k_2} {k_1}$ etc. denotes binomial coefficients.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction.\nFor all $m \\in \\Z_{> 1}$, let $\\map P m$ be the proposition:\n:$\\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1, k_2, \\ldots, k_m} = \\dbinom {k_1 + k_2} {k_1} \\dbinom {k_1 + k_2 + k_3} {k_1 + k_2} \\cdots \\dbinom {k_1 + k_2 + \\cdots + k_m} {k_1 + k_2 + \\cdots + k_{m - 1} }$\n\\end{proof}\n\n"}}, "15526": {"score": 0.8343754410743713, "content": {"text": "\\section{Count of All Permutations on n Objects}\nTags: Permutation Theory, Count of All Permutations on n Objects\n\n\\begin{theorem}\nLet $S$ be a set of $n$ objects.\nLet $N$ be the number of permutations of $r$ objects from $S$, where $1 \\le r \\le N$.\nThen:\n:$\\ds N = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}$\n\\end{theorem}\n\n\\begin{proof}\nThe number of permutations on $k$ objects, from $n$ is denoted ${}^k P_{10}$.\nFrom Number of Permutations:\n:${}^k P_n = \\dfrac {n!} {\\paren {n - k}!}$\nHence:\n{{begin-eqn}}\n{{eqn | q = \n      | l = N\n      | r = \\sum_{k \\mathop = 1}^n {}^k P_n\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {\\paren {n - k}!}\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {k!}\n      | c = \n}}\n{{eqn | r = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Permutation Theory\nCategory:Count of All Permutations on n Objects\n\\end{proof}\n\n"}}, "19708": {"score": 0.8333538174629211, "content": {"text": "\\section{Product Rule for Counting}\nTags: Product Rule for Counting, Counting Arguments, Combinatorics, combinatorics\n\n\\begin{theorem}\nLet it be possible to choose an element $\\alpha$ from a given set $S$ in $m$ different ways.\nLet it be possible to choose an element $\\beta$ from a given set $T$ in $n$ different ways.\nThen the ordered pair $\\tuple {\\alpha, \\beta}$ can be chosen from the cartesian product $S \\times T$ in $m n$ different ways.\n\\end{theorem}\n\n\\begin{proof}\n{{handwaving}}\nThe validity of this rule follows directly from the definition of multiplication of integers.\nThe product $a b$ (for $a, b \\in \\N_{>0}$) is the number of sequences $\\sequence {A, B}$, where $A$ can be any one of $a$ items and $B$ can be any one of $b$ items.\n{{qed}}\n\\end{proof}\n\n"}}, "17942": {"score": 0.8590610027313232, "content": {"text": "\\section{Number of Permutations with Repetition}\nTags: Number of Permutations with Repetition, Combinatorics\n\n\\begin{theorem}\nSet $S$ be a set of $n$ elements.\nLet $\\sequence T_m$ be a sequence of $m$ terms of $S$.\nThen there are $n^m$ different instances of $\\sequence T_m$.\n\\end{theorem}\n\n\\begin{proof}\nLet $N_m$ denote the set $\\set {1, 2, \\ldots, m}$.\nLet $f: N_m \\to S$ be the mapping defined as:\n:$\\forall k \\in N_m: \\map f t = t_m$\nBy definition, $f$ corresponds to one of the specific instances of $\\sequence T_m$.\nHence the number of different instances of $\\sequence T_m$ is found from Cardinality of Set of All Mappings:\n:$\\card S^{\\card {N_m} }$\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "17753": {"score": 0.8483644723892212, "content": {"text": "\\section{1782 is 3 Times Sum of all 2-Digit Numbers from its Digits}\nTags: 1782, Recreational Mathematics\n\n\\begin{theorem}\n$1782$ equals $3$ multiplied by the sum of all the $2$-digit integers that can be formed from its digits.\n\\end{theorem}\n\n\\begin{proof}\nThe number of $2$-digit integers that can be formed from the digits of $1782$ equals the number of $2$-permutations of $\\set {1, 7, 8, 2}$.\nThat is:\n:$\\set {17, 18, 12, 71, 78, 72, 81, 87, 82, 21, 27, 28}$\nHence:\n:$17 + 18 + 12 + 71 + 78 + 72 + 81 + 87 + 82 + 21 + 27 + 28 = 594 = \\dfrac {1782} 3$\n{{qed}}\n\\end{proof}\n\n"}}, "16939": {"score": 0.8757233023643494, "content": {"text": "\\section{Cardinality of Set of Combinations with Repetition}\nTags: Combinations with Repetition\n\n\\begin{theorem}\nLet $S$ be a finite set with $n$ elements\nThe number of $k$-combinations of $S$ with repetition is given by:\n:$N = \\dbinom {n + k - 1} k$\n\\end{theorem}\n\n\\begin{proof}\nLet the elements of $S$ be (totally) ordered in some way, by assigning an index to each element.\nThus let $S = \\left\\{ {a_1, a_2, a_3, \\ldots, a_n}\\right\\}$.\nThus each $k$-combination of $S$ with repetition can be expressed as:\n:$\\left\\{ {\\left\\{ {a_{r_1}, a_{r_1}, \\ldots, a_{r_k} }\\right\\} }\\right\\}$\nwhere:\n:$r_1, r_2, \\ldots, r_k$ are all elements of $\\left\\{ {1, 2, \\ldots, n}\\right\\}$\nand such that:\n:$r_1 \\le r_2 \\le \\cdots \\le r_k$\nHence the problem reduces to the number of integer solutions $\\left({r_1, r_2, \\ldots, r_k}\\right)$ such that $1 \\le r_1 \\le r_2 \\le \\cdots \\le r_k \\le n$.\nThis is the same as the number of solutions to:\n:$0 < r_1 < r_2 + 1 < \\cdots < r_k + k - 1 < n + k$\nwhich is the same as the number of solutions to:\n:$0 < s_1 < s_2 < \\cdots < s_k < n + k$\nwhich is the number of ways to choose $k$ elements from $n + k - 1$.\nThis is the same as the number of subsets as a set with $n + k - 1$ elements.\nFrom Cardinality of Set of Subsets:\n:$N = \\dbinom {n + k - 1} k$\n{{qed}}\n\\end{proof}\n\n"}}, "17192": {"score": 0.8544687032699585, "content": {"text": "\\section{Binomial Coefficient/Examples/Number of Bridge Hands}\nTags: Binomial Coefficients, Examples of Binomial Coefficients\n\n\\begin{theorem}\nThe total number $N$ of possible different hands for a game of [https://en.wikipedia.org/wiki/Contract_bridge bridge] is:\n:$N = \\dfrac {52!} {13! \\, 39!} = 635 \\ 013 \\ 559 \\ 600$\n\\end{theorem}\n\n\\begin{proof}\nThe total number of cards in a standard deck is $52$.\nThe number of cards in a single bridge hand is $13$.\nThus $N$ is equal to the number of ways $13$ things can be chosen from $52$.\nThus:\n{{begin-eqn}}\n{{eqn | l = N\n      | r = \\dbinom {52} {23}\n      | c = Cardinality of Set of Subsets\n}}\n{{eqn | r = \\frac {52!} {13! \\left({52 - 13}\\right)!}\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\frac {52!} {13! \\, 39!}\n      | c = \n}}\n{{eqn | r = 635 \\ 013 \\ 559 \\ 600\n      | c = after calculation\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "1333": {"score": 0.8578039407730103, "content": {"text": "\\begin{definition}[Definition:Combination with Repetition]\nLet $S$ be a (finite) set with $n$ elements.\nA '''$k$ combination of $S$ with repetition''' is a multiset with $k$ elements selected from $S$.\n\\end{definition}"}}}}, "TheoremQA_mingyin/Fundamental-Theorem-of-Calculus3.json": {"gold": {"11986": 1}, "retrieved": {"19397": {"score": 0.9305736422538757, "content": {"text": "\\section{Primitive of Reciprocal of x cubed by Root of a squared minus x squared}\nTags: Primitives involving Root of a squared minus x squared\n\n\\begin{theorem}\n:$\\ds \\int \\frac {\\d x} {x^3 \\sqrt {a^2 - x^2} } = \\frac {-\\sqrt {a^2 - x^2} } {2 a^2 x^2} - \\frac 1 {2 a^3} \\map \\ln {\\frac {a + \\sqrt {a^2 - x^2} } {\\size x} } + C$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n{{begin-eqn}}\n{{eqn | l = z\n      | r = x^2\n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac {\\d z} {\\d x}\n      | r = 2 x\n      | c = Power Rule for Derivatives\n}}\n{{eqn | ll= \\leadsto\n      | l = \\int \\frac {\\d x} {x^3 \\sqrt {a^2 - x^2} }\n      | r = \\int \\frac {\\d z} {2 z^{3/2} \\sqrt z \\sqrt {a^2 - z} }\n      | c = Integration by Substitution\n}}\n{{eqn | r = \\frac 1 2 \\int \\frac {\\d z} {z^2 \\sqrt {-z + a^2} }\n      | c = \n}}\n{{end-eqn}}\nUsing Primitive of $ \\dfrac 1{x^m \\sqrt{a x + b} }$:\n:$\\ds \\int \\frac {\\d x} {x^m \\sqrt {a x + b} } = -\\frac {\\sqrt{a x + b} } {\\paren {m - 1} b x^{m - 1} } - \\frac {\\paren {2 m - 3} a} {\\paren {2 m - 2} b} \\int \\frac {\\d x} {x^{m - 1} \\sqrt{a x + b} }$\nSetting:\n{{begin-eqn}}\n{{eqn | l = x\n      | o = :=\n      | r = z\n}}\n{{eqn | l = m\n      | o = :=\n      | r = 2\n}}\n{{eqn | l = a\n      | o = :=\n      | r = -1\n}}\n{{eqn | l = b\n      | o = :=\n      | r = a^2\n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l = \\frac 1 2 \\int \\frac {\\d z} {z^2 \\sqrt {-z + a^2} }\n      | r = \\frac {-\\sqrt {-z + a^2} } {2 \\paren {2 - 1} a^2 z^{2 - 1} } - \\frac {\\paren {2 \\paren 2 - 3} \\paren {-1} } {2 \\paren {2 \\paren 2 - 2} a^2} \\int \\frac {\\d z} {z^{2 - 1} \\sqrt {-z + a^2} } + C\n      | c = Primitive of $ \\dfrac 1{x^m \\sqrt{a x + b} }$\n}}\n{{eqn | r = \\frac {-\\sqrt {a^2 - z} } {2 a^2 z} - \\frac 1 {4 a^2} \\int \\frac {\\d z} {z \\sqrt{a^2 - z} } + C\n      | c = simplifying\n}}\n{{eqn | r = \\frac {-\\sqrt {a^2 - x^2} } {2 a^2 x^2} + \\frac 1 {4 a^2} \\paren {\\int \\frac {2 x \\rd x} {x^2 \\sqrt {a^2 - x^2} } } + C\n      | c = substituting for $z$\n}}\n{{eqn | r = \\frac {-\\sqrt {a^2 - x^2} } {2 a^2 x^2} + \\frac 1 {2 a^2} \\paren {\\int \\frac {\\d x} {x \\sqrt {a^2 - x^2} } } + C\n      | c = simplification\n}}\n{{eqn | r =\\frac {-\\sqrt {a^2 - x^2} } {2 a^2 x^2} + \\frac 1 {2 a^3} \\paren {-\\map \\ln {\\frac {a + \\sqrt {a^2 - x^2} } {\\size x} } } + C\n      | c = Primitive of $\\dfrac 1 {x \\sqrt {a^2 - x^2} }$\n}}\n{{eqn | r =\\frac {-\\sqrt {a^2 - x^2} } {2 a^2 x^2} - \\frac 1 {2 a^3} \\map \\ln {\\frac {a + \\sqrt {a^2 - x^2} } {\\size x} } + C\n      | c = simplifying\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "19405": {"score": 0.930683434009552, "content": {"text": "\\section{Primitive of Reciprocal of x squared by Root of a squared minus x squared}\nTags: Primitives involving Root of a squared minus x squared\n\n\\begin{theorem}\n:$\\ds \\int \\frac {\\d x} {x^2 \\sqrt {a^2 - x^2} } = \\frac {-\\sqrt {a^2 - x^2} } {a^2 x} + C$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n{{begin-eqn}}\n{{eqn | l = z\n      | r = x^2\n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac {\\d z} {\\d x}\n      | r = 2 x\n      | c = Power Rule for Derivatives\n}}\n{{eqn | ll= \\leadsto\n      | l = \\int \\frac {\\d x} {x^2 \\sqrt {a^2 - x^2} }\n      | r = \\int \\frac {\\d z} {2 z \\sqrt z \\sqrt {a^2 - z} }\n      | c = Integration by Substitution\n}}\n{{eqn | r = \\frac 1 2 \\int \\frac {\\d z} {z^{3/2} \\sqrt {a^2 - z} }\n      | c = \n}}\n{{end-eqn}}\nUsing Primitive of $ \\dfrac 1 {x^m \\sqrt {a x + b} }$:\n:$\\ds \\int \\frac {\\d x} {x^m \\sqrt {a x + b} } = -\\frac {\\sqrt {a x + b} } {\\paren {m - 1} b x^{m - 1} } - \\frac {\\paren {2 m - 3} a} {\\paren {2 m - 2} b} \\int \\frac {\\d x} {x^{m - 1} \\sqrt {a x + b} }$\nSetting:\n{{begin-eqn}}\n{{eqn | l = x\n      | o = :=\n      | r = -z\n}}\n{{eqn | l = m\n      | o = :=\n      | r = \\frac 3 2\n}}\n{{eqn | l = a\n      | o = :=\n      | r = 1\n}}\n{{eqn | l = b\n      | o = :=\n      | r = a^2\n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l = \\frac 1 2 \\int \\frac {\\d z} {\\paren {-z}^{3/2} \\sqrt {-z + a^2} }\n      | r = \\frac {-\\sqrt {-z + a^2} } {2 \\paren {\\paren {\\frac 3 2} - 1} a^2 \\paren {-z}^{\\paren {3/2} - 1} } - \\frac {2 \\paren {\\frac 3 2} - 3} {2 \\paren {2 \\paren {\\frac 3 2} - 2} a^2} \\int \\frac {\\d z} {\\paren {-z}^{\\paren {\\frac 3 2} - 1} \\sqrt {-z + a^2} } + C\n      | c = Primitive of $ \\dfrac 1 {x^m \\sqrt {a x + b} }$\n}}\n{{eqn | r = \\frac {-\\sqrt {a^2 - z} } {a^2 z^{1/2} } - 0 + C\n      | c = simplifying\n}}\n{{eqn | r = \\frac {-\\sqrt {a^2 - x^2} } {a^2 x} + C\n      | c = substituting back for $z$\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "19439": {"score": 0.9326663017272949, "content": {"text": "\\section{Primitive of Root of x squared minus a squared/Inverse Hyperbolic Cosine Form}\nTags: Primitive of Root of x squared minus a squared\n\n\\begin{theorem}\n:$\\ds \\int \\sqrt {x^2 - a^2} \\rd x = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\cosh^{-1} \\frac x a + C$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = a \\cosh \\theta\n}}\n{{eqn | n = 1\n      | ll= \\leadsto\n      | l = \\frac {\\d x} {\\d \\theta}\n      | r = a \\sinh \\theta\n      | c = Derivative of Hyperbolic Cosine\n}}\n{{end-eqn}}\nAlso:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = a \\cosh \\theta\n}}\n{{eqn | ll= \\leadsto\n      | l = x^2 - a^2\n      | r = a^2 \\cosh^2 \\theta - a^2\n      | c = \n}}\n{{eqn | r = a^2 \\paren {\\cosh^2 \\theta - 1}\n      | c = \n}}\n{{eqn | r = a^2 \\sinh^2 \\theta\n      | c = Difference of Squares of Hyperbolic Cosine and Sine\n}}\n{{eqn | n = 2\n      | ll= \\leadsto\n      | l = \\sqrt {x^2 - a^2}\n      | r = a \\sinh \\theta\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = a \\cosh \\theta\n}}\n{{eqn | n = 3\n      | ll= \\leadsto\n      | l = \\theta\n      | r = \\cosh^{-1} \\frac x a\n      | c = {{Defof|Inverse Hyperbolic Cosine/Real|index = 1|Real Inverse Hyperbolic Cosine}}\n}}\n{{end-eqn}}\nThus:\n{{begin-eqn}}\n{{eqn | l = \\int \\sqrt {x^2 - a^2} \\rd x\n      | r = \\int \\sqrt {x^2 - a^2} \\, a \\sinh \\theta \\rd \\theta\n      | c = Integration by Substitution from $(1)$\n}}\n{{eqn | r = \\int a^2 \\sinh^2 \\theta \\rd \\theta\n      | c = substituting for $\\sqrt {x^2 - a^2}$ from $(2)$\n}}\n{{eqn | r = a^2 \\int \\sinh^2 \\theta \\rd \\theta\n      | c = Primitive of Constant Multiple of Function\n}}\n{{eqn | r = a^2 \\frac {\\sinh \\theta \\cosh \\theta - \\theta} 2 + C\n      | c = Primitive of $\\sinh^2 \\theta$: Corollary\n}}\n{{eqn | r = \\frac 1 2 a \\sinh \\theta a \\cosh \\theta - \\frac {a^2 \\theta} 2 + C\n      | c = rearranging\n}}\n{{eqn | r = \\frac 1 2 x a \\sinh \\theta - \\frac {a^2 \\theta} 2 + C\n      | c = substituting $x = a \\cosh \\theta$\n}}\n{{eqn | r = \\frac 1 2 x \\sqrt {x^2 - a^2} - \\frac {a^2 \\theta} 2 + C\n      | c = substituting $\\sqrt {x^2 - a^2} = a \\sinh \\theta$ from $(2)$\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\cosh^{-1} \\frac x a + C\n      | c = substituting for $\\theta$ from $(3)$\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "19317": {"score": 0.9324754476547241, "content": {"text": "\\section{Primitive of Reciprocal of Root of a squared minus x squared/Arccosine Form}\nTags: Primitive of Reciprocal of Root of a squared minus x squared, Arccosine Function\n\n\\begin{theorem}\n:$\\ds \\int \\frac 1 {\\sqrt {a^2 - x^2} } \\rd x = -\\arccos \\frac x a + C$\nwhere $a$ is a strictly positive constant and $a^2 > x^2$.\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\int \\frac 1 {\\sqrt {a^2 - x^2} } \\rd x\n      | r = \\int \\frac {\\rd x} {\\sqrt {a^2 \\paren {1 - \\frac {x^2} {a^2} } } } \n      | c = factor $a^2$ out of the radicand\n}}\n{{eqn | r = \\int \\frac {\\rd x} {\\sqrt{a^2} \\sqrt {1 - \\paren {\\frac x a}^2} }\n      | c = \n}}\n{{eqn | r = \\frac 1 a \\int \\frac {\\rd x} {\\sqrt {1 - \\paren {\\frac x a}^2} }\n}}\n{{end-eqn}}\nSubstitute:\n:$\\cos \\theta = \\dfrac x a \\iff x = a \\cos \\theta$\nfor $\\theta \\in \\openint 0 \\pi$.\nFrom Real Cosine Function is Bounded and Shape of Cosine Function, this substitution is valid for all $x / a \\in \\openint {-1} 1$.\nBy hypothesis:\n{{begin-eqn}}\n{{eqn | l = a^2\n      | o = > \n      | m = x^2\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = 1\n      | o = >\n      | m = \\frac {x^2} {a^2}\n      | c = dividing both terms by $a^2$\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = 1\n      | o = >\n      | m = \\paren {\\frac x a}^2\n      | c = Powers of Group Elements\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = 1\n      | o = >\n      | m = \\size {\\frac x a}\n      | c = taking the square root of both terms\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = -1\n      | o = <\n      | m = \\frac x a\n      | mo= <\n      | r = 1\n      | c = Negative of Absolute Value\n}}\n{{end-eqn}}\nso this substitution will not change the domain of the integrand.\nThen:\n{{begin-eqn}}\n{{eqn | l = x \n      | r = a \\cos \\theta\n      | c = from above\n}}\n{{eqn | ll= \\leadsto\n      | l = 1\n      | r = -a \\sin \\theta \\frac {\\rd \\theta} {\\rd x}\n      | c = differentiating {{WRT|Differentiation}} $x$, Derivative of Cosine Function, Chain Rule for Derivatives\n}}\n{{eqn | l = \\frac 1 a \\int \\frac 1 {\\sqrt {1 - \\paren {\\frac x a}^2 } } \\rd x\n      | r = \\frac 1 a \\int \\frac {-a \\sin \\theta} {\\sqrt {1 - \\cos^2 \\theta} } \\frac {\\rd \\theta} {\\rd x} \\rd x\n      | c = from above\n}}\n{{eqn | r = -\\frac a a \\int \\frac {\\sin \\theta} {\\sqrt {1 - \\cos^2 \\theta} } \\rd \\theta\n      | c = Integration by Substitution\n}}\n{{eqn | r = -\\int \\frac {\\sin \\theta} {\\sqrt {\\sin^2 \\theta} } \\rd \\theta\n      | c = Sum of Squares of Sine and Cosine\n}}\n{{eqn | r = -\\int \\frac {\\sin \\theta} {\\size {\\sin \\theta} } \\rd \\theta\n      | c = \n}}\n{{end-eqn}}\nWe have defined $\\theta$ to be in the open interval $\\openint 0 \\pi$.\nFrom Sine and Cosine are Periodic on Reals, $\\sin \\theta > 0$ for the entire interval. Therefore the absolute value is unnecessary, and the integral simplifies to:\n{{begin-eqn}}\n{{eqn | l = -\\int \\rd \\theta\n      | r = -\\theta + C\n}}\n{{end-eqn}}\nAs $\\theta$ was stipulated to be in the open interval $\\openint 0 \\pi$:\n:$\\cos \\theta = \\dfrac x a \\iff \\theta = \\arccos \\dfrac x a$\nThe answer in terms of $x$, then, is:\n:$\\ds \\int \\frac 1 {\\sqrt {a^2 - x^2} } \\rd x = -\\arccos \\frac x a + C$\n{{qed}}\n\\end{proof}\n\n"}}, "15365": {"score": 0.9317294359207153, "content": {"text": "\\section{Definite Integral from 0 to Half Pi of Reciprocal of a plus b Cosine x}\nTags: Definite Integrals involving Cosine Function\n\n\\begin{theorem}\n:$\\ds \\int_0^{\\pi/2} \\frac 1 {a + b \\cos x} \\rd x = \\frac 1 {\\sqrt {a^2 - b^2} } \\map \\arccos {\\frac b a}$\nwhere $a$ and $b$ are real numbers with $a > b > 0$.\n\\end{theorem}\n\n\\begin{proof}\nSince $a > b > 0$, we have $a^2 > b^2$.\nSo:\n{{begin-eqn}}\n{{eqn\t| l = \\int_0^{\\pi/2} \\frac 1 {a + b \\cos x} \\rd x\n\t| r = \\intlimits {\\frac 2 {\\sqrt {a^2 - b^2} } \\map \\arctan {\\sqrt {\\frac {a - b} {a + b} } \\tan \\frac x 2} } 0 1\n\t| c = Primitive of $\\dfrac 1 {p + q \\cos x}$\n}}\n{{eqn\t| r = \\frac 1 {\\sqrt {a^2 - b^2} } \\paren {2 \\map \\arctan {\\sqrt {\\frac {a - b} {a + b} } } }\n}}\n{{eqn\t| r = \\frac 1 {\\sqrt {a^2 - b^2} } \\paren {2 \\map \\arctan {\\sqrt {\\frac {1 - \\frac b a} {1 + \\frac b a} } } }\n}}\n{{eqn\t| r = \\frac 1 {\\sqrt {a^2 - b^2} } \\map \\arccos {\\frac b a}\n\t| c = Arccosine in terms of Arctangent\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "19440": {"score": 0.9357802867889404, "content": {"text": "\\section{Primitive of Root of x squared minus a squared/Logarithm Form}\nTags: Primitive of Root of x squared minus a squared\n\n\\begin{theorem}\n:$\\ds \\int \\sqrt {x^2 - a^2} \\rd x = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\ln \\size {x + \\sqrt {x^2 - a^2} } + C$\nfor $\\size x \\ge a$.\n\\end{theorem}\n\n\\begin{proof}\nWe have that $\\sqrt {x^2 - a^2}$ is defined only when $x^2 \\ge a^2$, that is, either:\n:$x \\ge a$\nor:\n:$x \\le -a$\nwhere it is assumed that $a > 0$.\nFirst let $x \\ge a$.\n{{begin-eqn}}\n{{eqn | l = x\n      | r = a \\cosh u\n}}\n{{eqn | n = 1\n      | ll= \\leadsto\n      | l = \\frac {\\d x} {\\d u}\n      | r = a \\sinh u\n      | c = Derivative of Hyperbolic Cosine\n}}\n{{end-eqn}}\nAlso:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = a \\cosh u\n}}\n{{eqn | ll= \\leadsto\n      | l = x^2 - a^2\n      | r = a^2 \\cosh^2 u - a^2\n      | c = \n}}\n{{eqn | r = a^2 \\paren {\\cosh^2 u + 1}\n      | c = \n}}\n{{eqn | r = a^2 \\sinh^2 u\n      | c = Difference of Squares of Hyperbolic Cosine and Sine\n}}\n{{eqn | n = 2\n      | ll= \\leadsto\n      | l = \\sqrt {x^2 - a^2}\n      | r = a \\sinh u\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = a \\cosh u\n}}\n{{eqn | n = 3\n      | ll= \\leadsto\n      | l = u\n      | r = \\arcosh \\frac x a\n      | c = {{Defof|Real Area Hyperbolic Cosine}}\n}}\n{{end-eqn}}\nThus:\n{{begin-eqn}}\n{{eqn | l = \\int \\sqrt {x^2 - a^2} \\rd x\n      | r = \\int \\sqrt {x^2 - a^2} \\, a \\sinh u \\rd u\n      | c = Integration by Substitution from $(1)$\n}}\n{{eqn | r = \\int a^2 \\sinh^2 u \\rd u\n      | c = substituting for $\\sqrt {x^2 - a^2}$ from $(2)$\n}}\n{{eqn | r = a^2 \\int \\sinh^2 u \\rd u\n      | c = Primitive of Constant Multiple of Function\n}}\n{{eqn | r = a^2 \\frac {\\sinh u \\cosh u - u} 2 + C\n      | c = Primitive of $\\sinh^2 u$: Corollary\n}}\n{{eqn | r = \\frac 1 2 a \\sinh u a \\cosh u - \\frac {a^2 u} 2 + C\n      | c = rearranging\n}}\n{{eqn | r = \\frac 1 2 x a \\sinh u - \\frac {a^2 u} 2 + C\n      | c = substituting $x = a \\cosh u$\n}}\n{{eqn | r = \\frac 1 2 x \\sqrt {x^2 - a^2} - \\frac {a^2 u} 2 + C\n      | c = substituting $\\sqrt {x^2 - a^2} = a \\sinh u$ from $(2)$\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\cosh^{-1} \\frac x a + C\n      | c = substituting for $\\theta$ from $(3)$\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\paren {\\map \\ln {x + \\sqrt {x^2 - a^2} } - \\ln a} + C\n      | c = $\\arcosh \\dfrac x a$ in Logarithm Form\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\map \\ln {x + \\sqrt {x^2 - a^2} } + \\frac {a^2} 2 \\ln a + C\n      | c = simplifying\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\map \\ln {x + \\sqrt {x^2 - a^2} } + C\n      | c = subsuming $\\dfrac {a^2} 2 \\ln a$ into arbitrary constant\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\ln \\size {x + \\sqrt {x^2 - a^2} } + C\n      | c = {{Defof |Absolute Value}}\n}}\n{{end-eqn}}\nNow suppose $x \\le -a$.\nLet $z = -x$.\nThen:\n:$\\d x = -\\d z$\nand we then have:\n{{begin-eqn}}\n{{eqn | l = \\int \\sqrt {x^2 - a^2} \\rd x\n      | r = -\\int \\sqrt {\\paren {-z}^2 - a^2} \\rd z\n      | c = Integration by Substitution\n}}\n{{eqn | r = -\\int \\sqrt {\\paren z^2 - a^2} \\rd z\n      | c = simplifying\n}}\n{{eqn | r = -\\frac {z \\sqrt {z^2 - a^2} } 2 + \\frac {a^2} 2 \\map \\ln {z + \\sqrt {z^2 - a^2} } + C\n      | c = from above\n}}\n{{eqn | r = -\\frac {z \\sqrt {z^2 - a^2} } 2 - \\frac {a^2} 2 \\paren {\\map \\ln {z - \\sqrt {z^2 - a^2} } - \\map \\ln {a^2} } + C\n      | c = Negative of $\\map \\ln {z + \\sqrt {z^2 - a^2} }$\n}}\n{{eqn | r = -\\frac {z \\sqrt {z^2 - a^2} } 2 - \\frac {a^2} 2 \\map \\ln {z - \\sqrt {z^2 - a^2} } + C\n      | c = subsuming $-\\dfrac {a^2 \\map \\ln {a^2} } 2$ into constant\n}}\n{{eqn | r = -\\frac {\\paren {-x} \\sqrt {\\paren {-x}^2 - a^2} } 2 - \\frac {a^2} 2 \\map \\ln {\\paren {-x} - \\sqrt {\\paren {-x}^2 - a^2} } + C\n      | c = substituting back for $x$\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\map \\ln {-x - \\sqrt {x^2 - a^2} } + C\n      | c = simplifying\n}}\n{{eqn | r = \\frac {x \\sqrt {x^2 - a^2} } 2 - \\frac {a^2} 2 \\ln \\size {x + \\sqrt {x^2 - a^2} } + C\n      | c = as $-x - \\sqrt {x^2 - a^2} > 0$: {{Defof |Absolute Value}}\n}}\n{{end-eqn}}\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "19416": {"score": 0.93370121717453, "content": {"text": "\\section{Primitive of Reciprocal of x squared minus a squared/Logarithm Form 1/size of x less than a}\nTags: Primitive of Reciprocal of x squared minus a squared\n\n\\begin{theorem}\nLet $a \\in \\R_{>0}$ be a strictly positive real constant.\nLet $\\size x < a$.\nThen:\n:$\\ds \\int \\frac {\\d x} {x^2 - a^2} = \\dfrac 1 {2 a} \\map \\ln {\\dfrac {a - x} {a + x} } + C$\n\\end{theorem}\n\n\\begin{proof}\nLet $\\size x < a$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\int \\frac {\\d x} {x^2 - a^2}\n      | r = -\\frac 1 a \\artanh {\\frac x a} + C\n      | c = Primitive of $\\dfrac 1 {x^2 - a^2}$: $\\artanh$ form\n}}\n{{eqn | r = -\\frac 1 a \\paren {\\dfrac 1 2 \\map \\ln {\\dfrac {a + x} {a - x} } } + C\n      | c = $\\artanh \\dfrac x a$ in Logarithm Form\n}}\n{{eqn | r = -\\dfrac 1 {2 a} \\map \\ln {\\dfrac {a + x} {a - x} } + C\n      | c = simplifying\n}}\n{{eqn | r = \\dfrac 1 {2 a} \\map \\ln {\\dfrac {a - x} {a + x} } + C\n      | c = Logarithm of Reciprocal\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Primitive of Reciprocal of x squared minus a squared\n\\end{proof}\n\n"}}, "19387": {"score": 0.9387462139129639, "content": {"text": "\\section{Primitive of Reciprocal of x by Root of a x squared plus b x plus c}\nTags: Primitives involving Root of a x squared plus b x plus c, Primitive of Reciprocal of x by Root of a x squared plus b x plus c\n\n\\begin{theorem}\nLet $a, b, c \\in \\R_{\\ne 0}$.\nThen for $x \\in \\R$ such that $a x^2 + b x + c > 0$ and $x \\ne 0$:\n:$\\ds \\int \\frac {\\d x} {x \\sqrt {a x^2 + b x + c} } = \\begin {cases}\n\\dfrac {-1} {\\sqrt c} \\dfrac {\\size x} x \\ln \\size {\\dfrac {2 \\sqrt c \\sqrt {a x^2 + b x + c} + b x + 2 c} x} + C & : c > 0, b^2 - 4 a c > 0 \\\\\n\\dfrac {-1} {\\sqrt c} \\map {\\sinh^{-1} } {\\dfrac {b x + 2 c} {\\size x \\sqrt {4 a c - b^2} } } + C & : c > 0, b^2 - 4 a c < 0 \\\\\n\\dfrac {-1} {\\sqrt c} \\dfrac {\\size x} x \\ln \\size {\\dfrac {2 c} x + b} + C & : c > 0, b^2 - 4 a c = 0 \\\\\n\\dfrac 1 {\\sqrt {-c} } \\map \\arcsin {\\dfrac {b x + 2 c} {\\size x \\sqrt {\\size {b^2 - 4 a c} } } } & : c < 0, b^2 - 4 a c \\ne 0  \\\\\n\\end {cases}$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = x \\sqrt {a x^2 + b x + c}\n      | r = \\frac x {\\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} }\n      | c = \n}}\n{{eqn | r = \\frac{x \\left({2 \\sqrt c \\sqrt{a x^2 + b x + c} + b x + 2 c}\\right)} {\\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} \\left({2 \\sqrt c \\sqrt{a x^2 + b x + c} + b x + 2 c}\\right)}\n      | c = \n}}\n{{eqn | r = \\frac{x \\left({2 \\sqrt c \\sqrt{a x^2 + b x + c} + b x + 2 c}\\right)} {2 \\sqrt c + \\left({b x + 2 c}\\right) \\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} }\n      | c = \n}}\n{{eqn | r = \\frac N D\n      | c = \n}}\n{{eqn-intertext|where:}}\n{{eqn | l = N\n      | r = \\frac {2 \\sqrt c \\sqrt{a x^2 + b x + c} + b x + 2 c} x\n      | c = \n}}\n{{eqn | l = D\n      | r = \\frac {2 \\sqrt c + \\left({b x + 2 c}\\right) \\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} } {x^2}\n      | c = \n}}\n{{end-eqn}}\nThen:\n{{begin-eqn}}\n{{eqn | l = \\frac {\\mathrm d N} {\\mathrm d x}\n      | r = \\frac {x \\left({\\sqrt c \\left({2 a x + b}\\right) \\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} + b}\\right) - \\left({2 \\sqrt c \\sqrt{a x^2 + b x + c} + b x + 2 c}\\right)} {x^2}\n      | c = \n}}\n{{eqn | r = \\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} \\frac{\\left({\\sqrt c \\left({2 a x^2 + b x}\\right) + b x \\sqrt{a x^2 + b x + c} }\\right) - \\left({2 \\sqrt c \\left({a x^2 + b x + c}\\right) + \\left({b x + 2 c}\\right) \\sqrt{a x^2 + b x + c} }\\right)} {x^2}\n      | c = \n}}\n{{eqn | r = \\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} \\frac{\\left({\\sqrt c \\left({2 a x^2 + b x - 2 a x^2 - 2 b x - 2 c}\\right) + b x \\sqrt{a x^2 + b x + c} }\\right) - \\left({\\left({b x + 2 c}\\right) \\sqrt{a x^2 + b x + c} }\\right)} {x^2}\n      | c = \n}}\n{{eqn | r = \\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} \\frac{\\left({\\sqrt c \\left({-b x - 2 c}\\right) - \\left({\\left({2 c}\\right) \\sqrt{a x^2 + b x + c} }\\right)}\\right)} {x^2}\n      | c = \n}}\n{{eqn | r = -\\frac {2 c + \\sqrt c \\left({b x + 2 c}\\right) \\left({a x^2 + b x + c}\\right)^{-\\frac 1 2} } {x^2}\n      | c = \n}}\n{{eqn | r = \\sqrt c \\times D\n      | c = \n}}\n{{end-eqn}}\nNow provided $N$ is real and non zero:\n{{begin-eqn}}\n{{eqn | l = \\int \\frac {\\mathrm d x} {x \\sqrt {a x^2 + b x + c} }\n      | r = \\int \\frac D N \\ \\mathrm d x\n      | c = \n}}\n{{eqn | r = \\frac {-1} {\\sqrt c} \\int \\frac {\\frac {\\mathrm d N} {\\mathrm d x} } N \\ \\mathrm d x\n      | c = \n}}\n{{eqn | r = \\frac {-1} {\\sqrt c} \\ln \\left\\vert{N}\\right\\vert\n      | c = Primitive of Function under its Derivative\n}}\n{{eqn | r = \\frac {-1}{\\sqrt c} \\ln \\left\\vert\\frac{\\left({2 \\sqrt c \\sqrt{a x^2 + b x + c} + b x + 2 c}\\right)} x \\right\\vert\n      | c = \n}}\n{{end-eqn}}\n{{WIP|Still to be tidied, still have the $\\sinh^{-1}$ case to do}}\n{{Proofread}}\n\\end{proof}\n\n"}}, "19600": {"score": 0.9344794154167175, "content": {"text": "\\section{Primitive of x squared over a squared minus x squared/Inverse Hyperbolic Tangent Form}\nTags: Primitive of x squared over a squared minus x squared\n\n\\begin{theorem}\n:$\\ds \\int \\frac {x^2 \\rd x} {a^2 - x^2} = -x + a \\tanh^{-1} \\frac x a + C$\nfor $x^2 < a^2$.\n\\end{theorem}\n\n\\begin{proof}\nLet:\n{{begin-eqn}}\n{{eqn | l = \\int \\frac {x^2 \\rd x} {a^2 - x^2}\n      | r = \\int \\frac {x^2 - a^2 + a^2} {a^2 - x^2} \\rd x\n      | c = \n}}\n{{eqn | r = \\int \\frac {-\\paren {a^2 - x^2} } {a^2 - x^2} \\rd x + \\int \\frac {a^2} {a^2 - x^2} \\rd x\n      | c = Linear Combination of Integrals\n}}\n{{eqn | r = - \\int \\rd x + a^2 \\int \\frac {\\rd x} {a^2 - x^2}\n      | c = Linear Combination of Integrals\n}}\n{{eqn | r = -x + a^2 \\int \\frac {\\rd x} {a^2 - x^2} + C\n      | c = Primitive of Constant\n}}\n{{eqn | r = -x + a^2 \\paren {\\frac 1 a \\tanh^{-1} \\frac x a} + C\n      | c = Primitive of Reciprocal of $a^2 - x^2$: $\\tanh^{-1}$ form\n}}\n{{eqn | r = -x + a \\tanh^{-1} \\frac x a + C\n      | c = simplifying\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Primitive of x squared over a squared minus x squared\n\\end{proof}\n\n"}}, "19389": {"score": 0.9352589845657349, "content": {"text": "\\section{Primitive of Reciprocal of x by Root of x squared minus a squared/Arcsine Form}\nTags: Arcsine Function, Primitive of Reciprocal of x by Root of x squared minus a squared\n\n\\begin{theorem}\n:$\\ds \\int \\frac {\\d x} {x \\sqrt {x^2 - a^2} } = -\\frac 1 a \\arcsin \\size {\\frac a x} + C$\nfor $0 < a < \\size x$.\n\\end{theorem}\n\n\\begin{proof}\nWe have that $\\sqrt {x^2 - a^2}$ is defined only when $x^2 > a^2$, that is, either:\n:$x > a$\nor:\n:$x < -a$\nwhere it is assumed that $a > 0$.\nHence:\n{{begin-eqn}}\n{{eqn | l = \\int \\frac {\\d x} {x \\sqrt {x^2 - a^2} }\n      | r = \\frac 1 a \\arcsec \\size {\\frac x a} + C\n      | c = Primitive of $\\dfrac 1 {x \\sqrt {x^2 - a^2} }$: Arcsecant Form\n}}\n{{eqn | r = \\frac 1 a \\arccos \\size {\\frac a x} + C\n      | c = Arcsecant of Reciprocal equals Arccosine\n}}\n{{eqn | r = \\dfrac \\pi 2 - \\frac 1 a \\arcsin \\size {\\frac a x} + C\n      | c = Sum of Arcsine and Arccosine\n}}\n{{eqn | r = -\\frac 1 a \\arcsin \\size {\\frac a x} + C\n      | c = subsuming $\\dfrac \\pi 2$ into arbitrary constant $C$\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_elainewan/math_calculus_2_11.json": {"gold": {"12012": 1, "18841": 1, "12010": 1, "20761": 1, "12011": 1}, "retrieved": {"18841": {"score": 0.8624183535575867, "content": {"text": "\\section{Positive Real has Real Square Root}\nTags: Real Numbers\n\n\\begin{theorem}\nLet $x \\in \\R_{>0}$ be a (strictly) positive real number.\nThen:\n:$\\exists y \\in \\R: x = y^2$\n\\end{theorem}\n\n\\begin{proof}\nLet $f: \\R \\to \\R$ be defined as:\n:$\\forall x \\in \\R: \\map f x = x^2$\nWe have that $f$ is the pointwise product of the identity mapping with itself.\nBy Product Rule for Continuous Real Functions and Identity Mapping is Continuous, $f$ is continuous.\nBy Power Function is Unbounded Above:\n:$\\exists q \\in \\R: \\map f q > x$\nThen:\n:$0^2 = 0 \\le x$\nBy the Intermediate Value Theorem:\n:$\\exists y \\in \\R: 0 < y < q: y^2 = x$\n{{qed}}\nCategory:Real Numbers\n\\end{proof}\n\n"}}, "21299": {"score": 0.8632484674453735, "content": {"text": "\\section{Shape of Tangent Function}\nTags: Tangent Function, Analysis\n\n\\begin{theorem}\nThe nature of the tangent function on the set of real numbers $\\R$ is as follows:\n:$\\tan x$ is continuous and strictly increasing on the interval $\\openint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$\n:$\\tan x \\to +\\infty$ as $x \\to \\dfrac \\pi 2 ^-$\n:$\\tan x \\to -\\infty$ as $x \\to -\\dfrac \\pi 2 ^+$\n:$\\tan x$ is not defined on $\\forall n \\in \\Z: x = \\paren {n + \\dfrac 1 2} \\pi$, at which points it is discontinuous\n:$\\forall n \\in \\Z: \\tan \\left({n \\pi}\\right) = 0$.\n\\end{theorem}\n\n\\begin{proof}\n$\\tan x$ is continuous and strictly increasing on $\\openint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$:\nContinuity follows from the Quotient Rule for Continuous Real Functions:\n:$(1): \\quad$ Both $\\sin x$ and $\\cos x$ are continuous on $\\openint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$ from Real Sine Function is Continuous and Cosine Function is Continuous\n:$(2): \\quad$ $\\cos x > 0$ on this interval.\nThe fact of $\\tan x$ being strictly increasing on this interval has been demonstrated in the discussion on Tangent Function is Periodic on Reals.\n$\\tan x \\to + \\infty$ as $x \\to \\dfrac \\pi 2 ^-$:\nFrom Sine and Cosine are Periodic on Reals, we have that both $\\sin x > 0$ and $\\cos x > 0$ on $\\openint 0 {\\dfrac \\pi 2}$.\nWe have that:\n:$(1): \\quad \\cos x \\to 0$ as $x \\to \\dfrac \\pi 2^-$\n:$(2): \\quad \\sin x \\to 1$ as $x \\to \\dfrac \\pi 2^-$\nFrom the Infinite Limit Theorem it follows that:\n:$\\tan x = \\dfrac {\\sin x} {\\cos x} \\to + \\infty$ as $x \\to \\dfrac \\pi 2 ^-$\n$\\tan x \\to - \\infty$ as $x \\to -\\dfrac \\pi 2 ^+$:\nFrom Sine and Cosine are Periodic on Reals, we have that $\\sin x < 0$ and $\\cos x > 0$ on $\\openint {-\\dfrac \\pi 2} 0$.\nWe have that:\n:$(1): \\quad \\cos x \\to 0$ as $x \\to -\\dfrac \\pi 2 ^+$\n:$(2): \\quad \\sin x \\to -1$ as $x \\to -\\dfrac \\pi 2 ^+$\nThus it follows that $\\tan x = \\dfrac {\\sin x} {\\cos x} \\to -\\infty$ as $x \\to -\\dfrac \\pi 2 ^+$.\n$\\tan x$ is not defined and discontinuous at $x = \\paren {n + \\dfrac 1 2} \\pi$:\nFrom the discussion of Sine and Cosine are Periodic on Reals, it was established that:\n:$\\forall n \\in \\Z: x = \\paren {n + \\dfrac 1 2} \\pi \\implies \\cos x = 0$\nAs division by zero is not defined, it follows that at these points $\\tan x$ is not defined either.\nNow, from the above, we have:\n:$(1): \\quad \\tan x \\to + \\infty$ as $x \\to \\dfrac \\pi 2^-$\n:$(2): \\quad \\tan x \\to - \\infty$ as $x \\to -\\dfrac \\pi 2^+$\nAs $\\map \\tan {x + \\pi} = \\tan x$ from Tangent Function is Periodic on Reals, it follows that:\n:$\\tan x \\to - \\infty$ as $x \\to \\dfrac \\pi 2 ^+$\nHence the left hand limit and right hand limit at $x = \\dfrac \\pi 2$ are not the same.\nFrom Tangent Function is Periodic on Reals, it follows that the same applies $\\forall n \\in \\Z: x = \\paren {n + \\dfrac 1 2} \\pi$.\nThe fact of its discontinuity at these points follows from the definition of discontinuity.\n$\\tan \\left({n \\pi}\\right) = 0$:\nFollows directly from Sine and Cosine are Periodic on Reals::\n:$\\forall n \\in \\Z: \\map \\sin {n \\pi} = 0$\n{{qed}}\n\\end{proof}\n\n"}}, "12954": {"score": 0.8657200932502747, "content": {"text": "\\section{Graph of Real Function in Cartesian Plane intersects Vertical at One Point}\nTags: Real Functions, Graphs of Mappings\n\n\\begin{theorem}\nLet $f: \\R \\to \\R$ be a real function.\nLet its graph be embedded in the Cartesian plane $\\CC$:\n:520px\nEvery vertical line through a point $a$ in the domain of $f$ intersects the graph of $f$ at exactly one point $P = \\tuple {a, \\map f a}$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Equation of Vertical Line, a vertical line in $\\CC$ through the point $\\tuple {a, 0}$ on the $x$-axis has an equation $x = a$.\nA real function is by definition a mapping.\nHence:\n:$\\forall a_1, a_2 \\in \\Dom f: a_1 = a_2 \\implies \\map f {a_1} = \\map f {a_2}$\nwhere $\\Dom f$ denotes the domain of $f$.\nThus for each $a \\in \\Dom f$ there exists exactly one ordered pair $\\tuple {a, y}$ such that $y = \\map f a$.\nThat is, there is exactly one point on $x = a$ which is also on the graph of $f$.\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}, "16906": {"score": 0.8653683066368103, "content": {"text": "\\section{Cauchy Mean Value Theorem}\nTags: Differential Calculus, Named Theorems\n\n\\begin{theorem}\nLet $f$ and $g$ be real functions which are continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nSuppose:\n:$\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$\nThen:\n:$\\exists \\xi \\in \\openint a b: \\dfrac {\\map {f'} \\xi} {\\map {g'} \\xi} = \\dfrac {\\map f b - \\map f a} {\\map g b - \\map g a}$\n\\end{theorem}\n\n\\begin{proof}\nFirst we check $\\map g a \\ne \\map g b$.\n{{AimForCont}} $\\map g a = \\map g b$.\nFrom Rolle's Theorem:\n:$\\exists \\xi \\in \\openint a b: \\map {g'} \\xi = 0$.\nThis contradicts $\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$.\nThus by Proof by Contradiction $\\map g a \\ne \\map g b$.\nLet $h = \\dfrac {\\map f b - \\map f a} {\\map g b - \\map g a}$.\nLet $F$ be the real function defined on $\\closedint a b$ by:\n:$\\map F x = \\map f x - h \\map g x$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map F b - \\map F a\n      | r = \\paren {\\map f b - h \\map g b} - \\paren {\\map f a - h \\map g a}\n      | c = as $\\map F x = \\map f x - h \\map g x$\n}}\n{{eqn | r = \\paren {\\map f b - \\map f a} - h \\paren {\\map g b - \\map g a}\n}}\n{{eqn | r = 0\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map F a\n      | r = \\map F b\n}}\n{{eqn | ll= \\leadsto\n      | q = \\exists \\xi \\in \\openint a b\n      | l = \\map {F'} \\xi\n      | r = \\map {f'} \\xi - h \\map {g'} \\xi\n      | c = Sum Rule for Derivatives, Derivative of Constant Multiple\n}}\n{{eqn | r = 0\n      | c = Rolle's Theorem\n}}\n{{eqn | ll= \\leadsto\n      | q = \\exists \\xi \\in \\openint a b\n      | l = \\frac {\\map {f'} \\xi} {\\map {g'} \\xi}\n      | r = h\n      | c = $\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$\n}}\n{{eqn | r = \\frac {\\map f b - \\map f a} {\\map g b - \\map g a}\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "17400": {"score": 0.8650914430618286, "content": {"text": "\\section{At Most Two Horizontal Asymptotes}\nTags: Limits of Real Functions, Limits of Functions, Analytic Geometry\n\n\\begin{theorem}\nThe graph of a real function has at most two horizontal asymptotes.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from the definition of a horizontal asymptote.\n{{qed}}\n\\end{proof}\n\n"}}, "20207": {"score": 0.8904861807823181, "content": {"text": "\\section{Rational Points on Graph of Logarithm Function}\nTags: Logarithms\n\n\\begin{theorem}\nConsider the graph of the logarithm function in the real Cartesian plane $\\R^2$:\n:$f := \\set {\\tuple {x, y} \\in \\R^2: y = \\ln x}$\nThe only rational point of $f$ is $\\tuple {1, 0}$.\n\\end{theorem}\n\n\\begin{proof}\nConsider the graph of the exponential function in the real Cartesian plane $\\R^2$:\n:$g := \\set {\\tuple {x, y} \\in \\R^2: y = e^x}$\nFrom Rational Points on Graph of Exponential Function, the only rational point of $g$ is $\\tuple {0, 1}$.\nBy definition of the exponential function, $f$ and $g$ are inverses.\nThus:\n:$\\tuple {x, y} \\in g \\iff \\tuple {y, x} \\in f$\nThus for $\\tuple {x, y} \\in g$ to be rational, $\\tuple {y, x} = \\tuple {0, 1}$.\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "8096": {"score": 0.87156742811203, "content": {"text": "\\begin{definition}[Definition:Root of Equation]\nLet $\\map E x$ be a mathematical expression representing an equation which is dependent upon a variable $x$.\nA '''root''' of $\\map E x$ is a constant which, when substituted for $x$ in $\\map E x$, makes $\\map E x$ a true statement.\n\\end{definition}"}}, "20761": {"score": 0.9172508120536804, "content": {"text": "\\section{Root of Equation e^x (x - 1) = e^-x (x + 1)}\nTags: Analysis\n\n\\begin{theorem}\nThe equation:\n:$e^x \\paren {x - 1} = e^{-x} \\paren {x + 1}$\nhas a root:\n:$x = 1 \\cdotp 19966 \\, 78640 \\, 25773 \\, 4 \\ldots$\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map f x = e^x \\paren {x - 1} - e^{-x} \\paren {x + 1}$.\nThen if $\\map f c = 0$, $c$ is a root of $e^x \\paren {x - 1} = e^{-x} \\paren {x + 1}$.\nNotice that:\n:$\\map f 1 = e^1 \\paren {1 - 1} - e^{-1} \\paren {1 + 1} = -\\dfrac 2 e < 0$\n:$\\map f 2 = e^2 \\paren {2 - 1} - e^{-2} \\paren {2 + 1} = e^2 - \\dfrac 3 {e^2} > 0$\nBy Intermediate Value Theorem:\n:$\\exists c \\in \\openint 1 2: \\map f c = 0$.\nThis shows that our equation has a root between $1$ and $2$.\nThe exact value of this root can be found using any numerical method, e.g. Newton's Method.\n{{ProofWanted|Analysis needed of the Kepler Equation}}\n\\end{proof}\n\n"}}, "12011": {"score": 0.8763493895530701, "content": {"text": "\\section{Intermediate Value Theorem/Corollary}\nTags: Named Theorems, Analysis\n\n\\begin{theorem}\nLet $I$ be a real interval.\nLet $a, b \\in I$ such that $\\openint a b$ is an open interval.\nLet $f: I \\to \\R$ be a real function which is continuous on $\\openint a b$.\nLet $0 \\in \\R$ lie between $\\map f a$ and $\\map f b$.\nThat is, either:\n:$\\map f a < 0 < \\map f b$\nor:\n:$\\map f b < 0 < \\map f a$\nThen $f$ has a root in $\\openint a b$.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from the Intermediate Value Theorem and from the definition of root.\n{{qed}}\n\\end{proof}\n\n"}}, "13805": {"score": 0.8822263479232788, "content": {"text": "\\section{Exponential of Real Number is Strictly Positive/Proof 5/Lemma}\nTags:  Exponential Function, Exponential of Real Number is Strictly Positive\n\n\\begin{theorem}\nLet $x$ be a real number.\nLet $\\exp$ denote the (real) Exponential Function.\nThen:\n:$\\forall x \\in \\R: \\exp x \\ne 0$\n\\end{theorem}\n\n\\begin{proof}\nThis proof assumes the definition of $\\exp$ as the solution to an initial value problem.\nThat is, suppose $\\exp$ satisfies:\n:$(1): \\quad D_x \\exp x = \\exp x$\n:$(2): \\quad \\map \\exp 0 = 1$\non $\\R$.\n{{AimForCont}} $\\exists \\alpha \\in \\R: \\exp \\alpha = 0$.\nSuppose that $\\alpha > 0$.\nLet $J = \\closedint 0 \\alpha$.\nFrom Exponential Function is Continuous, $\\exp$ is continuous on $J$.\nFrom Max and Min of Function on Closed Real Interval:\n:$\\exists K \\in \\R: \\forall x \\in J: \\size {\\exp x} < K$\nThen, $\\forall n \\in \\N : \\exists c_n \\in J$ such that:\n{{begin-eqn}}\n{{eqn | l = 1\n      | r = \\exp 0\n      | c = by $(2)$\n}}\n{{eqn | r = \\sum_{j \\mathop = 0}^{n - 1} \\frac {\\map {\\exp^j} \\alpha} {j!} \\paren {-\\alpha}^j + \\frac {\\exp c_n} {n!} \\paren {-\\alpha}^n\n      | c = Taylor's Theorem for Univariate Functions\n}}\n{{eqn | r = \\sum_{j \\mathop = 0}^{n - 1} \\frac {\\map \\exp \\alpha} {j!} \\paren {-\\alpha}^j + \\frac {\\exp c_n} {n!} \\paren {-\\alpha}^n\n      | c = by $(1)$\n}}\n{{eqn | r = \\sum_{j \\mathop = 0}^{n - 1} \\frac 0 {j!} \\paren {-\\alpha}^j + \\frac {\\exp c_n} {n!} \\paren {-\\alpha}^n\n      | c = from our assumption aiming for a contradiction\n}}\n{{eqn | r = \\frac {\\exp c_n} {n!} \\paren {-\\alpha}^n\n}}\n{{end-eqn}}\nSo:\n:$\\forall n \\in \\N: 1 \\le K \\dfrac {\\alpha^n} {n!}$\nThat is, dividing both sides by $K$:\n:$\\forall n \\in \\N: \\dfrac 1 K \\le \\dfrac {\\alpha^n} {n!}$\nBut from Power over Factorial, $\\dfrac {\\alpha^n} {n!} \\to 0$.\nThis contradicts our assumption.\nThe same argument, mutatis mutandis proves the result for $\\alpha < 0$.\nBy hypothesis $(2)$:\n{{begin-eqn}}\n{{eqn | l = \\alpha\n      | r = 0\n}}\n{{eqn | ll= \\leadsto\n      | l = \\exp \\alpha \n      | r = 1\n}}\n{{eqn | o = \\ne\n      | r = 0\n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\nCategory:Exponential of Real Number is Strictly Positive\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/t_test3.json": {"gold": {"9320": 1, "4138": 1}, "retrieved": {"4137": {"score": 0.7278648614883423, "content": {"text": "\\begin{definition}[Definition:Hypothesis (Inductive Statistics)]\n;Context: Inductive statistics\nA '''hypothesis''' is a statement about a population parameter.\n\\end{definition}"}}, "20820": {"score": 0.7281254529953003, "content": {"text": "\\section{Sample Mean is Unbiased Estimator of Population Mean}\nTags: Inductive Statistics, Descriptive Statistics\n\n\\begin{theorem}\nLet $X_1, X_2, \\ldots, X_n$ form a random sample from a population with mean $\\mu$ and variance $\\sigma^2$.\nThen: \n:$\\ds \\bar X = \\frac 1 n \\sum_{i \\mathop = 1}^n X_i$\nis an unbiased estimator of $\\mu$.\n\\end{theorem}\n\n\\begin{proof}\nIf $\\bar X$ is an unbiased estimator of $\\mu$, then: \n:$\\ds \\expect {\\bar X} = \\mu$\nWe have: \n{{begin-eqn}}\n{{eqn\t| l = \\expect {\\bar X}\n\t| r = \\expect {\\frac 1 n \\sum_{i \\mathop = 1}^n X_i}\n}}\n{{eqn\t| r = \\frac 1 n \\sum_{i \\mathop = 1}^n \\expect {X_i}\n\t| c = Linearity of Expectation Function\n}}\n{{eqn\t| r = \\frac 1 n \\sum_{i \\mathop = 1}^n \\mu\n\t| c = as $\\expect {X_i} = \\mu$\n}}\n{{eqn\t| r = \\frac n n \\mu\n\t| c = as $\\ds \\sum_{i \\mathop = 1}^n 1 = n$\n}}\n{{eqn\t| r = \\mu\n}}\n{{end-eqn}}\nSo $\\bar X$ is an unbiased estimator of $\\mu$. \n{{qed}}\nCategory:Inductive Statistics\n\\end{proof}\n\n"}}, "23292": {"score": 0.7346067428588867, "content": {"text": "\\section{Variance of Sample Mean}\nTags: Inductive Statistics, Variance\n\n\\begin{theorem}\nLet $X_1, X_2, \\ldots, X_n$ form a random sample from a population with mean $\\mu$ and variance $\\sigma^2$.\nLet: \n:$\\ds \\overline X = \\frac 1 n \\sum_{i \\mathop = 1}^n X_i$\nThen: \n:$\\var {\\overline X} = \\dfrac {\\sigma^2} n$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn\t| l = \\var {\\overline X}\n\t| r = \\var {\\frac 1 n \\sum_{i \\mathop = 1}^n X_i}\n}}\n{{eqn\t| r = \\frac 1 {n^2} \\sum_{i \\mathop = 1}^n \\var {X_i}\n\t| c = repeated application of Variance of Linear Combination of Random Variables: Corollary\n}}\n{{eqn\t| r = \\frac 1 {n^2} \\sum_{i \\mathop = 1}^n \\sigma^2\n}}\n{{eqn\t| r = \\frac {\\sigma^2 n} {n^2}\n\t| c = as $\\ds \\sum_{i \\mathop = 1}^n 1 = n$\n}}\n{{eqn\t| r = \\frac {\\sigma^2} n\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Variance\nCategory:Inductive Statistics\n\\end{proof}\n\n"}}, "23294": {"score": 0.7321272492408752, "content": {"text": "\\section{Variance of Student's t-Distribution}\nTags: Student's t-Distribution, Variance\n\n\\begin{theorem}\nLet $k$ be a strictly positive integer. \nLet $X \\sim t_k$ where $t_k$ is the $t$-distribution with $k$ degrees of freedom.\nThen the variance of $X$ is given by: \n:$\\var X = \\dfrac k {k - 2}$\nfor $k > 2$, and does not exist otherwise.\n\\end{theorem}\n\n\\begin{proof}\nBy Expectation of Student's t-Distribution, we have that $\\expect X$ exists {{iff}} $k > 2$. \nHence, take $k > 2$ from here on.\nBy Expectation of Student's t-Distribution, we have that for $k > 2$: \n:$\\expect X = 0$\nFrom Square of Random Variable with t-Distribution has F-Distribution, we have: \n:$\\expect {X^2} = \\expect Y$\nwith $Y \\sim F_{1, k}$, where $F_{1, k}$ is the $F$-distribution with $\\tuple {1, k}$ degrees of freedom.\nSince $k > 2$, by Expectation of F-Distribution we have: \n:$\\expect {X^2} = \\dfrac k {k - 2}$\nWe therefore have: \n{{begin-eqn}}\n{{eqn\t| l = \\var X\n\t| r = \\expect {X^2} - \\paren {\\expect X}^2\n\t| c = Variance as Expectation of Square minus Square of Expectation\n}}\n{{eqn\t| r = \\frac k {k - 2} - 0^2\n}}\n{{eqn\t| r = \\frac k {k - 2}\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Variance\nCategory:Student's t-Distribution\n\\end{proof}\n\n"}}, "8159": {"score": 0.7304355502128601, "content": {"text": "\\begin{definition}[Definition:Sample Statistic/Continuous]\nData which can be described with a continuous variable are known as '''continuous data'''.\n\\end{definition}"}}, "8843": {"score": 0.762885332107544, "content": {"text": "\\begin{definition}[Definition:Statistic]\nA '''statistic''' is broadly defined as '''a quantity which is calculated from sample data.\n\\end{definition}"}}, "9008": {"score": 0.7439225316047668, "content": {"text": "\\begin{definition}[Definition:Student's t-Distribution]\nLet $X$ be a continuous random variable on a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $\\Img X = \\R$.\n$X$ is said to have a '''$t$-distribution''' with $k$ degrees of freedom {{iff}} it has probability density function: \n:$\\map {f_X} x = \\dfrac {\\map \\Gamma {\\frac {k + 1} 2} } {\\sqrt {\\pi k} \\map \\Gamma {\\frac k 2} } \\paren {1 + \\dfrac {x^2} k}^{-\\frac {k + 1} 2}$\nfor some $k \\in \\R_{> 0}$.\nThis is written: \n:$X \\sim \\StudentT k$\n\\end{definition}"}}, "9320": {"score": 0.7651975750923157, "content": {"text": "\\begin{definition}[Definition:Test Statistic]\nLet $\\theta$ be a population parameter of some population $P$. \nLet $\\Omega$ be the parameter space of $\\theta$. \nLet $\\mathbf X$ be a random sample from $P$. \nLet $T = \\map f {\\mathbf X}$ be a sample statistic.\nLet $\\delta$ be a test procedure of the form: \n:reject $H_0$ if $T \\in C$\nfor some null hypothesis $H_0$ and some $C \\subset \\Omega$.\nWe refer to $T$ as the '''test statistic''' of $\\delta$.\n\\end{definition}"}}, "8158": {"score": 0.749011218547821, "content": {"text": "\\begin{definition}[Definition:Sample Statistic]\nA '''sample statistic''' is a numerical description of a sample.\n\\end{definition}"}}, "4138": {"score": 0.7613393664360046, "content": {"text": "\\begin{definition}[Definition:Hypothesis Test]\nA '''hypothesis test''' is a rule that specifies, for a null hypothesis $H_0$ and alternative hypothesis $H_1$: \n* For which sample values the decision is made to accept $H_0$.\n* For which sample values $H_0$ is rejected and $H_1$ is accepted.\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/viterbi1.json": {"gold": {"5461": 1}, "retrieved": {"17250": {"score": 0.7487415671348572, "content": {"text": "\\section{Bernoulli Process as Negative Binomial Distribution/First Form}\nTags: Negative Binomial Distribution, Bernoulli Distribution\n\n\\begin{theorem}\nLet $\\sequence {X_i}$ be a Bernoulli process with parameter $p$.\nLet $\\EE$ be the experiment which consists of performing the Bernoulli trial $X_i$ until a total of $n$ failures have been encountered.\nLet $X$ be the discrete random variable defining the number of successes before $n$ failures have been encountered.\nThen $X$ is modeled by a negative binomial distribution of the first form.\n\\end{theorem}\n\n\\begin{proof}\nThe number of Bernoulli trials may be as few as $0$, so the image is correct:\n:$\\Img X = \\set {0, 1, 2, \\ldots}$\nIf $X$ takes the value $x$, then there must have been $n + x$ trials altogether.\nSo, after $n + x - 1$ trials, there must have been $n - 1$ failures, as (from the description of the experiment) the last trial is a failure.\nSo the probability of the occurrence of the event $\\sqbrk {X = x}$ is given by the binomial distribution, as follows:\n:$\\map {p_X} x = \\dbinom {n + x - 1} {n - 1} p^x \\paren {1 - p}^n$\nwhere $x \\in \\set {0, 1, 2, \\ldots}$\nHence the result, by definition of first form of the negative binomial distribution.\n{{qed}}\nCategory:Negative Binomial Distribution\nCategory:Bernoulli Distribution\n\\end{proof}\n\n"}}, "17248": {"score": 0.750717282295227, "content": {"text": "\\section{Bernoulli Process as Geometric Distribution}\nTags: Probability Theory, Geometric Distribution, Bernoulli Distribution\n\n\\begin{theorem}\nLet $\\sequence {X_i}$ be a Bernoulli process with parameter $p$.\nLet $\\EE$ be the experiment which consists of performing the Bernoulli trial $X_i$ until a failure occurs, and then stop.\nLet $k$ be the number of successes before a failure is encountered.\nThen $k$ is modelled by a geometric distribution with parameter $p$.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from the definition of geometric distribution.\nLet $X$ be the discrete random variable defined as the number of successes before a failure is encountered.\nThus the last trial (and the last trial only) will be a failure, and the others will be successes.\nThe probability that $k$ successes are followed by a failure is:\n:$\\map \\Pr {X = k} = p^k \\paren {1 - p}$\nHence the result.\n{{Qed}}\n\\end{proof}\n\n"}}, "17359": {"score": 0.7641716599464417, "content": {"text": "\\section{Bayes' Theorem}\nTags: Probability Theory, Named Theorems\n\n\\begin{theorem}\nLet $\\Pr$ be a probability measure on a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $\\condprob A B$ denote the conditional probability of $A$ given $B$.\nLet $\\map \\Pr A > 0$ and $\\map \\Pr B > 0$.\nThen:\n:$\\condprob B A = \\dfrac {\\condprob A B \\, \\map \\Pr B} {\\map \\Pr A}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of conditional probabilities, we have:\n:$\\condprob A B = \\dfrac {\\map \\Pr {A \\cap B} } {\\map \\Pr B}$\n:$\\condprob B A = \\dfrac {\\map \\Pr {A \\cap B} } {\\map \\Pr A}$\nAfter some algebra:\n:$\\condprob A B \\, \\map \\Pr B = \\map \\Pr {A \\cap B} = \\condprob B A \\, \\map \\Pr A$\nDividing both sides by $\\map \\Pr A$ (we are told that it is non-zero), the result follows:\n:$\\condprob B A = \\dfrac {\\condprob A B \\, \\map \\Pr B} {\\map \\Pr A}$\n{{Qed}}\n\\end{proof}\n\n"}}, "547": {"score": 0.7533022165298462, "content": {"text": "\\begin{definition}[Definition:Bernoulli Distribution]\nLet $X$ be a discrete random variable on a probability space.\nThen $X$ has the '''Bernoulli distribution with parameter $p$''' {{iff}}:\n: $(1): \\quad X$ has exactly two possible values, for example $\\Img X = \\set {a, b}$\n: $(2): \\quad \\map \\Pr {X = a} = p$\n: $(3): \\quad \\map \\Pr {X = b} = 1 - p$\nwhere $0 \\le p \\le 1$.\nThat is, the probability mass function is given by:\n:$\\map {p_X} x = \\begin {cases}\np & : x = a \\\\\n1 - p & : x = b \\\\\n0 & : x \\notin \\set {a, b} \\\\\n\\end {cases}$\nIf we allow:\n:$\\Img X = \\set {0, 1}$\nthen we can write:\n:$\\map {p_X} x = p^x \\paren {1 - p}^{1 - x}$\n\\end{definition}"}}, "13890": {"score": 0.7519698739051819, "content": {"text": "\\section{Expectation of Hat-Check Distribution}\nTags: Hat-Check Distribution, Expectation of Hat-Check Distribution, Expectation, Hat-Check Problem\n\n\\begin{theorem}\nLet $X$ be a discrete random variable with the Hat-Check distribution with parameter $n$.\nThen the expectation of $X$ is given by:\n:$\\expect X = n - 1$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of expectation:\n:$\\ds \\expect X = \\sum_{x \\mathop \\in \\Img X} x \\map \\Pr {X = x}$\nBy definition of hat-check distribution:\n:$\\ds \\expect X = \\sum_{k \\mathop = 0}^n k \\dfrac 1 {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!}$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\expect X\n      | r = \\sum_{k \\mathop = 1}^n k \\dfrac 1 {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!}\n      | c = as the $k = 0$ term vanishes\n}}\n{{eqn | r = \\sum_{y \\mathop = n - 1}^0 \\paren {n - y } \\dfrac 1 {y!} \\sum_{s \\mathop = 0}^{n - y} \\dfrac {\\paren {-1}^s} {s!}\n      | c = Let $y = n - k$\n}}\n{{eqn | r = n \\sum_{y \\mathop = 0}^{n - 1} \\dfrac 1 {y!} \\sum_{s \\mathop = 0}^{n - y} \\dfrac {\\paren {-1}^s} {s!} - \\sum_{y \\mathop = 0}^{n - 1} \\dfrac y {y!} \\sum_{s \\mathop = 0}^{n - y} \\dfrac {\\paren {-1}^s} {s!}\n      | c = \n}}\n{{eqn | r = n \\sum_{k \\mathop = 1}^n \\dfrac 1 {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!} - \\sum_{k \\mathop = 1}^n \\dfrac {n - k} {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!}\n      | c = Let $y = n - k$\n}}\n{{eqn | r = n \\sum_{k \\mathop = 1}^n \\dfrac 1 {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!} + \\dfrac n {n!} - \\dfrac n {n!} - \\sum_{k \\mathop = 1}^n \\dfrac {n - k} {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!}\n      | c = adding $0$\n}}\n{{eqn | r = n \\sum_{k \\mathop = 0}^n \\dfrac 1 {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!} - \\sum_{k \\mathop = 0}^n \\dfrac {n - k} {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!}\n      | c = \n}}\n{{eqn | r = n \\sum_{k \\mathop = 0}^n \\dfrac 1 {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!} - \\sum_{k \\mathop = 0}^{n - 1} \\dfrac {n - k} {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!}\n      | c = as the $k = n$ term vanishes\n}}\n{{eqn | r = n \\sum_{k \\mathop = 0}^n \\dfrac 1 {\\paren {n - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!} - \\sum_{k \\mathop = 0}^{n - 1} \\dfrac 1 {\\paren {n - 1 - k }!} \\sum_{s \\mathop = 0}^k \\dfrac {\\paren {-1}^s} {s!}\n      | c = \n}}\n{{eqn | r = n - 1\n      | c = Hat-Check Distribution Gives Rise to Probability Mass Function\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "17360": {"score": 0.7731835246086121, "content": {"text": "\\section{Bayes' Theorem/General Result}\nTags: Probability Theory\n\n\\begin{theorem}\nLet $\\Pr$ be a probability measure on a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $\\set {B_1, B_2, \\ldots}$ be a  partition of the event space $\\Sigma$.\nThen, for any $B_i$ in the partition:\n:$\\condprob {B_i} A = \\dfrac {\\condprob A {B_i} \\map \\Pr {B_i} } {\\map \\Pr A} = \\dfrac {\\condprob A {B_i} \\map \\Pr {B_i} } {\\sum_j \\condprob A {B_j} \\map \\Pr {B_j} }$\nwhere $\\ds \\sum_j$ denotes the sum over $j$.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from the Total Probability Theorem:\n:$\\ds \\map \\Pr A = \\sum_i \\condprob A {B_i} \\map \\Pr {B_i}$\nand Bayes' Theorem:\n:$\\condprob {B_i} A = \\dfrac {\\condprob A {B_i} \\map \\Pr {B_i} } {\\map \\Pr A}$\n{{qed}}\n{{Namedfor|Thomas Bayes|cat = Bayes}}\n\\end{proof}\n\n"}}, "17247": {"score": 0.7652310729026794, "content": {"text": "\\section{Bernoulli Process as Binomial Distribution}\nTags: Binomial Distribution, Probability Theory, Bernoulli Distribution\n\n\\begin{theorem}\nLet $\\sequence {X_i}$ be a finite Bernoulli process of length $n$ such that each of the $X_i$ in the sequence is a Bernoulli trial with parameter $p$.\nThen the number of successes in $\\sequence {X_i}$ is modelled by a binomial distribution with parameters $n$ and $p$.\nHence it can be seen that:\n:$X \\sim \\Binomial 1 p$ is the same thing as $X \\sim \\Bernoulli p$\n\\end{theorem}\n\n\\begin{proof}\nConsider the sample space $\\Omega$ of all sequences $\\sequence {X_i}$ of length $n$.\nThe $i$th entry of any such sequence is the result of the $i$th trial.\nWe have that $\\Omega$ is finite.\nLet us take the event space $\\Sigma$ to be the power set of $\\Omega$.\nAs the elements of $\\Omega$ are independent, by definition of the Bernoulli process, we have that:\n:$\\forall \\omega \\in \\Omega: \\map \\Pr \\omega = p^{\\map s \\omega} \\paren {1 - p}^{n - \\map s \\omega}$\nwhere $\\map s \\omega$ is the number of successes in $\\omega$.\nIn the same way:\n:$\\ds \\forall A \\in \\Sigma: \\map \\Pr A = \\sum_{\\omega \\mathop \\in A} \\map \\Pr \\omega$\nNow, let us define the discrete random variable $Y_i$ as follows:\n:$\\map {Y_i} \\omega = \\begin{cases}\n1 & : \\text {$\\omega_i$ is a success} \\\\\n0 & : \\text {$\\omega_i$ is a failure}\n\\end{cases}$\nwhere $\\omega_i$ is the $i$th element of $\\omega$.\nThus, each $Y_i$ has image $\\set {0, 1}$ and a probability mass function:\n:$\\map \\Pr {Y_i = 0} = \\map \\Pr {\\set {\\omega \\in \\Omega: \\text {$\\omega_i$ is a success} } }$\nThus we have:\n{{begin-eqn}}\n{{eqn | l = \\map \\Pr {Y_i = 1}\n      | r = \\sum_{\\omega: \\text {$\\omega_i$ success} } p^{\\map s \\omega} \\paren {1 - p}^{n - \\map s \\omega}\n}}\n{{eqn | r = \\sum_{r \\mathop = 1}^n \\sum_{\\substack {\\omega: \\text {$\\omega_i$ success} \\\\ \\map s \\omega = r} } p^r \\paren {1 - p}^{n - r}\n}}\n{{eqn | r = \\sum_{r \\mathop = 1}^n \\binom {n - 1} {r - 1} p^r \\paren {1 - p}^{n - r}\n      | c = as we already know the position of one success (namely $i$)\n}}\n{{eqn | r = p \\sum_{r \\mathop = 0}^{n - 1} \\binom {n - 1} r p^r \\paren {1 - p}^{\\paren {n - 1} - r}\n      | c = switching summation index\n}}\n{{eqn | r = p \\paren {p + \\paren {1 - p} }^{n - 1}\n      | c = Binomial Theorem\n}}\n{{eqn | r = p\n}}\n{{end-eqn}}\nThen:\n:$\\map \\Pr {Y_i = 0} = 1 - \\map \\Pr {Y_i = 1} = 1 - p$\nSo (by a roundabout route) we have confirmed that $Y_i$ has the Bernoulli distribution with parameter $p$.\nNow, let us define the random variable:\n:$\\ds \\map {S_n} \\omega = \\sum_{i \\mathop = 1}^n \\map {Y_i} \\omega$\nBy definition:\n:$\\map {S_n} \\omega$ is the number of successes in $\\omega$\n:$S_n$ takes values in $\\set {0, 1, 2, \\ldots, n}$ (as each $Y_i$ can be $0$ or $1$).\nAlso, we have that:\n{{begin-eqn}}\n{{eqn | l = \\map \\Pr {S_n = k}\n      | r = \\map \\Pr {\\set {\\omega \\in \\Omega: \\map s \\omega = k} }\n}}\n{{eqn | r = \\sum_{\\omega: \\map s \\omega \\mathop = k} \\map \\Pr \\omega\n}}\n{{eqn | r = \\sum_{\\omega: \\map s \\omega \\mathop = k} p^k \\paren {1 - p}^{n - k}\n}}\n{{eqn | r = \\binom n k p^k \\paren {1 - p}^{n - k}\n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "16849": {"score": 0.781369149684906, "content": {"text": "\\section{Chain Rule for Probability}\nTags: Definitions: Probability Theory, Named Theorems, Conditional Probabilities\n\n\\begin{theorem}\nLet $\\EE$ be an experiment with probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $A, B \\in \\Sigma$ be events of $\\EE$.\nThe '''conditional probability of $A$ given $B$''' is:\n:$\\condprob A B = \\dfrac {\\map \\Pr {A \\cap B} } {\\map \\Pr B}$\n\\end{theorem}\n\n\\begin{proof}\nSuppose it is given that $B$ has occurred.\nThen the probability of $A$ having occurred may not be $\\map \\Pr A$ after all.\nIn fact, we ''can'' say that $A$ has occurred {{iff}} $A \\cap B$ has occurred.\nSo, if we ''know'' that $B$ has occurred, the conditional probability of $A$ given $B$ is $\\map \\Pr {A \\cap B}$.\nIt follows then, that if we ''don't'' actually know whether $B$ has occurred or not, but we know its probability $\\map \\Pr B$, we can say that:\n:The probability that $A$ and $B$ have both occurred is the conditional probability of $A$ given $B$ multiplied by the probability that $B$ has occurred.\nHence:\n:$\\condprob A B = \\dfrac {\\map \\Pr {A \\cap B} } {\\map \\Pr B}$\n{{qed}}\n\\end{proof}\n\n"}}, "13874": {"score": 0.7682092189788818, "content": {"text": "\\section{Expectation of Bernoulli Distribution}\nTags: Expectation, Expectation of Bernoulli Distribution, Bernoulli Distribution\n\n\\begin{theorem}\nLet $X$ be a discrete random variable with a Bernoulli distribution with parameter $p$.\nThen the expectation of $X$ is given by:\n:$\\expect X = p$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of expectation:\n:<math>E \\left({X}\\right) = \\sum_{x \\in \\operatorname{Im} \\left({X}\\right)} x \\Pr \\left({X = x}\\right)</math>\nBy definition of Bernoulli distribution:\n:<math>E \\left({X}\\right) = 1 \\times p + 0 \\times \\left({1-p}\\right)</math>\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "5461": {"score": 0.7686552405357361, "content": {"text": "\\begin{definition}[Definition:Markov Chain]\nLet $\\sequence {X_n}_{n \\mathop \\ge 0}$ be a stochastic process over a countable set $S$.\nLet $\\map \\Pr X$ denote the probability of the random variable $X$.\nLet $\\sequence {X_n}_{n \\mathop \\ge 0}$ satisfy the Markov property:\n:$\\condprob {X_{n + 1} = i_{n + 1} } {X_0 = i_0, X_1 = i_1, \\ldots, X_n = i_n} = \\condprob {X_{n + 1} = i_{n + 1} } {X_n = i_n}$\nfor all $n \\ge 0$ and all $i_0, i_1, \\ldots, i_{n + 1} \\in S$.\nThat is, such that the conditional probability of $X_{i + 1}$ is dependent only upon $X_i$ and upon no earlier values of $\\sequence {X_n}$.\nThat is, the state of $\\sequence {X_n}$ in the future is unaffected by its history.\nThen $\\sequence {X_n}_{n \\mathop \\ge 0}$ is a '''Markov chain'''.\n\\end{definition}"}}}}, "TheoremQA_xueguangma/fundamental_theorem_of_calculus.json": {"gold": {"11986": 1, "19418": 1}, "retrieved": {"19265": {"score": 0.7041853070259094, "content": {"text": "\\section{Primitive of Power of p x + q over Root of a x + b}\nTags: Primitives involving Root of a x + b and p x + q\n\n\\begin{theorem}\n:$\\ds \\int \\frac {\\paren {p x + q}^n} {\\sqrt {a x + b} } \\rd x = \\frac {2 \\paren {p x + q}^n \\sqrt {a x + b} } {\\paren {2 n + 1} a} + \\frac {2 n \\paren {a q - b p} } {\\paren {2 n + 1} a} \\int \\frac {\\paren {p x + q}^{n - 1} } {\\sqrt {a x + b} } \\rd x$\n\\end{theorem}\n\n\\begin{proof}\nFrom Reduction Formula for Primitive of Power of $a x + b$ by Power of $p x + q$: Decrement of Power:\n:$\\ds \\int \\paren {a x + b}^m \\paren {p x + q}^n \\rd x = \\frac {\\paren {a x + b}^{m + 1} \\paren {p x + q}^n} {\\paren {m + n + 1} a} - \\frac {n \\paren {b p - a q} } {\\paren {m + n + 1} a} \\int \\paren {a x + b}^m \\paren {p x + q}^{n - 1} \\rd x$\nSetting $m := -\\dfrac 1 2$:\n{{begin-eqn}}\n{{eqn | l = \\int \\frac {\\paren {p x + q}^n} {\\sqrt {a x + b} } \\rd x\n      | r = \\frac {\\paren {a x + b}^{-1/2 + 1} \\paren {p x + q}^n} {\\paren {-\\frac 1 2 + n + 1} a} - \\frac {n \\paren {b p - a q} } {\\paren {-\\frac 1 2 + n + 1} a} \\int \\paren {a x + b}^{-1/2} \\paren {p x + q}^{n - 1} \\rd x\n      | c = \n}}\n{{eqn | r = \\frac {2 \\paren {p x + q}^n \\sqrt {a x + b} } {\\paren {2 n + 1} a} + \\frac {2 n \\paren {a q - b p} } {\\paren {2 n + 1} a} \\int \\frac {\\paren {p x + q}^{n - 1} } {\\sqrt {a x + b} } \\rd x\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "19264": {"score": 0.7048924565315247, "content": {"text": "\\section{Primitive of Power of p x + q by Root of a x + b}\nTags: Primitives involving Root of a x + b and p x + q\n\n\\begin{theorem}\n:$\\ds \\int \\paren {p x + q}^n \\sqrt {a x + b} \\rd x = \\frac {2 \\paren {p x + q}^{n + 1} \\sqrt {a x + b} } {\\paren {2 n + 3} p} + \\frac {b p - a q} {\\paren {2 n + 3} p} \\int \\frac {\\paren {p x + q}^n} {\\sqrt{a x + b} } \\rd x$\n\\end{theorem}\n\n\\begin{proof}\nFrom Reduction Formula for Primitive of Power of $a x + b$ by Power of $p x + q$: Decrement of Power:\n:$\\ds \\int \\paren {a x + b}^m \\paren {p x + q}^n \\rd x = \\frac {\\paren {a x + b}^m \\paren {p x + q}^{n + 1} } {\\paren {m + n + 1} p} + \\frac {m \\paren {b p - a q} } {\\paren {m + n + 1} p} \\int \\paren {a x + b}^{m - 1} \\paren {p x + q}^n \\rd x$\nSetting $m := \\dfrac 1 2$:\n{{begin-eqn}}\n{{eqn | l = \\int \\paren {a x + b}^{1 / 2} \\paren {p x + q}^n \\rd x\n      | r = \\frac {\\paren {a x + b}^{1 / 2} \\paren {p x + q}^{n + 1} } {\\paren {\\frac 1 2 + n + 1} p} + \\frac {\\dfrac 1 2 \\paren {b p - a q} } {\\paren {\\frac 1 2 + n + 1} p} \\int \\paren {a x + b}^{1 / 2 - 1} \\paren {p x + q}^n \\rd x\n      | c = \n}}\n{{eqn | r = \\frac {2 \\paren {p x + q}^{n + 1} \\sqrt {a x + b} } {\\paren {2 n + 3} p} + \\frac {b p - a q} {\\paren {2 n + 3} p} \\int \\frac {\\paren {p x + q}^n} {\\sqrt {a x + b} } \\rd x\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "19431": {"score": 0.7083597779273987, "content": {"text": "\\section{Primitive of Root of a x + b by Root of p x + q}\nTags: Primitives involving Root of a x + b and Root of p x + q\n\n\\begin{theorem}\n:$\\ds \\int \\sqrt {\\paren {a x + b} \\paren {p x + q} } \\rd x = \\frac {2 a p x + b p + a q} {4 a p} \\sqrt {\\paren {a x + b} \\paren {p x + q} } - \\frac {\\paren {b p - a q}^2} {8 a p} \\int \\frac {\\d x} {\\sqrt {\\paren {a x + b} \\paren {p x + q} } }$\n\\end{theorem}\n\n\\begin{proof}\nFrom Primitive of $\\paren {p x + q}^n \\sqrt {a x + b}$:\n:$\\ds \\int \\paren {p x + q}^n \\sqrt {a x + b} \\rd x = \\frac {2 \\paren {p x + q}^{n + 1} \\sqrt {a x + b} } {\\paren {2 n + 3} p} + \\frac {b p - a q} {\\paren {2 n + 3} p} \\int \\frac {\\paren {p x + q}^n} {\\sqrt {a x + b} } \\rd x$\nPutting $n = \\dfrac 1 2$:\n{{begin-eqn}}\n{{eqn | o = \n      | r = \\int \\sqrt {\\paren {a x + b} \\paren {p x + q} } \\rd x\n      | c = \n}}\n{{eqn | r = \\frac {2 \\paren {p x + q}^{1 / 2 + 1} \\sqrt {a x + b} } {\\paren {2 \\cdot \\frac 1 2 + 3} p} + \\frac {b p - a q} {\\paren {2 \\cdot \\frac 1 2 + 3} p} \\int \\frac {\\paren {p x + q}^{1 / 2} } {\\sqrt {a x + b} } \\rd x\n      | c = \n}}\n{{eqn | r = \\frac {2 \\paren {p x + q} \\sqrt {p x + q} \\sqrt {a x + b} } {4 p} + \\frac {b p - a q} {4 p} \\int \\frac {\\sqrt {p x + q} } {\\sqrt {a x + b} } \\rd x\n      | c = \n}}\n{{eqn | r = \\frac {2 \\paren {p x + q} \\sqrt {p x + q} \\sqrt {a x + b} } {4 p} + \\frac {b p - a q} {4 p} \\paren {\\frac {\\sqrt {\\paren {a x + b} \\paren {p x + q} } } a + \\frac {a q - b p} {2 a} \\int \\frac {\\d x} {\\sqrt {\\paren {a x + b} \\paren {p x + q} } } }\n      | c = Primitive of $\\dfrac {\\sqrt {p x + q} } {\\sqrt {a x + b} }$\n}}\n{{eqn | r = \\frac {\\paren {2 a \\paren {p x + q} + \\paren {b p - a q} } \\sqrt {\\paren {a x + b} \\paren {p x + q} } } {4 a p} - \\frac {\\paren {b p - a q}^2 } {8 a p} \\int \\frac {\\d x} {\\sqrt {\\paren {a x + b} \\paren {p x + q} } }\n      | c = extracting common factor\n}}\n{{eqn | r = \\frac {2 a p x + b p + a q} {4 a p} \\sqrt {\\paren {a x + b} \\paren {p x + q} } - \\frac {\\paren {b p - a q}^2} {8 a p} \\int \\frac {\\d x} {\\sqrt {\\paren {a x + b} \\paren {p x + q} } }\n      | c = simplifying\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "9859": {"score": 0.7073938250541687, "content": {"text": "\\begin{definition}[Definition:Unknown]\nAn '''unknown''' is an indeterminate variable in a system of equations or inequalities whose value the solver is invited to determine.\n\\end{definition}"}}, "17190": {"score": 0.7057835459709167, "content": {"text": "\\section{Binet Form}\nTags: Binet Form, Analysis\n\n\\begin{theorem}\nLet $m \\in \\R$.\nDefine:\n{{begin-eqn}}\n{{eqn | l = \\Delta\n      | r = \\sqrt {m^2 + 4}\n}}\n{{eqn | l = \\alpha\n      | r = \\frac {m + \\Delta} 2\n}}\n{{eqn | l = \\beta\n      | r = \\frac {m - \\Delta} 2\n}}\n{{end-eqn}}\n\\end{theorem}\n\n\\begin{proof}\nProof by induction:\nLet $\\map P n$ be the proposition:\n:$U_{n - 1} + U_{n + 1} = V_n$\n\\end{proof}\n\n"}}, "8006": {"score": 0.7297558784484863, "content": {"text": "\\begin{definition}[Definition:Right Hand Side]\nIn an equation:\n:$\\text {Expression $1$} = \\text {Expression $2$}$\nthe term $\\text {Expression $2$}$ is the '''right hand side'''.\n\\end{definition}"}}, "20413": {"score": 0.7091624140739441, "content": {"text": "\\section{Reduction Formula for Primitive of Power of a x + b by Power of p x + q/Increment of Power}\nTags: Primitives involving a x + b and p x + q\n\n\\begin{theorem}\n:$\\ds \\int \\paren {a x + b}^m \\paren {p x + q}^n \\rd x = \\frac 1 {\\paren {n + 1} \\paren {b p - a q} } \\paren {\\paren {a x + b}^{m + 1} \\paren {p x + q}^{n + 1} - a \\paren {m + n + 2} \\int \\paren {a x + b}^m \\paren {p x + q}^{n + 1} \\rd x}$\n\\end{theorem}\n\n\\begin{proof}\nFrom Reduction Formula for Primitive of Power of $a x + b$ by Power of $p x + q$: Decrement of Power:\n:$\\ds \\int \\paren {a x + b}^m \\paren {p x + q}^n \\rd x = \\frac {\\paren {a x + b}^{m + 1} \\paren {p x + q}^n} {\\paren {m + n + 1} a} - \\frac {n \\paren {b p - a q} } {\\paren {m + n + 1} a} \\int \\paren {a x + b}^m \\paren {p x + q}^{n - 1} \\rd x$\nPutting $n + 1$ for $n$, this yields:\n{{begin-eqn}}\n{{eqn | l = \\int \\paren {a x + b}^m \\paren {p x + q}^{n + 1} \\rd x\n      | r = \\frac {\\paren {a x + b}^{m + 1} \\paren {p x + q}^{n + 1} } {\\paren {m + n + 2} a} - \\frac {\\paren {n + 1} \\paren {b p - a q} } {\\paren {m + n + 2} a} \\int \\paren {a x + b}^m \\paren {p x + q}^n \\rd x\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\int \\paren {a x + b}^m \\paren {p x + q}^n\n      | r = \\frac {\\paren {a x + b}^{m + 1} \\paren {p x + q}^{n + 1} } {\\paren {n + 1} \\paren {b p - a q} } - \\frac {\\paren {m + n + 2} a} {\\paren {n + 1} \\paren {b p - a q} } \\int \\paren {a x + b}^m \\paren {p x + q}^{n + 1} \\rd x\n      | c = \n}}\n{{eqn | r = \\frac 1 {\\paren {n + 1} \\paren {b p - a q} } \\paren {\\paren {a x + b}^{m + 1} \\paren {p x + q}^{n + 1} - a \\paren {m + n + 2} \\int \\paren {a x + b}^m \\paren {p x + q}^{n + 1} \\rd x}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "4991": {"score": 0.7301982641220093, "content": {"text": "\\begin{definition}[Definition:Left Hand Side]\nIn an equation:\n:$\\text {Expression $1$} = \\text {Expression $2$}$\nthe term $\\text {Expression $1$}$ is the '''left hand side'''.\n\\end{definition}"}}, "5471": {"score": 0.7175533771514893, "content": {"text": "\\begin{definition}[Definition:Mathematical Model]\nA '''mathematical model''' is an equation, or a system of equations, whose purpose is to provide an approximation to the behavior of a real-world phenomenon.\n\\end{definition}"}}, "2850": {"score": 0.7243675589561462, "content": {"text": "\\begin{definition}[Definition:Equation]\nAn '''equation''' is a mathematical statement that states that two expressions are equal.\nFor expressions $A$ and $B$, this would usually be portrayed:\n:$A = B$\nwhere $A$ is known as the {{LHS}} and $B$ the {{RHS}}.\n\\end{definition}"}}}}, "TheoremQA_wenhuchen/definite_matrix1.json": {"gold": {"5725": 1, "20341": 1}, "retrieved": {"20340": {"score": 0.8360300064086914, "content": {"text": "\\section{Real Symmetric Matrix is Hermitian}\nTags: Linear Algebra, Hermitian Matrices\n\n\\begin{theorem}\nEvery real symmetric matrix is Hermitian.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\mathbf A$ be a real symmetric matrix.\nThen we have:\n{{begin-eqn}}\n{{eqn | l = \\sqbrk {\\mathbf A}^\\dagger_{i j}\n      | r = \\overline {\\sqbrk {\\mathbf A}_{ji} }\n      | c = {{Defof|Hermitian Conjugate}}\n}}\n{{eqn | r = \\sqbrk {\\mathbf A}_{ji}\n      | c = Complex Number equals Conjugate iff Wholly Real: $\\mathbf A$ is Real\n}}\n{{eqn | r = \\sqbrk {\\mathbf A}_{ij}\n      | c = $\\mathbf A$ is Symmetric\n}}\n{{end-eqn}}\nSo:\n:$\\mathbf A = \\mathbf A^\\dagger$\nThus, by definition, $\\mathbf A$ is Hermitian.\n{{qed}}\nCategory:Linear Algebra\nCategory:Hermitian Matrices\n\\end{proof}\n\n"}}, "15069": {"score": 0.8373987078666687, "content": {"text": "\\section{Diagonal Matrix is Symmetric}\nTags: Diagonal Matrices, Symmetric Matrices\n\n\\begin{theorem}\nLet $D$ be a diagonal matrix.\nThen $D$ is symmetric.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of diagonal matrix:\n:$\\forall j, k: j \\ne k \\implies a_{jk} = 0 = a_{kj}$\nSo by definition of transpose of $D$:\n:$D = D^\\intercal$\nwhere $D^\\intercal$ denotes the transpose.\nHence the result, by definition of symmetric matrix.\n{{qed}}\nCategory:Symmetric Matrices\nCategory:Diagonal Matrices\n\\end{proof}\n\n"}}, "6075": {"score": 0.8423776626586914, "content": {"text": "\\begin{definition}[Definition:Non-Negative Definite Mapping]\nLet $\\C$ be the field of complex numbers.\nLet $\\F$ be a subfield of $\\C$.\nLet $V$ be a vector space over $\\F$\nLet $\\left \\langle {\\cdot, \\cdot} \\right \\rangle : V \\times V \\to \\mathbb F$ be a mapping.\nThen $\\left \\langle {\\cdot, \\cdot} \\right \\rangle : V \\times V \\to \\mathbb F$ is '''non-negative definite''' {{iff}}:\n:$\\forall x \\in V: \\quad \\left \\langle {x, x} \\right \\rangle \\in \\R_{\\geq 0}$\nThat is, the image of $\\left \\langle {x, x} \\right \\rangle$ is always a non-negative real number.\n\\end{definition}"}}, "8520": {"score": 0.8392770290374756, "content": {"text": "\\begin{definition}[Definition:Signature of Symmetric Bilinear Form]\nLet $\\struct {V, q}$ be the scalar product space.\nLet $V^*$ be the vector space dual to $V$.\nSuppose there exists a basis $\\tuple {\\beta^i}$ for $V^*$ such that $q$ is expressible as:\n:$q = \\paren {\\beta^1}^2 + \\ldots + \\paren {\\beta^r}^2 - \\paren {\\beta^{r + 1}}^2 - \\ldots - \\paren {\\beta^{r + s}}^2$\nwhere:\n:$r, s \\in \\N : r + s = n$.\nThen the ordered pair $\\tuple {r, s}$ is known as the '''signature of $q$'''.\n\\end{definition}"}}, "12793": {"score": 0.8375215530395508, "content": {"text": "\\section{Hilbert Matrix is Cauchy Matrix}\nTags: Hilbert Matrix, Cauchy Matrix\n\n\\begin{theorem}\nA Hilbert matrix is a special case of a Cauchy matrix.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of Hilbert matrix, the element $a_{i j}$ is:\n:$a_{i j} = \\dfrac 1 {i + j - 1}$\nFor all $i, j \\in \\Z$ such that $1 \\le i \\le n$ and $1 \\le j \\le n$, let:\n:$x_i = i$\n:$y_j = j - 1$\nThen:\n:$a_{i j} = \\dfrac 1 {x_i + y_j}$\nThe result follows by definition of a Cauchy matrix.\n{{qed}}\n\\end{proof}\n\n"}}, "16760": {"score": 0.9303063154220581, "content": {"text": "\\section{Characterisation of Real Symmetric Positive Definite Matrix/Sufficient Condition}\nTags: Characterisation of Real Symmetric Positive Definite Matrix\n\n\\begin{theorem}\nLet $A$ be an $n \\times n$ symmetric matrix over $\\mathbb R$ such that: \n:there exists an invertible matrix $C$ such that $A = C^\\intercal C$.\nThen $A$ is positive definite.\n\\end{theorem}\n\n\\begin{proof}\nLet $A$ be a symmetric matrix such that: \n:there exists an invertible matrix $C$ such that $A = C^\\intercal C$.\nLet $\\mathbf v$ be a non-zero vector. \nThen:\n{{begin-eqn}}\n{{eqn\t| l = \\mathbf v^\\intercal A \\mathbf v\n\t| r = \\mathbf v^\\intercal C^\\intercal C \\mathbf v\n}}\n{{eqn\t| r = \\paren {C \\mathbf v}^\\intercal C \\mathbf v\n\t| c = Transpose of Matrix Product\n}}\n{{eqn\t| r = \\paren {C \\mathbf v} \\cdot \\paren {C \\mathbf v}\n\t| c = {{Defof|Dot Product}}\n}}\n{{eqn\t| r = \\norm {C \\mathbf v}^2\n\t| c = Dot Product of Vector with Itself\n}}\n{{eqn\t| o = >\n\t| r = 0\n\t| c = Euclidean Space is Normed Space\n}}\n{{end-eqn}}\nSo $A$ is positive definite. \n{{qed}}\nCategory:Characterisation of Real Symmetric Positive Definite Matrix\n\\end{proof}\n\n"}}, "15165": {"score": 0.8588007092475891, "content": {"text": "\\section{Determinant of Autocorrelation Matrix is Strictly Positive}\nTags: Autocorrelation Matrices, Determinant of Autocorrelation Matrix is Strictly Positive\n\n\\begin{theorem}\nLet $S$ be a strictly stationary stochastic process giving rise to a time series $T$.\nLet $\\sequence {s_n}$ be a sequence of $n$ successive values of $T$:\n:$\\sequence {s_n} = \\tuple {z_1, z_2, \\dotsb, z_n}$\nLet $\\mathbf P_n$ denote the '''autocorrelation matrix''' associated with $S$ for $\\sequence {s_n}$.\nThe determinant of $\\mathbf P_n$ is strictly positive.\n\\end{theorem}\n\n\\begin{proof}\nWe have that the Autocorrelation Matrix is Positive Definite.\nThe result follows from Determinant of Positive Definite Matrix is Strictly Positive.\n{{qed}}\n\\end{proof}\n\n"}}, "7143": {"score": 0.9515376091003418, "content": {"text": "\\begin{definition}[Definition:Positive Definite Matrix]\nLet $\\mathbf A$ be a square matrix of order $n$.\n$\\mathbf A$ is '''positive definite''' {{iff}}:\n:$(1): \\quad \\mathbf A$ is symmetric\n:$(2): \\quad$ for all column matrices $\\mathbf x$ of order $n$, $\\mathbf x^\\intercal \\mathbf A \\mathbf x$ is strictly positive.\n\\end{definition}"}}, "20341": {"score": 0.8963035345077515, "content": {"text": "\\section{Real Symmetric Positive Definite Matrix has Positive Eigenvalues}\nTags: Symmetric Matrices, Positive Definite Matrices\n\n\\begin{theorem}\nLet $A$ be a symmetric positive definite matrix over $\\mathbb R$.\nLet $\\lambda$ be an eigenvalue of $A$. \nThen $\\lambda$ is real with $\\lambda > 0$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\lambda$ be an eigenvalue of $A$ and let $\\mathbf v$ be a corresponding eigenvector.\nFrom Real Symmetric Matrix has Real Eigenvalues, $\\lambda$ is real.\nFrom the definition of a positive definite matrix, we have: \n:$\\mathbf v^\\intercal A \\mathbf v > 0$\nThat is: \n{{begin-eqn}}\n{{eqn\t| l = 0\n\t| o = <\n\t| r = \\mathbf v^\\intercal A \\mathbf v\n}}\n{{eqn\t| r = \\mathbf v^\\intercal \\paren {\\lambda \\mathbf v}\n\t| c = {{Defof|Eigenvector of Real Square Matrix}}\n}}\n{{eqn\t| r = \\lambda \\paren {\\mathbf v^\\intercal \\mathbf v}\n}}\n{{eqn\t| r = \\lambda \\paren {\\mathbf v \\cdot \\mathbf v}\n\t| c = {{Defof|Dot Product}}\n}}\n{{eqn\t| r = \\lambda \\norm {\\mathbf v}^2\n\t| c = Dot Product of Vector with Itself\n}}\n{{end-eqn}}\nFrom Euclidean Space is Normed Space, we have: \n:$\\norm {\\mathbf v}^2 > 0$\nso:\n:$\\lambda > 0$\n{{qed}}\nCategory:Symmetric Matrices\nCategory:Positive Definite Matrices\n\\end{proof}\n\n"}}, "16759": {"score": 0.8965806365013123, "content": {"text": "\\section{Characterisation of Real Symmetric Positive Definite Matrix/Necessary Condition}\nTags: Characterisation of Real Symmetric Positive Definite Matrix\n\n\\begin{theorem}\nLet $A$ be an $n \\times n$ positive definite symmetric matrix over $\\RR$.\nThen:\n:there exists an invertible matrix $C$ such that $A = C^\\intercal C$.\n\\end{theorem}\n\n\\begin{proof}\nLet $A$ be positive definite. \nFrom Real Symmetric Matrix is Orthogonally Diagonalizable:\n:there exists an orthogonal matrix $P$ and diagonal matrix $D$ such that $A = P^\\intercal D P$.\nFurther:\n:the diagonal entries of $D$ are the eigenvalues of $A$. \nFrom Real Symmetric Positive Definite Matrix has Positive Eigenvalues:\n:the diagonal entries of $D$ are positive. \nWe can therefore construct a real diagonal matrix $S$ by: \n:$\\paren S_{i j} = \\begin{cases} \\sqrt {\\paren D_{i i} } & i = j \\\\ 0 & i \\ne j \\end{cases}$\nFrom Product of Diagonal Matrices is Diagonal, we have: \n:$\\paren {S^2}_{i j} = \\begin{cases} \\paren D_{i i} & i = j \\\\ 0 & i \\ne j \\end{cases}$\nso:\n:$S^2 = D$\nWe also have: \n{{begin-eqn}}\n{{eqn\t| l = \\det S\n\t| r = \\prod_{i \\mathop = 1}^n \\sqrt {\\paren D_{i i} }\n}}\n{{eqn\t| r = \\sqrt {\\prod_{i \\mathop = 1}^n \\paren D_{i i} }\n}}\n{{eqn\t| o = >\n\t| r = 0\n\t| c = as $\\paren D_{i i} > 0$ for each $i$\n}}\n{{end-eqn}}\nWe therefore have: \n{{begin-eqn}}\n{{eqn\t| l = \\map \\det {P^\\intercal S P}\n\t| r = \\map \\det {P^\\intercal} \\det S \\det P\n\t| c = Determinant of Matrix Product\n}}\n{{eqn\t| r = \\paren {\\det P}^2 \\det S\n\t| c = Determinant of Transpose\n}}\n{{eqn\t| r = \\det S\n\t| c = Determinant of Orthogonal Matrix is Plus or Minus One\n}}\n{{eqn\t| o = >\n\t| r = 0\n}}\n{{end-eqn}}\nSo from Matrix is Invertible iff Determinant has Multiplicative Inverse:\n:$P^\\intercal S P$ is invertible.\nLet $C = P^\\intercal S P$. \nThen:\n{{begin-eqn}}\n{{eqn\t| l = C^\\intercal C\n\t| r = \\paren {P^\\intercal S P}^\\intercal P^\\intercal S P\n}}\n{{eqn\t| r = P^\\intercal \\paren {P^\\intercal S}^\\intercal P^\\intercal S P\n\t| c = Transpose of Matrix Product\n}}\n{{eqn\t| r = P^\\intercal S^\\intercal P P^\\intercal S P\n\t| c = Transpose of Matrix Product\n}}\n{{eqn\t| r = P^\\intercal S^\\intercal S P\n\t| c = as $P$ is orthogonal\n}}\n{{eqn\t| r = P^\\intercal S^2 P\n\t| c = Diagonal Matrix is Symmetric\n}}\n{{eqn\t| r = P^\\intercal D P\n}}\n{{eqn\t| r = A\n}}\n{{end-eqn}}\nAs $C$ is invertible, the proof is complete.\n{{qed}}\nCategory:Characterisation of Real Symmetric Positive Definite Matrix\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/Liouville\u2019s_theorem2.json": {"gold": {"11018": 1, "20530": 1}, "retrieved": {"4060": {"score": 0.8512850999832153, "content": {"text": "\\begin{definition}[Definition:Homogeneous Function/Real Space]\nLet $f: \\R^2 \\to \\R$ be a real-valued function of two variables.\n$\\map f {x, y}$ is a '''homogeneous function''' {{iff}}:\n:$\\exists n \\in \\Z: \\forall t \\in \\R: \\map f {t x, t y} = t^n \\map f {x, y}$\nThus, loosely speaking, a '''homogeneous function''' of $x$ and $y$ is one where $x$ and $y$ are both of the same \"power\".\n\\end{definition}"}}, "4063": {"score": 0.8518257737159729, "content": {"text": "\\begin{definition}[Definition:Homogeneous Function/Zero Degree]\nLet $V$ and $W$ be two vector spaces over a field $F$.\nLet $f: V \\to W$ be a function from $V$ to $W$.\n$f$ is a '''homogeneous function of degree zero''' {{iff}}:\n:$\\map f {\\alpha \\mathbf v} = \\alpha^0 \\map f {\\mathbf v} = \\map f {\\mathbf v}$\nCategory:Definitions/Homogeneous Functions\n\\end{definition}"}}, "4062": {"score": 0.8581655025482178, "content": {"text": "\\begin{definition}[Definition:Homogeneous Function/Real Space/Zero Degree]\nLet $f: \\R^2 \\to \\R$ be a real-valued function of two variables.\n$\\map f {x, y}$ is a '''homogeneous function of degree zero''' or '''of zero degree''' {{iff}}:\n:$\\forall t \\in \\R: \\map f {t x, t y} = t^0 \\map f {x, y} = \\map f {x, y}$\nCategory:Definitions/Homogeneous Functions\n\\end{definition}"}}, "13243": {"score": 0.8577910661697388, "content": {"text": "\\section{Function of Exponential Order of Scalar Multiple}\nTags: Exponential Order\n\n\\begin{theorem}\nLet $f: \\R \\to \\F$ be a function, where $\\F \\in \\set {\\R, \\C}$.\nLet $\\lambda$ be a real constant.\nLet $\\map f t$ be of exponential order $a$.\nThen the function defined by $t \\mapsto \\map f {\\lambda t}$ is of exponential order $a\\lambda$.\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\size {\\map f t}\n      | o = <\n      | r = K e^{a t}\n      | c = {{Defof|Exponential Order to Real Index}}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\size {\\map f {\\lambda t} }\n      | o = <\n      | r = K e^{a \\lambda t}\n      | c = replacing $t$ with $\\lambda t$\n}}\n{{end-eqn}}\nThe result follows by the definition of exponential order.\n{{qed}}\nCategory:Exponential Order\n\\end{proof}\n\n"}}, "20830": {"score": 0.8566952347755432, "content": {"text": "\\section{Scalar Multiple of Function of Exponential Order}\nTags: Exponential Order\n\n\\begin{theorem}\nLet $f: \\R \\to \\F$ be a function, where $\\F \\in \\set {\\R, \\C}$.\nLet $\\lambda$ be a complex constant.\nSuppose $f$ is of exponential order $a$.\nThen $\\lambda f$ is also of exponential order $a$.\n\\end{theorem}\n\n\\begin{proof}\nIf $\\lambda = 0$, the theorem holds trivially.\nLet $\\lambda \\ne 0$.\n{{begin-eqn}}\n{{eqn | l = \\size {\\map f t}\n      | o = <\n      | r = K e^{a t}\n      | c = {{Defof|Exponential Order to Real Index}}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\size \\lambda \\size {\\map f t}\n      | o = <\n      | r = \\size \\lambda K e^{a t}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\size {\\lambda \\, \\map f t}\n      | o = <\n      | r = K' e^{a t}\n      | c = Modulus of Product, $K' = \\size \\lambda K$\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Exponential Order\n\\end{proof}\n\n"}}, "11159": {"score": 0.8661905527114868, "content": {"text": "\\section{Linear Operator on General Logarithm}\nTags: Logarithms, Linear Algebra, Linear Operators, Analysis\n\n\\begin{theorem}\nLet $\\phi: \\R^\\R \\to \\R^\\R, y \\mapsto \\map \\phi y$ be a linear operator on the space of functions from $\\R\\to\\R$.\nLet $y$ be a real function such that:\n:$\\forall x \\in \\R: \\map y x > \\map \\bszero x = 0$.\nLet $\\log_a y$ be the logarithm of $y$ to base $a$.\nThen:\n:$\\map \\phi {\\log_a y} = \\dfrac 1 {\\ln a} \\paren {\\map \\phi {\\ln y} }$\nwhere $\\ln$ is the natural logarithm.\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\map \\phi {\\log_a y}\n      | r = \\map \\phi {\\frac {\\ln y}{\\ln a} }\n      | c = Change of Base of Logarithm\n}}\n{{eqn | r = \\frac 1 {\\ln a} \\paren {\\map \\phi {\\ln y} }\n      | c = {{Defof|Linear Operator}}\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Linear Operators\nCategory:Logarithms\n\\end{proof}\n\n"}}, "11803": {"score": 0.859954833984375, "content": {"text": "\\section{Inverse Integral Operator is Linear if Unique}\nTags: Integral Transforms\n\n\\begin{theorem}\nLet $T$ be an integral operator.\nLet $f$ be an integrable real function on a domain appropriate to $T$.\nLet $F = \\map T f$ and $G = \\map T g$.\nLet $T$ have a unique inverse $T^{-1}$.\nThen $T^{-1}$ is a linear operator:\n:$\\forall p, q \\in \\R: \\map {T^{-1} } {p F + q G} = p \\map {T^{-1} } F + q \\map {T^{-1} } G$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$x_1 = \\map {T^{-1} } F$\n:$x_2 = \\map {T^{-1} } G$\nThus:\n:$F = \\map T {x_1}$\n:$G = \\map T {x_2}$\nThen for all $p, q \\in \\R$:\n{{begin-eqn}}\n{{eqn | l = \\map T {p x_1 + q x_2}\n      | r = p \\map T {x_1} + q \\map T {x_2}\n      | c = Integral Operator is Linear\n}}\n{{eqn | r = p F + q G\n      | c = \n}}\n{{end-eqn}}\nand so $x = p F + q G$ is a solution to the equation:\n:$\\map T x = p F + q G$\nBut this equation has only one solution:\n:$x = \\map {T^{-1} } {p F + q G}$\nThus $p F + q G$ must coincide with the above:\n:$p \\map {T^{-1} } F + q \\map {T^{-1} } G = \\map {T^{-1} } {p F + q G}$\nwhich proves that $T^{-1}$ is a linear operator.\n{{qed}}\n\\end{proof}\n\n"}}, "13807": {"score": 0.8689238429069519, "content": {"text": "\\section{Exponential of Sum/Complex Numbers}\nTags: Exponential Function, Exponential of Sum, Exponent of Sum\n\n\\begin{theorem}\nLet $z_1, z_2 \\in \\C$ be complex numbers.\nLet $\\exp z$ be the exponential of $z$.\nThen:\n:$\\map \\exp {z_1 + z_2} = \\paren {\\exp z_1} \\paren {\\exp z_2}$\n\\end{theorem}\n\n\\begin{proof}\nThis proof is based on the definition of the complex exponential as the unique solution of the differential equation:\n:$\\dfrac \\d {\\d z} \\exp = \\exp$\nwhich satisfies the initial condition $\\map \\exp 0 = 1$.\nDefine the complex function  $f: \\C \\to \\C$ by:\n:$\\map f z = \\map \\exp z \\, \\map \\exp {z_1 + z_2 - z}$\nThen find its derivative:\n{{begin-eqn}}\n{{eqn | l = D_z \\, \\map f z\n      | r = \\paren {D_z \\, \\map \\exp z} \\map \\exp {z_1 + z_2 - z} + \\map \\exp z \\paren {D_z \\map \\exp {z_1 + z_2 - z} }\n      | c = Derivative of Complex Composite Function\n}}\n{{eqn | r = \\map \\exp z \\, \\map \\exp {z_1 + z_2 - z} + \\map \\exp z \\, \\map \\exp {z_1 + z_2 - z} \\map {D_z} {z_1 + z_2 - z}\n      | c = as $\\exp$ is its own derivative\n}}\n{{eqn | r = \\map \\exp z \\, \\map \\exp {z_1 + z_2 - z} - \\map \\exp z \\, \\map \\exp {z_1 + z_2 - z}\n      | c = Derivative of Complex Power Series\n}}\n{{eqn | r = 0\n}}\n{{end-eqn}}\nFrom Zero Derivative implies Constant Complex Function, it follows that $f$ is constant.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map \\exp {z_1} \\, \\map \\exp {z_2}\n      | r = \\map f {z_1}\n}}\n{{eqn | r = \\map f 0\n      | c = as $f$ is constant\n}}\n{{eqn | r = \\map \\exp 0 \\, \\map \\exp {z_1 + z_2}\n}}\n{{eqn | r = \\map \\exp {z_1 + z_2}\n      | c = as $\\map \\exp 0 = 1$ \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "5781": {"score": 0.8615956902503967, "content": {"text": "\\begin{definition}[Definition:Modulus of Complex-Valued Function]\nLet $f: S \\to \\C$ be a complex-valued function.\nThen the '''(complex) modulus of $f$''' is written $\\left|{f}\\right|: S \\to \\R$ and is the real-valued function defined as:\n:$\\forall z \\in S: \\left|{f}\\right| \\left({z}\\right) = \\left|{f \\left({z}\\right)}\\right|$.\n\\end{definition}"}}, "15822": {"score": 0.8621901273727417, "content": {"text": "\\section{Conjugate of Polynomial is Polynomial of Conjugate}\nTags: Complex Conjugates, Polynomial Theory\n\n\\begin{theorem}\nLet $\\map f z = a_n z^n + a_{n - 1} z^{n - 1} + \\cdots + a_1 z + a_0$ be a polynomial over complex numbers where $a_0, \\ldots, a_n$ are real numbers.\nLet $\\alpha \\in \\C$ be a complex number.\nThen:\n:$\\overline {\\map f \\alpha} = \\map f {\\overline \\alpha}$\nwhere $\\overline \\alpha$ denotes the complex conjugate of $\\alpha$.\n\\end{theorem}\n\n\\begin{proof}\nBy Power of Complex Conjugate is Complex Conjugate of Power:\n:$\\overline {\\alpha^k} = \\paren {\\overline \\alpha}^k$\nfor all $k$ between $0$ and $n$.\nThen from Product of Complex Conjugates:\n:$\\overline {a_k \\alpha^k} = \\overline {a_k} \\cdot \\overline {\\alpha^k}$\nBut $a_k$ is real.\nSo by Complex Number equals Conjugate iff Wholly Real:\n:$\\overline {a_k} = a_k$.\nFrom Sum of Complex Conjugates, it follows that:\n:$\\overline {\\map f \\alpha} = a_n \\paren {\\overline \\alpha}^n + a_{n - 1} \\paren {\\overline \\alpha}^{n - 1} + \\cdots + a_1 \\overline \\alpha + a_0$\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Stirling_number_first_kind_5.json": {"gold": {"15792": 1}, "retrieved": {"21803": {"score": 0.8326810598373413, "content": {"text": "\\section{Stirling Number of the Second Kind of Number with Self}\nTags: Stirling Numbers\n\n\\begin{theorem}\n:$\\ds {n \\brace n} = 1$\nwhere $\\ds {n \\brace n}$ denotes a Stirling number of the second kind.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction.\nFor all $n \\in \\N_{> 0}$, let $\\map P n$ be the proposition:\n:$\\ds {n \\brace n} = 1$\n\\end{proof}\n\n"}}, "17941": {"score": 0.8334463834762573, "content": {"text": "\\section{Number of Permutations}\nTags: Permutations, Permutation Theory, Number of Permutations, Combinatorics\n\n\\begin{theorem}\nLet $S$ be a set of $n$ elements.\nLet $r \\in \\N: r \\le n$.\nThen the number of $r$-permutations of $S$ is:\n:${}^r P_n = \\dfrac {n!} {\\paren {n - r}!}$\nWhen $r = n$, this becomes:\n:${}^n P_n = \\dfrac {n!} {\\paren {n - n}!} = n!$\nUsing the falling factorial symbol, this can also be expressed:\n:${}^r P_n = n^{\\underline r}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition, an <math>r</math>-permutations of <math>S</math> is an ordered selection of <math>r</math> elements of <math>S</math>.\nIt can be seen that an <math>r</math>-permutation is an injection from a subset of <math>S</math> into <math>S</math>.\nFrom Cardinality of Set of Injections, we see that the number of <math>r</math>-permutations <math>{}^r P_n</math> on a set of <math>n</math> elements is given by:\n:<math>{}^r P_n = \\frac {n!} {\\left({n-r}\\right)!}</math>\nFrom this definition, it can be seen that a bijection <math>f: S \\to S</math> (as defined above) is an '''<math>n</math>-permutation'''.\nHence the number of <math>r</math>-permutations on a set of <math>n</math> elements is <math>{}^n P_n = \\frac {n!} {\\left({n-n}\\right)!} = n!</math>.\n{{Qed}}\nCategory:Combinatorics\n24405\n24403\n2010-01-14T06:55:12Z\nPrime.mover\n59\n24405\nwikitext\ntext/x-wiki\n\\end{proof}\n\n"}}, "17192": {"score": 0.8378458023071289, "content": {"text": "\\section{Binomial Coefficient/Examples/Number of Bridge Hands}\nTags: Binomial Coefficients, Examples of Binomial Coefficients\n\n\\begin{theorem}\nThe total number $N$ of possible different hands for a game of [https://en.wikipedia.org/wiki/Contract_bridge bridge] is:\n:$N = \\dfrac {52!} {13! \\, 39!} = 635 \\ 013 \\ 559 \\ 600$\n\\end{theorem}\n\n\\begin{proof}\nThe total number of cards in a standard deck is $52$.\nThe number of cards in a single bridge hand is $13$.\nThus $N$ is equal to the number of ways $13$ things can be chosen from $52$.\nThus:\n{{begin-eqn}}\n{{eqn | l = N\n      | r = \\dbinom {52} {23}\n      | c = Cardinality of Set of Subsets\n}}\n{{eqn | r = \\frac {52!} {13! \\left({52 - 13}\\right)!}\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\frac {52!} {13! \\, 39!}\n      | c = \n}}\n{{eqn | r = 635 \\ 013 \\ 559 \\ 600\n      | c = after calculation\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "17952": {"score": 0.8368983864784241, "content": {"text": "\\section{Number of k-Cycles in Symmetric Group}\nTags: Symmetric Groups, Cyclic Permutations, Permutation Theory, Number of k-Cycles in Symmetric Group, Combinatorics\n\n\\begin{theorem}\nLet $n \\in \\N$ be a natural number.\nLet $S_n$ denote the symmetric group on $n$ letters.\nLet $k \\in N$ such that $k \\le n$.\nThe number of elements $m$ of $S_n$ which are $k$-cycles is given by:\n:$m = \\paren {k - 1}! \\dbinom n k = \\dfrac {n!} {k \\paren {n - k}!}$\n\\end{theorem}\n\n\\begin{proof}\nLet $m$ be the number of elements of $S_n$ which are $k$-cycles.\nFrom Cardinality of Set of Subsets, there are $\\dfrac {n!} {k! \\paren {n - k}!}$ different ways to select $k$ elements of $\\set {1, 2, \\ldots, n}$.\nFrom Number of k-Cycles on Set of k Elements, each of these $\\dfrac {n!} {k! \\paren {n - k}!}$ sets with $k$ elements has $\\paren {k - 1}!$ $k$-cycles.\nIt follows from Product Rule for Counting that:\n{{begin-eqn}}\n{{eqn | l = m\n      | r = \\paren {k - 1}! \\dfrac {n!} {k! \\paren {n - k}!}\n      | c = \n}}\n{{eqn | r = \\paren {k - 1}! \\dbinom n k\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\dfrac {n! \\paren {k - 1}! } {k! \\paren {n - k}!}\n      | c = \n}}\n{{eqn | r = \\dfrac {n!} {k \\paren {n - k}!}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Cyclic Permutations\nCategory:Symmetric Groups\nCategory:Combinatorics\n431841\n431839\n2019-10-21T07:26:09Z\nPrime.mover\n59\n431841\nwikitext\ntext/x-wiki\n{{mergeto|Number of m-Cycles in Symmetric Group}}\n\\end{proof}\n\n"}}, "16531": {"score": 0.8337088823318481, "content": {"text": "\\section{Closed Form for Number of Derangements on Finite Set}\nTags: Counting Arguments, Number of Derangements on Finite Set, Combinatorics, Closed Forms\n\n\\begin{theorem}\nThe number of derangements $D_n$ on a finite set $S$ of cardinality $n$ is:\n{{begin-eqn}}\n{{eqn | q = \n      | l = D_n\n      | r = n! \\paren {1 - \\dfrac 1 {1!} + \\dfrac 1 {2!} - \\dfrac 1 {3!} + \\cdots + \\dfrac {\\paren {-1}^n} {n!} }\n      | c = \n}}\n{{eqn | r = !n\n      | c = where $!n$ denotes the subfactorial of $n$\n}}\n{{end-eqn}}\n\\end{theorem}\n\n\\begin{proof}\nLet $s_i$ be the $i$th element of set $S$.\nBegin by defining set $A_m$, which is all of the permutations of $S$ which fixes $S_m$.\nThen the number of permutations, $W$, with ''at least'' one element fixed, $m$, is:\n:$\\displaystyle W = \\size {\\bigcup_{m \\mathop = 1}^n A_m}$\nApplying the Inclusion-Exclusion Principle:\n{{begin-eqn}}\n{{eqn | l = W\n      | r = \\sum_{m_1 \\mathop = 1}^n \\size {A_{m_1} }\n      | c = \n}}\n{{eqn | o = \n      | ro= -\n      | r = \\sum_{m_1, m_2 : 1 \\mathop \\le m_1 \\mathop < m_2 \\mathop \\le n} \\size {A_{m_1} \\cap A_{m_2} }\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\sum_{m_1, m_2, m_3 : 1 \\mathop \\le m_1 \\mathop < m_2 \\mathop < m_3 \\mathop \\le n} \\size {A_{m_1} \\cap A_{m_2} \\cap A_{m_3} }\n      | c = \n}}\n{{eqn | o = \n      | ro= -\n      | r = \\cdots\n      | c = \n}}\n{{end-eqn}}\nEach value $A_{m_1} \\cap \\cdots \\cap A_{m_p}$ represents the set of permutations which fix $p$ values $m_1, \\ldots, m_p$.\nNote that the number of permutations which fix $p$ values only depends on $p$, not on the particular values of $m$.\nThus from Cardinality of Set of Subsets there are $\\dbinom n p$ terms in each summation. \nSo:\n{{begin-eqn}}\n{{eqn | l = W\n      | r = \\binom n 1 \\size {A_1}\n      | c = \n}}\n{{eqn | o = \n      | ro= -\n      | r = \\binom n 2 \\size {A_1 \\cap A_2}\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\binom n 3 \\size {A_1 \\cap A_2 \\cap A_3}\n      | c = \n}}\n{{eqn | o = \n      | ro= -\n      | r = \\cdots\n      | c = \n}}\n{{eqn | o = \n      | ro= +\n      | r = \\paren {-1}^{p - 1} \\binom n p \\size {A_1 \\cap \\cdots \\cap A_p}\n      | c = \n}}\n{{eqn | o = \n      | r = \\cdots\n      | c = \n}}\n{{end-eqn}}\n$\\size {A_1 \\cap \\cdots \\cap A_p}$ is the number of permutations fixing $p$ elements in position.\nThis is equal to the number of permutations which rearrange the remaining $n - p$ elements, which is $\\paren {n - p}!$.\nThus we finally get:  \n:$W = \\dbinom n 1 \\paren {n - 1}! - \\dbinom n 2 \\paren {n - 2}! + \\dbinom n 3 \\paren {n - 3}! - \\cdots + \\paren {-1}^{p - 1} \\dbinom n p \\paren {n - p}! \\cdots$\nThat is:\n:$\\displaystyle W = \\sum_{p \\mathop = 1}^n \\paren {-1}^{p - 1} \\binom n p \\paren {n - p}!$\nNoting that $\\dbinom n p = \\dfrac {n!} {p! \\paren {n - p}!}$, this reduces to:\n:$\\displaystyle W = \\sum_{p \\mathop = 1}^n \\paren {-1}^{p - 1} \\dfrac {n!} {p!}$ \n{{qed}}\n\\end{proof}\n\n"}}, "17948": {"score": 0.8521289825439453, "content": {"text": "\\section{Number of Set Partitions by Number of Components}\nTags: Set Partitions, Number of Set Partitions, Stirling Numbers, Combinatorics, Number of Set Partitions by Number of Components\n\n\\begin{theorem}\nLet $S$ be a (finite) set whose cardinality is $n$.\nLet $\\map f {n, k}$ denote the number of different ways $S$ can be partitioned into $k$ components.\nThen:\n:$\\ds \\map f {n, k} = {n \\brace k}$\nwhere $\\ds {n \\brace k}$ denotes a Stirling number of the second kind.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction on $n$.\nFor all $n \\in \\Z_{\\ge 0}$, let $\\map P n$ be the proposition:\n:$\\ds \\map f {n, k} = {n \\brace k}$\n$\\map P 0$ is the degenerate case:\n:$\\ds \\map f {0, k} = \\delta_{0 k} = {0 \\brace k}$\nThat is: the empty set can be partitioned one and only one way: into $0$ subsets.\nThus $\\map P 0$ is seen to hold.\nThe remainder of the proof considers $n \\in \\Z_{> 0}$.\nFirst we note that when $k < 1$ or $k > n$:\n:$\\ds \\map f {n, k} = 0 = {n \\brace k}$\nHence, throughout, we consider only such $k$ as $1 \\le k \\le n$.\nWe define the representative set of cardinality $n$ to be:\n:$S_n := \\set {1, 2, \\ldots, n}$\n\\end{proof}\n\n"}}, "2271": {"score": 0.8437409996986389, "content": {"text": "\\begin{definition}[Definition:Derangement/Historical Note]\nThe number of a derangements of a finite set was first investigated by {{AuthorRef|Nicolaus I Bernoulli}} and {{AuthorRef|Pierre Raymond de Montmort}} between about $1708$ and $1713$.\nThey solved it at around the same time.\nThe question is often couched in the idea of counting the number of ways of placing letters at random in envelopes such that nobody receives the correct letter.\n\\end{definition}"}}, "15526": {"score": 0.8584979176521301, "content": {"text": "\\section{Count of All Permutations on n Objects}\nTags: Permutation Theory, Count of All Permutations on n Objects\n\n\\begin{theorem}\nLet $S$ be a set of $n$ objects.\nLet $N$ be the number of permutations of $r$ objects from $S$, where $1 \\le r \\le N$.\nThen:\n:$\\ds N = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}$\n\\end{theorem}\n\n\\begin{proof}\nThe number of permutations on $k$ objects, from $n$ is denoted ${}^k P_{10}$.\nFrom Number of Permutations:\n:${}^k P_n = \\dfrac {n!} {\\paren {n - k}!}$\nHence:\n{{begin-eqn}}\n{{eqn | q = \n      | l = N\n      | r = \\sum_{k \\mathop = 1}^n {}^k P_n\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {\\paren {n - k}!}\n      | c = \n}}\n{{eqn | r = \\sum_{k \\mathop = 1}^n \\dfrac {n!} {k!}\n      | c = \n}}\n{{eqn | r = n! \\sum_{k \\mathop = 1}^n \\dfrac 1 {k!}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Permutation Theory\nCategory:Count of All Permutations on n Objects\n\\end{proof}\n\n"}}, "16939": {"score": 0.8437834978103638, "content": {"text": "\\section{Cardinality of Set of Combinations with Repetition}\nTags: Combinations with Repetition\n\n\\begin{theorem}\nLet $S$ be a finite set with $n$ elements\nThe number of $k$-combinations of $S$ with repetition is given by:\n:$N = \\dbinom {n + k - 1} k$\n\\end{theorem}\n\n\\begin{proof}\nLet the elements of $S$ be (totally) ordered in some way, by assigning an index to each element.\nThus let $S = \\left\\{ {a_1, a_2, a_3, \\ldots, a_n}\\right\\}$.\nThus each $k$-combination of $S$ with repetition can be expressed as:\n:$\\left\\{ {\\left\\{ {a_{r_1}, a_{r_1}, \\ldots, a_{r_k} }\\right\\} }\\right\\}$\nwhere:\n:$r_1, r_2, \\ldots, r_k$ are all elements of $\\left\\{ {1, 2, \\ldots, n}\\right\\}$\nand such that:\n:$r_1 \\le r_2 \\le \\cdots \\le r_k$\nHence the problem reduces to the number of integer solutions $\\left({r_1, r_2, \\ldots, r_k}\\right)$ such that $1 \\le r_1 \\le r_2 \\le \\cdots \\le r_k \\le n$.\nThis is the same as the number of solutions to:\n:$0 < r_1 < r_2 + 1 < \\cdots < r_k + k - 1 < n + k$\nwhich is the same as the number of solutions to:\n:$0 < s_1 < s_2 < \\cdots < s_k < n + k$\nwhich is the number of ways to choose $k$ elements from $n + k - 1$.\nThis is the same as the number of subsets as a set with $n + k - 1$ elements.\nFrom Cardinality of Set of Subsets:\n:$N = \\dbinom {n + k - 1} k$\n{{qed}}\n\\end{proof}\n\n"}}, "17224": {"score": 0.846577525138855, "content": {"text": "\\section{Birthday Paradox/General/3}\nTags: Birthday Paradox\n\n\\begin{theorem}\nLet $n$ be a set of people.\nLet the probability that at least $3$ of them have the same birthday be greater than $50 \\%$.\nThen $n \\ge 88$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map F {r, n}$ be the number of ways to distribute $r$ objects into $n$ cells such that there are no more than $2$ objects in each cell.\nLet there be $d$ cells which are each occupied by $2$ objects.\nThese can be chosen in $\\dbinom n d$ ways.\nThere remain $s = r - 2 d$ objects which can then be distributed among $n - d$ cells in $\\dbinom {n - d} s$ ways.\nIn each such arrangement, the $r$ objects may be permuted in:\n:$\\dbinom r 2 \\dbinom {r - 2} 2 \\cdots \\dbinom {r - 2 d + 2} 2 \\paren {r - 2 d}! = \\dfrac {r!} {2^d}$\ndifferent ways.\nHence:\n:$\\map F {r, n} = \\dbinom n d \\dbinom {n - d} s \\dfrac {r!} {2^d}$\nSo the probability of exactly $d$ pairs and $s$ singletons, where $d - s \\le n$, is given by:\n:$\\dfrac {\\map F {r, n} } {n^r}$\nIf we assume a $365$-day year, we have that the probability that at least $3$ of them have the same birthday is given by:\n:$\\map \\Pr r = 1 - \\ds \\sum_{d \\mathop = 0}^{\\floor {r / 2} } \\dfrac {n! \\, r!} {n^r 2^d d! \\paren {r - 2 d}! \\paren {n + d - r}!}$\nwhere $n = 365$.\nWe require the smallest $r$ for which $\\map \\Pr r > \\dfrac 1 2$.\nThe result yields to calculation.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/L'H\u00f4pital_rule1.json": {"gold": {"15118": 1, "11592": 1}, "retrieved": {"11592": {"score": 0.8946660161018372, "content": {"text": "\\section{L'H\u00f4pital's Rule}\nTags: Calculus, limits, derivatives, infinity, zero, Calculus, Limits, L'H\u00f4pital's Rule, Limits of Real Functions, Limits of Functions, Differential Calculus, Named Theorems, Calculus, Limits\n\n\\begin{theorem}\nLet $f$ and $g$ be real functions which are continuous on the closed interval $\\closedint a b$ and differentiable on the open interval $\\openint a b$.\nLet:\n:$\\forall x \\in \\openint a b: \\map {g'} x \\ne 0$\nwhere $g'$ denotes the derivative of $g$ {{WRT|Differentiation}} $x$.\nLet:\n:$\\map f a = \\map g a = 0$\nThen:\n:$\\ds \\lim_{x \\mathop \\to a^+} \\frac {\\map f x} {\\map g x} = \\lim_{x \\mathop \\to a^+} \\frac {\\map {f'} x} {\\map {g'} x}$\nprovided that the second limit exists.\n\\end{theorem}\n\n\\begin{proof}\nLet $l = \\displaystyle \\lim_{x \\to a^+} {f' \\left({x}\\right)} {g' \\left({x}\\right)}$.\nLet $\\epsilon > 0$.\nBy the definition of limit, we ought to find a $\\delta > 0$ such that:\n:$\\forall x: \\left\\vert{x - a}\\right\\vert < \\delta \\implies \\left\\vert{ \\dfrac {f \\left({x}\\right)} {g \\left({x}\\right)} - l }\\right\\vert < \\epsilon$\nFix $\\delta$ such that:\n:$\\forall x: \\left\\vert{x - a}\\right\\vert < \\delta \\implies \\left\\vert{ \\dfrac {f' \\left({x}\\right)} {g' \\left({x}\\right)} - l }\\right\\vert < \\epsilon$\nwhich is possible by the definition of limit.\nLet $x$ be such that $\\left\\vert{x - a}\\right\\vert < \\delta$.\nBy the Cauchy Mean Value Theorem with $b = x$:\n: $\\exists \\xi \\in \\left({a \\,.\\,.\\, x}\\right): \\dfrac {f' \\left({\\xi}\\right)} {g' \\left({\\xi}\\right)} = \\dfrac {f \\left({x}\\right) - f \\left({a}\\right)} {g \\left({x}\\right) - g \\left({a}\\right)}$\nSince $f \\left({a}\\right) = g \\left({a}\\right) = 0$, we have:\n: $\\exists \\xi \\in \\left({a \\,.\\,.\\, x}\\right): \\dfrac {f' \\left({\\xi}\\right)} {g' \\left({\\xi}\\right)} = \\dfrac {f \\left({x}\\right)} {g \\left({x}\\right)}$\nNow, as $a < \\xi < x$, it follows that $\\left\\vert{\\xi - a}\\right\\vert < \\delta$ as well.\nTherefore:\n:$\\left\\vert{ \\dfrac {f \\left({x}\\right)} {g \\left({x}\\right)} - l }\\right\\vert = \\left\\vert{ \\dfrac {f' \\left({\\xi}\\right)} {g' \\left({\\xi}\\right)} - l }\\right\\vert < \\epsilon$\nwhich leads us to the desired conclusion that:\n:$\\displaystyle \\lim_{x \\to a^+} \\frac {f \\left({x}\\right)} {g \\left({x}\\right)} = \\lim_{x \\to a^+} \\frac {f^{\\prime} \\left({x}\\right)} {g^{\\prime} \\left({x}\\right)}$\n{{qed}}\n{{namedfor|Guillaume de l'H\u00f4pital|cat=L'H\u00f4pital}}\nHowever, this result was in fact discovered by Johann Bernoulli.\nBecause of variants in the rendition of his name, this proof is often seen written as '''L'Hospital's Rule'''.\n\\end{proof}\n\n"}}, "11210": {"score": 0.8957096338272095, "content": {"text": "\\section{Limit of Power of x by Absolute Value of Power of Logarithm of x}\nTags: Limits of Real Functions, Logarithms, Limit of Power of x by Absolute Value of Power of Logarithm of x\n\n\\begin{theorem}\nLet $\\alpha$ and $\\beta$ be positive real numbers. \nThen: \n:$\\ds \\lim_{x \\mathop \\to 0^+} x^\\alpha \\size {\\ln x}^\\beta = 0$\n\\end{theorem}\n\n\\begin{proof}\nFrom Order of Natural Logarithm Function, we have: \n:$\\ln x = \\map \\OO {x^{-\\frac \\alpha {2 \\beta} } }$ as $x \\to 0^+$\nThat is, by the definition of big-O notation there exists positive real numbers $x_0$ and $C$ such that: \n:$0 \\le \\size {\\ln x} \\le C x^{-\\frac \\alpha {2 \\beta} }$\nfor $0 < x \\le x_0$.\nSo:\n:$0 \\le \\size {\\ln x}^\\beta \\le C^\\beta x^{-\\alpha/2}$\nfor $0 < x \\le x_0$. \nThat is: \n:$0 \\le x^\\alpha \\size {\\ln x}^\\beta \\le C^\\beta x^{\\alpha/2}$\nWe have that: \n:$\\ds \\lim_{x \\mathop \\to 0^+} C^\\beta x^{\\alpha/2} = 0$\nso by the squeeze theorem for functions:\n:$\\ds \\lim_{x \\mathop \\to 0^+} x^\\alpha \\size {\\ln x}^\\beta = 0$\n{{qed}}\nCategory:Logarithms\nCategory:Limits of Real Functions\nCategory:Limit of Power of x by Absolute Value of Power of Logarithm of x\n\\end{proof}\n\n"}}, "18855": {"score": 0.8978346586227417, "content": {"text": "\\section{Power Function on Base between Zero and One Tends to One as Power Tends to Zero/Rational Number}\nTags:  Real Analysis, Powers\n\n\\begin{theorem}\nLet $a \\in \\R_{> 0}$ be a strictly positive real number such that $0 < a < 1$.\nLet $f: \\Q \\to \\R$ be the real-valued function defined as:\n:$\\map f r = a^r$\nwhere $a^r$ denotes $a$ to the power of $r$.\nThen:\n:$\\ds \\lim_{r \\mathop \\to 0} \\map f r = 1$\n\\end{theorem}\n\n\\begin{proof}\nFrom Ordering of Reciprocals:\n:$0 < a < 1 \\implies 1 < \\dfrac 1 a$\nSo:\n{{begin-eqn}}\n{{eqn | l = \\lim_{r \\mathop \\to 0} \\paren {\\frac 1 a}^r\n      | r = 1\n      | c = Power Function on Base greater than One tends to One as Power tends to Zero: Rational Number\n}}\n{{eqn | ll= \\leadsto\n      | l = \\lim_{r \\mathop \\to 0} \\frac 1 {a^r}\n      | r = 1\n      | c = Power of Quotient: Rational Numbers\n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac 1 {\\ds \\lim_{r \\mathop \\to 0} a^r}\n      | r = 1\n      | c = Quotient Rule for Limits of Real Functions\n}}\n{{eqn | ll= \\leadsto\n      | l = \\lim_{r \\mathop \\to 0} a^r \n      | r = 1\n      | c = taking reciprocal of each side\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Powers\n\\end{proof}\n\n"}}, "11062": {"score": 0.8974399566650391, "content": {"text": "\\section{Logarithm Tends to Negative Infinity}\nTags: Logarithms\n\n\\begin{theorem}\nLet $x \\in \\R$ be a real number such that $x > 0$.\nLet $\\ln x$ be the natural logarithm of $x$.\nThen:\n:$\\ln x \\to -\\infty$ as $x \\to 0^+$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of natural logarithm:\n{{begin-eqn}}\n{{eqn | l = \\ln x\n      | r = \\int_1^x \\dfrac 1 t \\ \\mathrm dt\n}}\n{{end-eqn}}\nThe result follows from Integral of Reciprocal is Divergent.\n{{qed}}\n\\end{proof}\n\n"}}, "15118": {"score": 0.8958736658096313, "content": {"text": "\\section{Derivative of Logarithm at One}\nTags: Differential Calculus, Derivatives, Logarithms, Derivative of Logarithm at One\n\n\\begin{theorem}\nLet $\\ln x$ be the natural logarithm of $x$ for real $x$ where $x > 0$.\nThen:\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\map \\ln {1 + x} } x = 1$\n\\end{theorem}\n\n\\begin{proof}\nL'H\u00f4pital's rule gives:\n:$\\displaystyle \\lim_{x \\to c} \\frac {f \\left({x}\\right)} {g \\left({x}\\right)} = \\lim_{x \\to c} \\frac {f^{\\prime} \\left({x}\\right)} {g^{\\prime} \\left({x}\\right)}$\n(provided the appropriate conditions are fulfilled).\nHere we have:\n* $\\ln \\left({1 + 0}\\right) = 0$\n* $D_x \\left({\\ln \\left({1 + x}\\right)}\\right) = \\dfrac 1 {1 + x}$ from the Chain Rule\n* $D_x x = 1$ from Differentiation of the Identity Function.\nThus:\n:$\\displaystyle \\lim_{x \\to 0} \\frac {\\ln \\left({1 + x}\\right)} {x} = \\lim_{x \\to 0} \\frac {\\left({1 + x}\\right)^{-1}} {1} = \\frac 1 {1 + 0} = 1$\n{{qed}}\n\\end{proof}\n\n"}}, "11318": {"score": 0.9125062227249146, "content": {"text": "\\section{Limit of (Cosine (X) - 1) over X at Zero}\nTags: Cosine Function, Limits of Real Functions, Limit of (Cosine (X) - 1) over X, Limits of Functions, Analysis, Differential Calculus, Examples of Limits of Real Functions, Limit of (Cosine (X) - 1) over X at Zero\n\n\\begin{theorem}\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac {\\cos x - 1} x = 0$\n\\end{theorem}\n\n\\begin{proof}\nThis proof works directly from the definition of the cosine function:\n{{begin-eqn}}\n{{eqn | l=\\cos x\n      | r=\\sum_{n=0}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n      | c=By the definition of the cosine function\n}}\n{{eqn | r=(-1)^0 \\cdot \\frac{x^{2\\cdot0} }{(2\\cdot0)!}+\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n}}\n{{eqn | r=1 + \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!}\n      | c=From the definition of $0!$ and the definition of $a^0$\n}}\n{{end-eqn}}\n{{begin-eqn}}\n{{eqn | l=\\lim_{x \\to 0} \\ \\frac{\\cos (x) - 1} x\n      | r=\\lim_{x \\to 0} \\ \\frac{1 + \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!} - 1} x\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\frac{\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n} }{\\left({2n}\\right)!} } x\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\frac{\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n-1} } {\\left({2n-1}\\right)!} } 1\n      | c=by Power Series Differentiable on Interval of Convergence and L'H\u00f4pital's Rule\n}}\n{{eqn | r=\\lim_{x \\to 0} \\ \\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {x^{2n-1} }{\\left({2n-1}\\right)!}\n}}\n{{eqn | r=\\sum_{n=1}^\\infty \\left({-1}\\right)^n \\frac {0^{2n-1} }{\\left({2n-1}\\right)!}\n      | c=by Polynomial is Continuous\n}}\n{{eqn | r=0\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "18860": {"score": 0.8980262279510498, "content": {"text": "\\section{Power Function on Base greater than One tends to One as Power tends to Zero/Rational Number}\nTags:  Real Analysis, Real Analysis, Powers\n\n\\begin{theorem}\nLet $a \\in \\R_{> 0}$ be a strictly positive real number such that $a > 1$.\nLet $f: \\Q \\to \\R$ be the real-valued function defined as:\n:$\\map f r = a^r$\nwhere $a^r$ denotes $a$ to the power of $r$.\nThen:\n:$\\ds \\lim_{r \\mathop \\to 0} \\map f r = 1$\n\\end{theorem}\n\n\\begin{proof}\nWe start by treating the right-sided limit.\nLet $0 < r < 1$.\n\\end{proof}\n\n"}}, "11216": {"score": 0.9191908240318298, "content": {"text": "\\section{Limit of Sine of X over X at Zero/Corollary}\nTags: Sine Function, Analysis, Differential Calculus, Limit of Sine of X over X, Limit of Sine of X over X at Zero\n\n\\begin{theorem}\n:$\\ds \\lim_{x \\mathop \\to 0} \\frac x {\\sin x} = 1$\n\\end{theorem}\n\n\\begin{proof}\nWe have the inequality:\n:$1 \\le \\dfrac \\theta {\\sin \\theta} \\le \\dfrac 1 {\\cos \\theta}$\nfor all $\\theta \\in \\openint {-\\dfrac \\pi 2} 0 \\cup \\openint 0 {\\dfrac \\pi 2}$.\n{{explain|Find a link for this}}\nTaking the limit of the leftmost term and the rightmost term:\n:$\\ds \\lim_{\\theta \\mathop \\to 0} \\ 1 = 1$\n:$\\ds \\lim_{\\theta \\mathop \\to 0} \\frac 1 {\\cos \\theta} = 1$\nSo by the Squeeze Theorem:\n:$\\ds \\lim_{\\theta \\mathop \\to 0} \\frac \\theta {\\sin \\theta} = 1$\n{{qed}}\nCategory:Differential Calculus\nCategory:Sine Function\nCategory:Limit of Sine of X over X at Zero\n\\end{proof}\n\n"}}, "18991": {"score": 0.8995175361633301, "content": {"text": "\\section{Powers Drown Logarithms/Corollary}\nTags: Powers, Logarithms\n\n\\begin{theorem}\nLet $r \\in \\R_{>0}$ be a (strictly) positive real number.\nThen:\n:$\\ds \\lim_{y \\mathop \\to 0_+} y^r \\ln y = 0$\n\\end{theorem}\n\n\\begin{proof}\nPut $y = \\dfrac 1 x$ in the Powers Drown Logarithms.\n{{qed}}\n\\end{proof}\n\n"}}, "11225": {"score": 0.9090073704719543, "content": {"text": "\\section{Limit to Infinity of Power of x by Exponential of -a x}\nTags: Exponential Function\n\n\\begin{theorem}\nLet $k$ and $a$ be positive real numbers. \nThen: \n:$\\ds \\lim_{x \\mathop \\to \\infty} x^k e^{-a x} = 0$\n\\end{theorem}\n\n\\begin{proof}\nBy Power Series Expansion for Exponential Function, we have: \n:$\\ds e^{a x} = \\sum_{n \\mathop = 0}^\\infty \\frac {\\paren {a x}^n} {n!}$\nSince for $x > 0$ each term in this sum is non-negative, we have: \n:$\\ds e^{a x} \\ge \\frac {\\paren {a x}^{\\floor k + 1} } {\\paren {\\floor k + 1}!}$\nfor each $k$.\nSo, for each $x > 0$ we have: \n:$\\ds 0 < e^{-a x} \\le \\frac {\\paren {\\floor k + 1}!} {\\paren {a x}^{\\floor k + 1} }$\nSo that: \n:$\\ds 0 \\le x^k e^{-a x} \\le \\frac 1 {a^{\\floor k + 1} } x^{k - \\floor k - 1} \\paren {\\floor k + 1}!$\nFrom the definition of the floor function, we have: \n:$0 \\le k - \\floor k < 1$\nso:\n:$k - \\floor k - 1 < 0$\nHence by Limit to Infinity of Power:\n:$\\ds \\lim_{x \\mathop \\to \\infty} \\frac 1 {a^{\\floor k + 1} } x^{k - \\floor k - 1} \\paren {\\floor k + 1}! = 0$\nSo, by the Squeeze Theorem, we have: \n:$\\ds \\lim_{x \\mathop \\to \\infty} x^k e^{-a x} = 0$\n{{qed}}\nCategory:Exponential Function\n\\end{proof}\n\n"}}}}, "TheoremQA_panlu/gravitational_force2.json": {"gold": {"5249": 1, "3830": 1}, "retrieved": {"10100": {"score": 0.8170670866966248, "content": {"text": "\\begin{definition}[Definition:Weight (Physics)/Warning]\nThere is a certain amount of confusion in the common mind between weight and mass.\nThe latter is usually determined by measuring its weight.\nBut while the mass of a body is (under normal circumstances) constant, its weight varies according to its position relative to the gravitational field it is in, and so is not a constant property of that body.\nHowever, under usual terrestrial conditions the gravitational field is more or less constant (any differences being detectable only by instruments).\nThis means that the weight and mass of a body are commonly considered \"the same\".\nThus a weighing machine, while indicating the mass of a body, does so by measuring its weight.\n\\end{definition}"}}, "10098": {"score": 0.8266358971595764, "content": {"text": "\\begin{definition}[Definition:Weight (Physics)]\nThe '''weight''' of a body is the magnitude of the force exerted on it by the influence of a gravitational field.\nThe context is that the gravitational field in question is usually that of the Earth.\n\\end{definition}"}}, "23422": {"score": 0.8579269051551819, "content": {"text": "\\section{Weight of Body at Earth's Surface}\nTags: Weight (Physics), Weight\n\n\\begin{theorem}\nLet $B$ be a body of mass $m$ situated at (or near) the surface of Earth.\nThen the weight of $B$ is given by:\n:$W = m g$\nwhere $g$ is the value of the acceleration due to gravity at the surface of Earth.\n\\end{theorem}\n\n\\begin{proof}\nThe weight of $B$ is the magnitude of the force exerted on it by the influence of the gravitational field it is in.\nBy Newton's Second Law of Motion, that force is given by:\n:$\\mathbf W = -m g \\mathbf k$\nwhere:\n:$g$ is the value of the acceleration due to gravity at the surface of Earth\n:$\\mathbf k$ is a unit vector directed vertically upwards.\nHence the magnitude of $\\mathbf W$ is given by:\n:$W = \\size {-m g \\mathbf k} = m g$\n{{qed}}\n\\end{proof}\n\n"}}, "3829": {"score": 0.8574425578117371, "content": {"text": "\\begin{definition}[Definition:Gravity]\n'''Gravity''' is the tendency of bodies with mass to attract each other.\n\\end{definition}"}}, "15043": {"score": 0.8317971229553223, "content": {"text": "\\section{Dimension of Gravitational Constant}\nTags: Dimensional Analysis\n\n\\begin{theorem}\nThe dimension of the gravitational constant $G$ is $M^{-1} L^3 T^{-2}$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Newton's Law of Universal Gravitation:\n:$\\mathbf F = \\dfrac {G m_1 m_2 \\mathbf r} {r^3}$\nWe have that:\n: The dimension of force is $M L T^{-2}$\n: The dimension of displacement is $L$\n: The dimension of mass is $M$.\nLet $x$ be the dimension of $G$.\nThen we have:\n:$M L T^{-2} = x \\dfrac {M^2 L}{L^3}$\nHence, after algebra:\n:$x = M^{-1} L^3 T^{-2}$\n{{qed}}\nCategory:Dimensional Analysis\n\\end{proof}\n\n"}}, "3484": {"score": 0.9023399949073792, "content": {"text": "\\begin{definition}[Definition:Force of Gravity]\nThe '''force of gravity''' or '''gravitational force''' is the force on a body as a result of Newton's Law of Universal Gravitation.\nWhen used in an unqualified sense, it is usual for this to mean the force on a body at the surface of the Earth.\nFrom Gravity at Earth's Surface, this is approximately $9.8 \\ \\mathrm N \\ \\mathrm{kg}^{-1}$\nThe force of gravity varies across the earth's surface, and therefore it makes little sense to use it as a standard.\nHowever, the {{WP|General_Conference_on_Weights_and_Measures|CGPM}} adopted a standard acceleration of gravity of $9.806 \\, 65  \\ \\mathrm N \\ \\mathrm{kg}^{-1}$ in 1901.\nCategory:Definitions/Gravity\n\\end{definition}"}}, "3539": {"score": 0.8599045276641846, "content": {"text": "\\begin{definition}[Definition:Free Fall]\nA body $B$ influenced by a gravitational field $M$ is in '''free fall''' {{iff}} the force on it caused by $M$ is the only force on $B$.\n\\end{definition}"}}, "5249": {"score": 0.9266651272773743, "content": {"text": "\\begin{definition}[Definition:Local Gravitational Constant]\nThe '''local gravitational constant''' is the value of the acceleration $g$ caused by the gravitational field given rise to by whatever body or bodies are in a position to exert that gravitational force.\nIn the everyday context, $g$ is the acceleration due to the gravitational field of Earth at whatever point on or near its surface the observer happens to be.\nThus in this context it is approximately equal to $9 \\cdotp 8 \\ \\mathrm m \\ \\mathrm s^{-2}$.\n\\end{definition}"}}, "3828": {"score": 0.8630519509315491, "content": {"text": "\\begin{definition}[Definition:Gravitational Field]\nEvery body which has mass influences every other body which has mass, according to Newton's Law of Universal Gravitation.\nThus any body can be considered as being surrounded by a field given rise to by its mass, called a '''gravitational field'''.\nIts value $\\mathbf g$ at any point is given by:\n:$\\mathbf g = \\dfrac {G M} {d^3} \\mathbf d$\nwhere:\n:$G$ is the gravitational constant;\n:$M$ is the mass of the body;\n:$\\mathbf d$ is the displacement vector from the point to the center of gravity of the body, whose magnitude is $d$.\n\\end{definition}"}}, "3830": {"score": 0.888066828250885, "content": {"text": "\\begin{definition}[Definition:Gravity/Gravitational Force]\nThe '''gravitational force''' on a body $B$ is the force which is exerted on $B$ as a result of the gravitational field whose influence it is under.\n\\end{definition}"}}}}, "TheoremQA_jianyu_xu/Ramsey_2.json": {"gold": {"12234": 1, "20117": 1, "7627": 1}, "retrieved": {"22043": {"score": 0.826991617679596, "content": {"text": "\\section{Subsets of Disjoint Sets are Disjoint}\nTags: Disjoint Sets, Subsets, Subset\n\n\\begin{theorem}\nLet $S$ and $T$ be disjoint sets.\nLet $S' \\subseteq S$ and $T' \\subseteq T$.\nThen $S'$ and $T'$ are disjoint.\n\\end{theorem}\n\n\\begin{proof}\nLet $S \\cap T = \\O$.\nLet $S' \\subseteq S$ and $T' \\subseteq T$.\n{{AimForCont}} $S' \\cap T' \\ne \\O$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\exists x\n      | o = \\in\n      | r = S' \\cap T'\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = x \n      | o = \\in\n      | r = S'\n      | c = {{Defof|Set Intersection}}\n}}\n{{eqn | lo= \\land\n      | l = x \n      | o = \\in\n      | r = T'\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = x \n      | o = \\in\n      | r = S\n      | c = {{Defof|Subset}}\n}}\n{{eqn | lo= \\land\n      | l = x \n      | o = \\in\n      | r = T\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = x \n      | o = \\in\n      | r = S \\cap T\n      | c = {{Defof|Set Intersection}}\n}}\n{{eqn | ll= \\leadsto\n      | l = S \\cap T\n      | o = \\ne\n      | r = \\O\n      | c = {{Defof|Set Intersection}}\n}}\n{{end-eqn}}\nFrom this contradiction:\n:$S' \\cap T' = \\O$\nHence the result by definition of disjoint sets.\n{{qed}}\n\\end{proof}\n\n"}}, "21375": {"score": 0.828068733215332, "content": {"text": "\\section{Simple Graph whose Vertices Incident to All Edges}\nTags: Graph Theory\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph whose vertices are incident to all its edges.\nThen $G$ is either:\n:the star graph $S_2$, which is also the complete graph $K_2$\n:an edgeless graph of any order.\n\\end{theorem}\n\n\\begin{proof}\nIf $G$ has no edges, then all the vertices are incident to all the edges vacuously.\nSo any of the edgeless graphs $N_n$ for order $n \\in \\Z_{\\ge 0}$ fulfils the criterion.\nSuppose $G$ has more than $2$ vertices $v_1, v_2, v_3$ and at least one edge.\n{{WLOG}}, let one edge be $v_1 v_2$.\nBut $v_3$ cannot be incident to edge $v_1 v_2$.\nSo $G$ can have no more than $2$ vertices.\nFurthermore, there can be only one edge joining those two vertices.\nThe result follows from inspection of $K_2$ and $S_2$.\n{{qed}}\n\\end{proof}\n\n"}}, "18695": {"score": 0.8394442200660706, "content": {"text": "\\section{Pigeonhole Principle}\nTags: Pigeonhole Principle, Named Theorems, Combinatorics\n\n\\begin{theorem}\nLet $S$ be a finite set whose cardinality is $n$.\nLet $S_1, S_2, \\ldots, S_k$ be a partition of $S$ into $k$ subsets.\nThen:\n:at least one subset $S_i$ of $S$ contains at least $\\ceiling {\\dfrac n k}$ elements\nwhere $\\ceiling {\\, \\cdot \\,}$ denotes the ceiling function.\n\\end{theorem}\n\n\\begin{proof}\n{{AimForCont}} no subset $S_i$ of $S$ has as many as $\\ceiling {\\dfrac n k}$ elements.\nThen the maximum number of elements of any $S_i$ would be $\\ceiling {\\dfrac n k} - 1$.\nSo the total number of elements of $S$ would be no more than $k \\paren {\\ceiling {\\dfrac n k} - 1} = k \\ceiling {\\dfrac n k} - k$.\nThere are two cases:\n:$n$ is divisible by $k$\n:$n$ is not divisible by $k$.\nSuppose $k \\divides n$.\nThen $\\ceiling {\\dfrac n k} = \\dfrac n k$ is an integer and:\n:$k \\ceiling {\\dfrac n k} - k = n - k$\nThus:\n:$\\ds \\card S = \\sum_{i \\mathop = 1}^k \\card {S_i} \\le n - k < n$\nThis contradicts the fact that $\\card S = n$.\nHence our assumption that no subset $S_i$ of $S$ has as many as $\\ceiling {\\dfrac n k}$ elements was false.\nNext, suppose that $k \\nmid n$.\nThen:\n:$\\card S = k \\ceiling {\\dfrac n k} - k < \\dfrac {k \\paren {n + k} } k - k = n$\nand again this contradicts the fact that $\\card S = n$.\nIn the same way, our assumption that no subset $S_i$ of $S$ has as many as $\\ceiling {\\dfrac n k}$ elements was false.\nHence, by Proof by Contradiction, there has to be at least $\\ceiling {\\dfrac n k}$ elements in at least one $S_i \\subseteq S$.\n{{qed}}\n\\end{proof}\n\n"}}, "22993": {"score": 0.8342177271842957, "content": {"text": "\\section{Union of Blocks is Set of Points}\nTags: Design Theory, Union of Blocks is Set of Points\n\n\\begin{theorem}\nLet $\\struct {X, \\BB}$ be a pairwise balanced design.\nThat is, let $\\struct {X, \\BB}$ be a design, with $\\size X \\ge 2$, and the number of occurrences of each pair of distinct points in $\\BB$ be $\\lambda$ for some $\\lambda > 0$ constant.\nThen the set union of all the subset elements in $\\BB$ is precisely $X$.\n\\end{theorem}\n\n\\begin{proof}\nLet $X = \\set {x_1, x_2, \\ldots, x_v}$.\nLet $\\BB = \\multiset {y_1, y_2,\\ldots, y_b}$, where the notation denotes a multiset.\nLet $Y = \\ds \\bigcup_{i \\mathop = 1}^b y_i$.\nWe shall show that $Y \\subseteq X$ and $X \\subseteq Y$.\n\\end{proof}\n\n"}}, "2565": {"score": 0.8305619955062866, "content": {"text": "\\begin{definition}[Definition:Distinct/Plural/Pairwise Distinct]\nA set $S$ of objects is '''pairwise distinct''' {{iff}}:\n:for each pair $\\set {x, y} \\subseteq S$ of elements of $S$, $x$ and $y$ are distinct.\nCategory:Definitions/Set Theory\nCategory:Definitions/Distinct\n\\end{definition}"}}, "17224": {"score": 0.8673233985900879, "content": {"text": "\\section{Birthday Paradox/General/3}\nTags: Birthday Paradox\n\n\\begin{theorem}\nLet $n$ be a set of people.\nLet the probability that at least $3$ of them have the same birthday be greater than $50 \\%$.\nThen $n \\ge 88$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map F {r, n}$ be the number of ways to distribute $r$ objects into $n$ cells such that there are no more than $2$ objects in each cell.\nLet there be $d$ cells which are each occupied by $2$ objects.\nThese can be chosen in $\\dbinom n d$ ways.\nThere remain $s = r - 2 d$ objects which can then be distributed among $n - d$ cells in $\\dbinom {n - d} s$ ways.\nIn each such arrangement, the $r$ objects may be permuted in:\n:$\\dbinom r 2 \\dbinom {r - 2} 2 \\cdots \\dbinom {r - 2 d + 2} 2 \\paren {r - 2 d}! = \\dfrac {r!} {2^d}$\ndifferent ways.\nHence:\n:$\\map F {r, n} = \\dbinom n d \\dbinom {n - d} s \\dfrac {r!} {2^d}$\nSo the probability of exactly $d$ pairs and $s$ singletons, where $d - s \\le n$, is given by:\n:$\\dfrac {\\map F {r, n} } {n^r}$\nIf we assume a $365$-day year, we have that the probability that at least $3$ of them have the same birthday is given by:\n:$\\map \\Pr r = 1 - \\ds \\sum_{d \\mathop = 0}^{\\floor {r / 2} } \\dfrac {n! \\, r!} {n^r 2^d d! \\paren {r - 2 d}! \\paren {n + d - r}!}$\nwhere $n = 365$.\nWe require the smallest $r$ for which $\\map \\Pr r > \\dfrac 1 2$.\nThe result yields to calculation.\n{{qed}}\n\\end{proof}\n\n"}}, "20117": {"score": 0.8422786593437195, "content": {"text": "\\section{Ramsey's Theorem}\nTags: Ramsey Theory, Named Theorems, Combinatorics\n\n\\begin{theorem}\nIn any coloring of the edges of a sufficiently large complete graph, one will find monochromatic complete subgraphs.\nFor 2 colors, Ramsey's theorem states that for any pair of positive integers $\\tuple {r, s}$, there exists a least positive integer $\\map R {r, s}$ such that for any complete graph on $\\map R {r, s}$ vertices, whose edges are colored red or blue, there exists either a complete subgraph on $r$ vertices which is entirely red, or a complete subgraph on $s$ vertices which is entirely blue.\nMore generally, for any given number of colors $c$, and any given integers $n_1, \\ldots, n_c$, there is a number $\\map R {n_1, \\ldots, n_c}$ such that:\n:if the edges of a complete graph of order $\\map R {n_1, \\ldots, n_c}$ are colored with $c$ different colours, then for some $i$ between $1$ and $c$, it must contain a complete subgraph of order $n_i$ whose edges are all color $i$.\nThis number $\\map R {n_1, \\ldots, n_c}$ is called the Ramsey number for $n_1, \\ldots, n_c$.\nThe special case above has $c = 2$ (and $n_1 = r$ and $n_2 = s$).\nHere $\\map R {r, s}$ signifies an integer that depends on both $r$ and $s$. It is understood to represent the smallest integer for which the theorem holds.\n\\end{theorem}\n\n\\begin{proof}\nFirst we prove the theorem for the 2-color case, by induction on $r + s$.\nIt is clear from the definition that\n:$\\forall n \\in \\N: \\map R {n, 1} = \\map R {1, n} = 1$\nbecause the complete graph on one node has no edges.\nThis is the base case.\nWe prove that $R \\left({r, s}\\right)$ exists by finding an explicit bound for it.\nBy the inductive hypothesis, $\\map R {r - 1, s}$ and $\\map R {r, s - 1}$ exist.\n\\end{proof}\n\n"}}, "20864": {"score": 0.8745126128196716, "content": {"text": "\\section{Schur's Theorem (Ramsey Theory)}\nTags: Ramsey Theory, Named Theorems, Combinatorics\n\n\\begin{theorem}\nLet $r$ be a positive integer.\nThen there exists a positive integer $S$ such that:\n:for every partition of the integers $\\set {1, \\ldots, S}$ into $r$ parts, one of the parts contains integers $x$, $y$ and $z$ such that:\n::$x + y = z$\n\\end{theorem}\n\n\\begin{proof}\nLet:\n:$n = \\map R {3, \\ldots, 3}$\nwhere $\\map R {3, \\ldots, 3}$ denotes the Ramsey number on $r$ colors.\nTake $S$ to be $n$.\n{{refactor|Extract the below process of \"coloring\" a partition into its own page}}\npartition the integers $\\set {1, \\ldots, n}$ into $r$ parts, which we denote by '''colors'''.\nThat is:\n:the integers in the first part are said to be '''colored''' $c_1$\n:the integers in the second part are said to be colored $c_2$ \nand so on till color $c_r$.\nThus $\\set {1, \\ldots, S}$ has been '''$r$-colored'''.\n(This terminology is common in Ramsey theory.)\nNow consider the complete graph $K_n$.\nNow color the edges of $K_n$ as follows:\n:An edge $xy$ is given color $c$ if $\\size {x - y}$ was colored $c$ in the partitioning.\n{{explain|When the page defining a \"coloring\" of a partition is written, make sure that the links are assigned appropriately from the two difference senses of \"coloring\" in the above.}}\nFrom the definition of $\\map R {3, \\ldots, 3}$ and Ramsey's Theorem, $K_n$ will definitely contain a monochromatic triangle, say built out of the vertices $i > j > k$.\nLet the triangle be colored $c_m$.\nNow $i - j$, $i - k$ and $j - k$ will also be colored $c_m$.\nThat is, $i - j$, $i - k$ and $j - k$ will belong to the same part in the partition.\nIt only remains to take $x = i - j$, $y = j - k$ and $z = i - k$ to complete the proof.\n{{qed}}\n\\end{proof}\n\n"}}, "23824": {"score": 0.8441924452781677, "content": {"text": "\\section{No Simple Graph is Perfect}\nTags: Simple Graphs, Graph Theory, Perfect Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph whose order is $2$ or greater.\nThen $G$ is not perfect.\n\\end{theorem}\n\n\\begin{proof}\nRecall that a perfect graph is one where each vertex is of different degree.\nWe note in passing that the simple graph consisting of one vertex trivially fulfils the condition for perfection.\n{{AimForCont}} $G$ is a simple graph of order $n$ where $n \\ge 2$ such that $G$ is perfect.\nFirst, suppose that $G$ has no isolated vertices.\nBy the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n$.\nThat means it must connect to at least $n$ other vertices.\nBut there are only $n - 1$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\nNow suppose $G$ has an isolated vertex.\nThere can be only one, otherwise there would be two vertices of degree zero, and so $G$ would not be perfect.\nAgain by the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n - 1$.\nBut of the remaining $n - 1$ vertices, one of them is of degree zero.\nSo it cannot be adjacent to any vertex.\nSo there are only $n - 2$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\n{{qed}}\n\\end{proof}\n\n"}}, "21374": {"score": 0.8485211133956909, "content": {"text": "\\section{Simple Graph where All Vertices and All Edges are Adjacent}\nTags: Simple Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph in which:\n:every vertex is adjacent to every other vertex\nand:\n:every edge is adjacent to every other edge.\nThen $G$ is of order no greater than $3$.\n\\end{theorem}\n\n\\begin{proof}\nIt is seen that examples exist of simple graphs which fulfil the criteria where the order of $G$ is no greater than $3$:\n:400px\nThe cases where the order of $G$ is $1$ or $2$ are trivial.\nWhen the order of $G$ is $3$, the criteria can be verified by inspection.\nLet the order of $G = \\struct {V, E}$ be $4$ or more.\nLet $v_1, v_2, v_3, v_4 \\in V$.\nSuppose every vertex is adjacent to every other vertex.\nAs $v_1$ is adjacent to $v_2$, there exists the edge $v_1 v_2$.\nAs $v_3$ is adjacent to $v_4$, there exists the edge $v_3 v_4$.\nBut $v_1 v_2$ and $v_3 v_4$ both join two distinct pairs of vertices.\nThus $v_1 v_2$ and $v_3 v_4$ are not adjacent, by definition.\nSo when there are $4$ or more vertices in $G$, it cannot fulfil the criteria.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_elainewan/math_algebra_3_2.json": {"gold": {"10009": 1, "11136": 1, "20859": 1}, "retrieved": {"9814": {"score": 0.827277421951294, "content": {"text": "\\begin{definition}[Definition:Unit Vector]\nA '''unit vector''' is a vector quantity which has a magnitude of $1$.\n\\end{definition}"}}, "11254": {"score": 0.83310866355896, "content": {"text": "\\section{Linear Combination of Non-Parallel Complex Numbers is Zero if Factors are Both Zero}\nTags: Geometry of Complex Plane\n\n\\begin{theorem}\nLet $z_1$ and $z_2$ be complex numbers expressed as vectors such taht $z_1$ is not parallel to $z_2$.\nLet $a, b \\in \\R$ be real numbers such that:\n:$a z_1 + b z_2 = 0$\nThen $a = 0$ and $b = 0$.\n\\end{theorem}\n\n\\begin{proof}\nSuppose it is not the case that $a = b = 0$.\nThen:\n{{begin-eqn}}\n{{eqn | l = a z_1 + b z_2\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = a \\paren {x_1 + i y_1} + b \\paren {x_2 + i y_2}\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = a x_1 + b x_2\n      | r = 0\n      | c = \n}}\n{{eqn | l = a y_1 + b y_2\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = a x_1\n      | r = -b x_2\n      | c = \n}}\n{{eqn | l = a y_1\n      | r = -b y_2\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\dfrac {y_1} {x_1}\n      | r = \\dfrac {y_2} {x_2}\n      | c = \n}}\n{{end-eqn}}\nand $z_1$ and $z_2$ are parallel.\n{{qed}}\n\\end{proof}\n\n"}}, "9988": {"score": 0.8361487984657288, "content": {"text": "\\begin{definition}[Definition:Vector Quantity/Component]\nLet $\\mathbf a$ be a vector quantity embedded in an $n$-dimensional Cartesian coordinate system $C_n$.\nLet $\\mathbf a$ be represented with its initial point at the origin of $C_n$.\nLet $\\mathbf e_1, \\mathbf e_2, \\ldots, \\mathbf e_n$ be the unit vectors in the positive direction of the coordinate axes of $C_n$.\nThen:\n:$\\mathbf a = a_1 \\mathbf e_1 + a_2 \\mathbf e_2 + \\cdots + a_3 \\mathbf e_n$\nwhere:\n:$a_1 \\mathbf e_1, a_2 \\mathbf e_2, \\ldots, a_3 \\mathbf e_n$ are the '''component vectors''' of $\\mathbf a$ in the directions of $\\mathbf e_1, \\mathbf e_2, \\ldots, \\mathbf e_n$ \n:$a_1, a_2, \\ldots, a_3$ are the '''components''' of $\\mathbf a$ in the directions of $\\mathbf e_1, \\mathbf e_2, \\ldots, \\mathbf e_n$.\nThe number of '''components''' in $\\mathbf a$ is determined by the number of dimensions in the Cartesian coordinate system of its frame of reference.\nA vector quantity with $n$ '''components''' can be referred to as an '''$n$-vector'''.\nIt is usually more convenient to write $\\mathbf a$ as the ordered tuple $\\tuple {a_1, a_2, \\ldots, a_n}$ instead of $\\mathbf a = a_1 \\mathbf e_1 + a_2 \\mathbf e_2 + \\cdots + a_3 \\mathbf e_n$.\nThere are two special cases:\n\\end{definition}"}}, "11274": {"score": 0.8351845741271973, "content": {"text": "\\section{Like Unit Vectors are Equal}\nTags: Vectors\n\n\\begin{theorem}\nLet $\\mathbf a$ and $\\mathbf b$ be like vector quantities.\nThen:\n:$\\mathbf {\\hat a} = \\mathbf {\\hat b}$\nwhere $\\mathbf {\\hat a}$ and $\\mathbf {\\hat b}$ denote the unit vectors in the direction of $\\mathbf a$ and $\\mathbf b$.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of like vector quantities, $\\mathbf a$ and $\\mathbf b$ have the same direction.\nBy definition of unit vector, $\\mathbf {\\hat a}$ and $\\mathbf {\\hat b}$ are both of magnitude $1$.\nHence the result, by Equality of Vector Quantities.\n{{qed}}\nCategory:Vectors\n\\end{proof}\n\n"}}, "11275": {"score": 0.8350309729576111, "content": {"text": "\\section{Like Vector Quantities are Multiples of Each Other}\nTags: Scalar Multiplication\n\n\\begin{theorem}\nLet $\\mathbf a$ and $\\mathbf b$ be like vector quantities.\nThen:\n:$\\mathbf a = \\dfrac {\\size {\\mathbf a} } {\\size {\\mathbf b} } \\mathbf b$\nwhere:\n:$\\size {\\mathbf a}$ denotes the magnitude of $\\mathbf a$\n:$\\dfrac {\\size {\\mathbf a} } {\\size {\\mathbf b} } \\mathbf b$ denotes the scalar product of $\\mathbf b$ by $\\dfrac {\\size {\\mathbf a} } {\\size {\\mathbf b} }$.\n\\end{theorem}\n\n\\begin{proof}\nBy the definition of like vector quantities:\n:$\\mathbf a$ and $\\mathbf b$ are '''like vector quantities''' {{iff}} they have the same direction.\nBy definition of unit vector:\n:$\\dfrac {\\mathbf a} {\\size {\\mathbf a} } = \\dfrac {\\mathbf b} {\\size {\\mathbf b} }$\nas both are in the same direction, and both have length $1$.\nBy definition of scalar division:\n:$\\dfrac 1 {\\size {\\mathbf a} } \\mathbf a = \\dfrac 1 {\\size {\\mathbf b} } \\mathbf b$\nHence, multiplying by $\\size {\\mathbf a}$:\n:$\\mathbf a = \\dfrac {\\size {\\mathbf a} } {\\size {\\mathbf b} } \\mathbf b$\n{{qed}}\n\\end{proof}\n\n"}}, "23312": {"score": 0.8575533628463745, "content": {"text": "\\section{Vector Quantity can be Expressed as Sum of 3 Non-Coplanar Vectors}\nTags: Vectors\n\n\\begin{theorem}\nLet $\\mathbf r$ be a vector quantity embedded in space.\nLet $\\mathbf a$, $\\mathbf b$ and $\\mathbf c$ be non-coplanar.\nThen $\\mathbf r$ can be expressed uniquely as the resultant of $3$ vector quantities which are each parallel to one of $\\mathbf a$, $\\mathbf b$ and $\\mathbf c$.\n\\end{theorem}\n\n\\begin{proof}\n400px\nLet $\\mathbf {\\hat a}$, $\\mathbf {\\hat b}$ and $\\mathbf {\\hat c}$ be unit vectors in the directions of $\\mathbf a$, $\\mathbf b$ and $\\mathbf c$ respectively.\nLet $O$ be a point in space.\nTake $\\vec {OP} := \\mathbf r$.\nWith $OP$ as its space diagonal, construct a parallelepiped with edges $OA$, $OB$ and $OC$ parallel to $\\mathbf {\\hat a}$, $\\mathbf {\\hat b}$ and $\\mathbf {\\hat c}$ respectively.\nOnly one such parallelepiped can be so constructed.\nLet $x$, $y$ and $z$ be the length of the edges $OA$, $OB$ and $OC$ respectively.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\mathbf r\n      | r = \\vec {OA} + \\vec {AF} + \\vec {FP}\n      | c = \n}}\n{{eqn | r = \\vec {OA} + \\vec {OB} + \\vec {OC}\n      | c = \n}}\n{{eqn | r = x \\mathbf {\\hat a} + y \\mathbf {\\hat b} + z \\mathbf {\\hat c}\n      | c = \n}}\n{{end-eqn}}\nThus $\\mathbf r$ is the resultant of the $3$ components $x \\mathbf {\\hat a}$, $y \\mathbf {\\hat b}$ and $z \\mathbf {\\hat c}$ which, by construction, are parallel to $\\mathbf a$, $\\mathbf b$ and $\\mathbf c$ respectively.\nThe fact that only one parallelepiped can be constructed in the above proves uniqueness.\n{{qed}}\n\\end{proof}\n\n"}}, "14423": {"score": 0.836907148361206, "content": {"text": "\\section{Equality of Vector Quantities}\nTags: Equality, Vectors\n\n\\begin{theorem}\nTwo vector quantities are equal {{iff}} they have the same magnitude and direction.\nThat is:\n:$\\mathbf a = \\mathbf b \\iff \\paren {\\size {\\mathbf a} = \\size {\\mathbf b} \\land \\hat {\\mathbf a} = \\hat {\\mathbf b} }$\nwhere:\n:$\\hat {\\mathbf a}$ denotes the unit vector in the direction of $\\mathbf a$\n:$\\size {\\mathbf a}$ denotes the magnitude of $\\mathbf a$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\mathbf a$ and $\\mathbf b$ be expressed in component form:\n{{begin-eqn}}\n{{eqn | l = \\mathbf a\n      | r = a_1 \\mathbf e_1 + a_2 \\mathbf e_2 + \\cdots + a_n \\mathbf e_n\n      | c = \n}}\n{{eqn | l = \\mathbf b\n      | r = b_1 \\mathbf e_1 + b_2 \\mathbf e_2 + \\cdots + b_n \\mathbf e_n\n      | c = \n}}\n{{end-eqn}}\nwhere $\\mathbf e_1, \\mathbf e_2, \\ldots, \\mathbf e_n$ denote the unit vectors in the positive directions of the coordinate axes of the Cartesian coordinate space into which $\\mathbf a$ has been embedded.\nThus $\\mathbf a$ and $\\mathbf b$ can be expressed as:\n{{begin-eqn}}\n{{eqn | l = \\mathbf a\n      | r = \\tuple {a_1, a_2, \\ldots, a_n}\n      | c = \n}}\n{{eqn | l = \\mathbf b\n      | r = \\tuple {b_1, b_2, \\ldots, b_n}\n      | c = \n}}\n{{end-eqn}}\nWe have that:\n{{begin-eqn}}\n{{eqn | l = \\size {\\mathbf a}\n      | r = \\size {\\tuple {a_1, a_2, \\ldots, a_n} }\n      | c = \n}}\n{{eqn | r = \\sqrt {\\paren {a_1^2 + a_2^2 + \\ldots + a_n^2} }\n      | c = \n}}\n{{end-eqn}}\nand similarly:\n{{begin-eqn}}\n{{eqn | l = \\size {\\mathbf b}\n      | r = \\size {\\tuple {b_1, b_2, \\ldots, b_n} }\n      | c = \n}}\n{{eqn | r = \\sqrt {\\paren {b_1^2 + b_2^2 + \\ldots + b_n^2} }\n      | c = \n}}\n{{end-eqn}}\nAlso:\n{{begin-eqn}}\n{{eqn | l = \\hat {\\mathbf a}\n      | r = \\widehat {\\tuple {a_1, a_2, \\ldots, a_n} }\n      | c = \n}}\n{{eqn | r = \\dfrac 1 {\\sqrt {\\paren {a_1^2 + a_2^2 + \\ldots + a_n^2} } } \\mathbf a\n      | c = \n}}\n{{end-eqn}}\nand similarly:\n{{begin-eqn}}\n{{eqn | l = \\hat {\\mathbf b}\n      | r = \\widehat {\\tuple {b_1, b_2, \\ldots, b_n} }\n      | c = \n}}\n{{eqn | r = \\dfrac 1 {\\sqrt {\\paren {b_1^2 + b_2^2 + \\ldots + b_n^2} } }\n      | c = \n}}\n{{end-eqn}}\nLet $\\mathbf a = \\mathbf b$.\nThen by Equality of Ordered Tuples:\n:$(1): \\quad a_1 = b_1, a_2 = b_2, \\ldots a_n = b_n$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\size {\\mathbf a}\n      | r = \\sqrt {\\paren {a_1^2 + a_2^2 + \\ldots + a_n^2} }\n      | c = \n}}\n{{eqn | r = \\sqrt {\\paren {b_1^2 + b_2^2 + \\ldots + b_n^2} }\n      | c = from $(1)$\n}}\n{{eqn | r = \\size {\\mathbf b}\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = \\hat {\\mathbf a}\n      | r = \\dfrac 1 {\\sqrt {\\paren {a_1^2 + a_2^2 + \\ldots + a_n^2} } } \\mathbf a\n      | c = \n}}\n{{eqn | r = \\dfrac 1 {\\sqrt {\\paren {b_1^2 + b_2^2 + \\ldots + b_n^2} } } \\mathbf b\n      | c = from $(1)$\n}}\n{{eqn | r = \\hat {\\mathbf b}\n      | c = \n}}\n{{end-eqn}}\n{{Finish|The other direction now needs to be attended to.}}\n\\end{proof}\n\n"}}, "18283": {"score": 0.8812553286552429, "content": {"text": "\\section{Ordered Basis for Coordinate Plane}\nTags: Definitions: Analytic Geometry, Analytic Geometry, Coordinate Systems\n\n\\begin{theorem}\nLet $a_1, a_2 \\in \\R^2$ such that $\\set {a_1, a_2}$ forms a linearly independent set.\nThen $\\tuple {a_1, a_2}$ is an ordered basis for the $\\R$-vector space $\\R^2$.\nHence the points on the plane can be uniquely identified by means of linear combinations of $a_1$ and $a_2$.\n\\end{theorem}\n\n\\begin{proof}\n:500pxrightthumb\nLet $P$ be any point in the plane for which we want to provide a linear combination of $a_1$ and $a_2$.\nLet the distance from $O$ to the point determined by $a_1$ be defined as being $1$ unit of length on the line $L_1$.\nLet the distance from $O$ to the point determined by $a_2$ be defined as being $1$ unit of length on the line $L_2$.\nDraw lines parallel to $L_1$ and $L_2$ through $P$.\nThen the coordinates $\\lambda_1$ and $\\lambda_2$ of $P$ are given by:\n:$P = \\lambda_1 a_1 + \\lambda_2 a_2$\nby the Parallelogram Law.\n{{ProofWanted}}\n\\end{proof}\n\n"}}, "23326": {"score": 0.8493112325668335, "content": {"text": "\\section{Vectors are Equal iff Components are Equal}\nTags: Equality, Vectors\n\n\\begin{theorem}\nTwo vector quantities are equal {{iff}} they have the same components.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\mathbf a$ and $\\mathbf b$ be vector quantities.\nThen by Vector Quantity can be Expressed as Sum of 3 Non-Coplanar Vectors, $\\mathbf a$ and $\\mathbf b$ can be expressed uniquely as components.\nSo if $\\mathbf a$ and $\\mathbf b$ then the components of $\\mathbf a$ are the same as the components of $\\mathbf b$\nSuppose $\\mathbf a$ and $\\mathbf b$ have the same components: $\\mathbf x$, $\\mathbf y$ and $\\mathbf z$.\nThen by definition:\n:$\\mathbf a = \\mathbf x + \\mathbf y + \\mathbf z$\nand also:\n:$\\mathbf b = \\mathbf x + \\mathbf y + \\mathbf z$\nand trivially:\n:$\\mathbf a = \\mathbf b$\n{{qed}}\n\\end{proof}\n\n"}}, "22065": {"score": 0.8494635224342346, "content": {"text": "\\section{Subspaces of Dimension 2 Real Vector Space}\nTags: Subspaces of Dimension 2 Real Vector Space, Linear Algebra\n\n\\begin{theorem}\nTake the $\\R$-vector space $\\left({\\R^2, +, \\times}\\right)_\\R$.\nLet $S$ be a subspace of $\\left({\\R^2, +, \\times}\\right)_\\R$.\nThen $S$ is one of:\n: $(1): \\quad \\left({\\R^2, +, \\times}\\right)_\\R$\n: $(2): \\quad \\left\\{{0}\\right\\}$\n: $(3): \\quad$ A line through the origin.\n\\end{theorem}\n\n\\begin{proof}\n* Let $S$ be a non-zero subspace of $\\left({\\R^2, +, \\times}\\right)_\\R$.\nThen $S$ contains a non-zero vector $\\left({\\alpha_1, \\alpha_2}\\right)$.\nHence $S$ also contains $\\left\\{{\\lambda \\times \\left({\\alpha_1, \\alpha_2}\\right), \\lambda \\in \\R}\\right\\}$.\nFrom Equation of a Straight Line, this set may be described as a line through the origin.\n* Suppose $S$ also contains a non-zero vector $\\left({\\beta_1, \\beta_2}\\right)$ which is not on that line.\nThen $\\alpha_1 \\times \\beta_2 - \\alpha_2 \\times \\beta_1 \\ne 0$.\nOtherwise $\\left({\\beta_1, \\beta_2}\\right)$ would be $\\zeta \\times \\left({\\alpha_1, \\alpha_2}\\right)$, where either $\\zeta = \\beta_1 / \\alpha_1$ or $\\zeta = \\beta_2 / \\alpha_2$ according to whether $\\alpha_1 \\ne 0$ or $\\alpha_2 \\ne 0$.\nBut then $S = \\left({\\R^2, +, \\times}\\right)_\\R$.\nBecause, if $\\left({\\gamma_1, \\gamma_2}\\right)$ is any vector at all, then:\n: $\\left({\\gamma_1, \\gamma_2}\\right) = \\lambda \\times \\left({\\alpha_1, \\alpha_2}\\right) + \\mu \\times \\left({\\beta_1, \\beta_2}\\right)$\nwhere $\\lambda = \\dfrac {\\gamma_1 \\times \\beta_2 - \\gamma_2 \\times \\beta_1} {\\alpha_1 \\times \\beta_2 - \\alpha_2 \\times \\beta_1}, \\mu = \\dfrac {\\alpha_1 \\times \\gamma_2 - \\alpha_2 \\times \\gamma_1} {\\alpha_1 \\times \\beta_2 - \\alpha_2 \\times \\beta_1}$\nwhich we get by solving the simultaneous eqns:\n{{begin-eqn}}\n{{eqn | l=\\alpha_1 \\times \\lambda + \\beta_1 \\times \\mu\n      | r=0\n      | c=\n}}\n{{eqn | l=\\alpha_2 \\times \\lambda + \\beta_2 \\times \\mu\n      | r=0\n      | c=\n}}\n{{end-eqn}}\nThe result follows.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_wenhuchen/ODE3.json": {"gold": {"5197": 1, "5201": 1, "8264": 1}, "retrieved": {"2283": {"score": 0.8541545867919922, "content": {"text": "\\begin{definition}[Definition:Derivative/Higher Derivatives/Zeroth Derivative]\nThe '''zeroth derivative''' of a real function $f$ is defined as $f$ itself:\n:$f^{\\paren 0} := f$\nwhere $f^{\\paren n}$ denotes the $n$th derivative of $f$.\nCategory:Definitions/Differential Calculus\n\\end{definition}"}}, "22600": {"score": 0.854726254940033, "content": {"text": "\\section{Temperature of Body under Newton's Law of Cooling}\nTags: Thermodynamics, Heat\n\n\\begin{theorem}\nLet $B$ be a body in an environment whose ambient temperature is $H_a$.\nLet $H$ be the temperature of $B$ at time $t$.\nLet $H_0$ be the temperature of $B$ at time $t = 0$.\nThen:\n:$H = H_a - \\paren {H_0 - H_a} e^{-k t}$\nwhere $k$ is some positive constant.\n\\end{theorem}\n\n\\begin{proof}\nBy Newton's Law of Cooling:\n:The rate at which a hot body loses heat is proportional to the difference in temperature between it and its surroundings.\nWe have the differential equation:\n:$\\dfrac {\\d H} {\\d t} \\propto - \\paren {H - H_a}$\nThat is:\n:$\\dfrac {\\d H} {\\d t} = - k \\paren {H - H_a}$\nwhere $k$ is some positive constant.\nThis is an instance of the Decay Equation, and so has a solution:\n:$H = H_a + \\paren {H_0 - H_a} e^{-k t}$\n{{qed}}\n{{Namedfor|Isaac Newton|cat = Newton}}\n\\end{proof}\n\n"}}, "20892": {"score": 0.8566317558288574, "content": {"text": "\\section{Second Derivative of Natural Logarithm Function}\nTags: Differential Calculus, Derivatives, Logarithms, Natural Logarithms\n\n\\begin{theorem}\nLet $\\ln x$ be the natural logarithm function.\nThen:\n:$\\map {\\dfrac {\\d^2} {\\d x^2} } {\\ln x} = -\\dfrac 1 {x^2}$\n\\end{theorem}\n\n\\begin{proof}\nFrom Derivative of Natural Logarithm Function:\n:$\\dfrac \\d {\\d x} \\ln x = \\dfrac 1 x$\nFrom the Power Rule for Derivatives: Integer Index:\n:$\\dfrac {\\d^2} {\\d x^2} \\ln x = \\dfrac \\d {\\d x} \\dfrac 1 x = -\\dfrac 1 {x^2}$\n{{qed}}\n\\end{proof}\n\n"}}, "15336": {"score": 0.8564150333404541, "content": {"text": "\\section{Decay Equation}\nTags: Decay Equation, Examples of Linear First Order ODEs, First Order ODEs, Examples of Separation of Variables, Examples of First Order ODE\n\n\\begin{theorem}\nThe first order ordinary differential equation:\n:$\\dfrac {\\d y} {\\d x} = k \\paren {y_a - y}$\nwhere $k \\in \\R: k > 0$\nhas the general solution:\n:$y = y_a + C e^{-k x}$\nwhere $C$ is an arbitrary constant.\nIf $y = y_0$ at $x = 0$, then:\n:$y = y_a + \\paren {y_0 - y_a} e^{-k x}$\nThis differential equation is known as the '''decay equation'''.\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\frac {\\d y} {\\d x}\n      | r = -k \\paren {y - y_a}\n      | c =\n}}\n{{eqn | ll= \\leadsto\n      | l = \\int \\frac {\\d y} {y - y_a}\n      | r = -\\int k \\rd x\n      | c = Separation of Variables\n}}\n{{eqn | ll= \\leadsto\n      | l = \\map \\ln {y - y_a}\n      | r = -k x + C_1\n      | c = Primitive of Reciprocal and Derivatives of Function of $a x + b$\n}}\n{{eqn | ll= \\leadsto\n      | l = y - y_a\n      | r = e^{-k x + C_1}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = y\n      | r = y_a + C e^{-k x}\n      | c = where we put $C = e^{C_1}$\n}}\n{{end-eqn}}\nThis is our general solution.\n{{qed|lemma}}\nSuppose we have the initial condition:\n:$y = y_0$ when $x = 0$\nThen:\n:$y_0 = y_a + C e^{-k \\cdot 0} = y_a + C$\nand so:\n:$C = y_0 - y_a$\nHence the solution:\n:$y = y_a + \\paren {y_0 - y_a} e^{-k x}$\n{{qed}}\n\\end{proof}\n\n"}}, "11203": {"score": 0.8562769293785095, "content": {"text": "\\section{Linear Second Order ODE/y'' + k^2 y = 0}\nTags: Examples of Second Order ODE, Examples of Constant Coefficient Homogeneous LSOODEs, Second Order ODE: y'' + k^2 y = 0, Linear Second Order ODE: y'' + k^2 y = 0, Examples of Second Order ODEs\n\n\\begin{theorem}\nThe second order ODE:\n:$(1): \\quad y'' + k^2 y = 0$\nhas the general solution:\n:$y = A \\, \\map \\sin {k x + B}$\nor can be expressed as:\n:$y = C_1 \\sin k x + C_2 \\cos k x$\n\\end{theorem}\n\n\\begin{proof}\nUsing Solution of Second Order Differential Equation with Missing Independent Variable, $(1)$ can be expressed as:\n:$p \\dfrac {\\mathrm d p} {\\mathrm d y} = -k^2 y = 0$\nwhere $p = \\dfrac {\\mathrm d y} {\\mathrm d x}$.\nFrom:\n:First Order ODE: $y \\, \\mathrm d y = k x \\, \\mathrm d x$\nthis has the solution:\n:$p^2 = -k^2 y^2 + C$\nor: \n:$p^2 + k^2 y^2 = C$\nAs the {{LHS}} is the sum of squares, $C$ has to be positive for this to have any solutions.\nThus, let $C = \\alpha^2$.\nThen:\n{{begin-eqn}}\n{{eqn | l = p^2 + k^2 y^2\n      | r = k^2 \\alpha^2\n      | c = \n}}\n{{eqn | ll= \\implies\n      | l = p = \\dfrac {\\mathrm d y} {\\mathrm d x}\n      | r = \\pm k \\sqrt {\\alpha^2 - y^2}\n      | c = \n}}\n{{eqn | ll= \\implies\n      | l = \\int \\dfrac {\\mathrm d y} {\\sqrt {\\alpha^2 - y^2} }\n      | r = \\int \\pm k \\, \\mathrm d x\n      | c = Separation of Variables\n}}\n{{eqn | ll= \\implies\n      | l = \\arcsin \\dfrac y \\alpha\n      | r = \\int \\pm k x + \\beta\n      | c = Primitive of $\\dfrac 1 {\\sqrt {a^2 - x^2} }$\n}}\n{{eqn | ll= \\implies\n      | l = y\n      | r = \\alpha \\sin \\left({\\pm k x + \\beta}\\right)\n      | c = \n}}\n{{eqn | ll= \\implies\n      | l = y\n      | r = A \\sin \\left({k x + B}\\right)\n      | c = \n}}\n{{end-eqn}}\nFrom Multiple of Sine plus Multiple of Cosine: Sine Form, this can be expressed as:\n:$y = C_1 \\sin k x + C_2 \\cos k x$\n{{qed}}\n\\end{proof}\n\n"}}, "17235": {"score": 0.8646283745765686, "content": {"text": "\\section{Body under Constant Acceleration/Distance after Time}\nTags: Mechanics\n\n\\begin{theorem}\nLet $B$ be a body under constant acceleration $\\mathbf a$.\nThen:\n:$\\mathbf s = \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\nwhere:\n:$\\mathbf s$ is the displacement of $B$ from its initial position at time $t$\n:$\\mathbf u$ is the velocity at time $t = 0$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Body under Constant Acceleration: Velocity after Time:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\nBy definition of velocity, this can be expressed as:\n:$\\dfrac {\\d \\mathbf s} {\\d t} = \\mathbf u + \\mathbf a t$\nwhere both $\\mathbf u$ and $\\mathbf a$ are constant.\nBy Solution to Linear First Order Ordinary Differential Equation:\n:$\\mathbf s = \\mathbf c + \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\nwhere $\\mathbf c$ is a constant vector.\nWe are (implicitly) given the initial condition:\n:$\\bigvalueat {\\mathbf s} {t \\mathop = 0} = \\mathbf 0$\nfrom which it follows immediately that:\n:$\\mathbf s = \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\n{{qed}}\n\\end{proof}\n\n"}}, "17237": {"score": 0.8580873608589172, "content": {"text": "\\section{Body under Constant Acceleration/Velocity after Time}\nTags: Mechanics\n\n\\begin{theorem}\nLet $B$ be a body under constant acceleration $\\mathbf a$.\nThen:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\nwhere:\n:$\\mathbf u$ is the velocity at time $t = 0$\n:$\\mathbf v$ is the velocity at time $t$.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of acceleration:\n: $\\dfrac {\\d \\mathbf v} {\\d t} = \\mathbf a$\nBy Solution to Linear First Order Ordinary Differential Equation:\n:$\\mathbf v = \\mathbf c + \\mathbf a t$\nwhere $\\mathbf c$ is a constant vector.\nWe are given the initial condition:\n:$\\bigvalueat {\\mathbf v} {t \\mathop = 0} = \\mathbf u$\nfrom which it follows immediately that:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\n{{qed}}\nCategory:Mechanics\n\\end{proof}\n\n"}}, "17234": {"score": 0.8663332462310791, "content": {"text": "\\section{Body under Constant Acceleration}\nTags: Physics, Applied Mathematics, Mechanics\n\n\\begin{theorem}\nLet $B$ be a body under constant acceleration $\\mathbf a$.\nThe following equations apply:\n\\end{theorem}\n\n\\begin{proof}\n{{MissingLinks}}\n{{improve|The justification for the derivation of $(1)$ from $(2)$ is handwavey and clumsy. Better to do $(1)$ first and then derive $(2)$ from it, to save having to solve a second order DE whose solution we have not yet put on {{ProofWiki}}. As the separate proofs for the separate parts have now been extracted into their own pages, complete with justifying links throughout (and no handwvery), it is suggested that this proof may be deleted.}}\n$B$ has acceleration $\\mathbf a$.\nLet $\\mathbf x$ be the vector corresponding to the position of $B$ at time $t$.\nThen:\n:$\\dfrac {\\d^2 \\mathbf x} {\\d t^2} = \\mathbf a$\nSolving this differential equation:\n:$\\mathbf x = \\mathbf c_0 + \\mathbf c_1 t + \\frac 1 2 \\mathbf a t^2$\nwith $\\mathbf c_0$ and $\\mathbf c_1$ constant vectors.\nEvaluating $\\mathbf x$ at $t = 0$ shows that $\\mathbf c_0$ is the value $\\mathbf x_0$ of $\\mathbf x$ at time $t=0$.\nTaking the derivative of $\\mathbf x$ at $t = 0$ shows that $\\mathbf c_1$ corresponds to $\\mathbf u$.\nTherefore, since $\\mathbf s = \\mathbf x - \\mathbf x_0$, we have:\n:$\\mathbf s = \\mathbf u t + \\dfrac {\\mathbf a t^2} 2$\nand by taking the derivative of $\\mathbf x$, we have:\n:$\\mathbf v = \\mathbf u + \\mathbf a t$\nNext, we dot $\\mathbf v$ into itself using the previous statement.\nFrom the linearity and commutativity of the dot product:\n{{begin-eqn}}\n{{eqn | l = \\mathbf v \\cdot \\mathbf v\n      | r = \\paren {\\mathbf u + \\mathbf a t} \\cdot \\paren {\\mathbf u + \\mathbf a t}\n      | c = \n}}\n{{eqn | r = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf u \\cdot \\mathbf a t + t^2 \\mathbf a \\cdot \\mathbf a\n      | c = \n}}\n{{eqn | r = \\mathbf u \\cdot \\mathbf u + \\mathbf a \\cdot \\paren {2 \\mathbf u t + t^2 \\mathbf a}\n      | c = \n}}\n{{end-eqn}}\nThe expression in parentheses is $2 \\mathbf s$, so:\n:$\\mathbf v \\cdot \\mathbf v = \\mathbf u \\cdot \\mathbf u + 2 \\mathbf a \\cdot \\mathbf s$\nand the proof is complete.\n{{qed}}\n\\end{proof}\n\n"}}, "5197": {"score": 0.8590362668037415, "content": {"text": "\\begin{definition}[Definition:Linear Second Order Ordinary Differential Equation]\nA '''linear second order ordinary differential equation''' is a differential equation which is in (or can be manipulated into) the form:\n:$\\dfrac {\\d^2 y} {\\d x^2} + \\map P x \\dfrac {\\d y} {\\d x} + \\map Q x y = \\map R x$\nwhere, as is indicated by the notation, $\\map P x$, $\\map Q x$ and $\\map R x$ are functions of $x$ alone (or constants).\n\\end{definition}"}}, "15023": {"score": 0.8608629107475281, "content": {"text": "\\section{Differential Equation of Family of Linear Combination of Functions is Linear}\nTags: Linear First Order ODEs\n\n\\begin{theorem}\nConsider the one-parameter family of curves:\n:$(1): \\quad y = C \\map f x + \\map g x$\nThe differential equation that describes $(1)$ is linear and of first order.\n\\end{theorem}\n\n\\begin{proof}\nDifferentiating $(1)$ {{WRT|Differentiation}} $x$ gives:\n:$(2): \\quad \\dfrac {\\d y} {\\d x} = C \\map {f'} x + \\map {g'} x$\nRearranging $(1)$, we have:\n:$C = \\dfrac {y - \\map g x} {\\map f x}$\nSubstituting for $C$ in $(2)$:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {\\d y} {\\d x}\n      | r = \\dfrac {y - \\map g x} {\\map f x} \\map {f'} x + \\map {g'} x\n      | c = \n}}\n{{eqn | r = \\dfrac {\\map {f'} x} {\\map f x} y - \\dfrac {\\map g x \\map {f'} x} {\\map f x} + \\map g x\n      | c = \n}}\n{{end-eqn}}\nwhich leaves:\n:$\\dfrac {\\d y} {\\d x} - \\dfrac {\\map {f'} x} {\\map f x} y = \\map g x \\paren {1 - \\dfrac {\\map {f'} x} {\\map f x} }$\nwhich is linear and of first order.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_jianyu_xu/Catalan_2.json": {"gold": {"17913": 1}, "retrieved": {"7766": {"score": 0.7832533717155457, "content": {"text": "\\begin{definition}[Definition:Recreational Chess]\n'''Recreational chess''' is a subgenre of recreational mathematics which explores the geometry of the chessboard and the moves of its pieces outside of the context of an actual game of chess itself.\nThe name amusingly belies the fact chess is itself a recreation.\n\\end{definition}"}}, "14641": {"score": 0.7839033603668213, "content": {"text": "\\section{Element of Leibniz Harmonic Triangle as Sum of Elements on Diagonal from Below/Lemma 2}\nTags: Element of Leibniz Harmonic Triangle as Sum of Elements on Diagonal from Below, Leibniz Harmonic Triangle\n\n\\begin{theorem}\nConsider the Leibniz harmonic triangle:\n{{:Definition:Leibniz Harmonic Triangle}}\nLet $\\tuple {n, m}$ be the element in the $n$th row and $m$th column.\nThen:\n:$\\ds \\forall r \\in \\N_{>0}: \\tuple {n, m} = \\tuple {n + r, m + r} + \\sum_{k \\mathop = 1}^r \\tuple {n + k, m + k - 1}$\nThat is, each number in the Leibniz harmonic triangle is equal to the sum of the number below it, $\\paren {r - 1}$ numbers diagonally below that number, and the number to the right of the last number.\nThis is similar to Rising Sum of Binomial Coefficients (Hockey-stick Identity) for Pascal's triangle.\n\\end{theorem}\n\n\\begin{proof}\nProof by induction:\nFor all $r \\in \\N_{>0}$, let $\\map P r$ be the proposition:\n:$\\ds \\tuple {n, m} = \\tuple {n + r, m + r} + \\sum_{k \\mathop = 1}^r \\tuple {n + k, m + k - 1}$\n\\end{proof}\n\n"}}, "1109": {"score": 0.790294349193573, "content": {"text": "\\begin{definition}[Definition:Chessboard Puzzle]\nA '''chessboard puzzle''' is a type of puzzle based on the geometry of a chessboard.\n\\end{definition}"}}, "5410": {"score": 0.7876358032226562, "content": {"text": "\\begin{definition}[Definition:Magic Hexagon/Line]\nA '''line''' of a magic hexagon is a set of consecutively adjacent cells, in any of the three directions parallel to the sides of the underlying hexagon.\n:400px\nCategory:Definitions/Magic Hexagons\n\\end{definition}"}}, "7582": {"score": 0.7876151204109192, "content": {"text": "\\begin{definition}[Definition:Queen's Tour]\nA '''queen's tour''' is a puzzle in recreational chess.\nOn a chessboard of arbitrary size, a chess queen is to make a complete tour:\n:passing over every square at least once\n:returning to its square of origin\n:using as few moves as possible.\n\\end{definition}"}}, "17904": {"score": 0.8133508563041687, "content": {"text": "\\section{Number of Different Ways to play First n Moves in Chess}\nTags: Chess, Chess Problems\n\n\\begin{theorem}\nThe sequence formed from the number of ways to play the first $n$ moves in chess begins:\n:$20, 400, 8902, 197 \\, 742, \\ldots$\n{{OEIS|A007545}}\nThe count for the fourth move is already ambiguous, as it depends on whether only legal moves count, or whether all moves, legal or illegal, are included.\nThe count as given here does include illegal moves in addition to legal ones.\n\\end{theorem}\n\n\\begin{proof}\nThere are $20$ ways to make the $1$st move by White:\n:Each of the $8$ pawns may be moved either $1$ or $2$ squares forward, making $16$ moves\n:Each of the $2$ knights may be moved to either of $2$ squares before it, making $4$ moves.\nFor each of those $20$ first moves by White, Black has the same $20$ options.\nThus there are $20 \\times 20$ possible different games after the $2$nd move.\nTo count the $3$rd moves, one needs to consider cases.\nFirst note that after the $1$st move, whatever it was, there are $7$ pawns on the $2$nd rank, each of which can be moved $1$ or $2$ squares forward, making $14$ moves for each of those $400$ possibilities.\nNote that if a knight was one of the pieces to have moved first, the pawn in the square behind where it ends up cannot move -- hence the $7$ pawns on the $2$nd rank that can move.\nThus there are $400 \\times 14 = 5600$ possible moves involving a so-far unmoved pawn.\nFor each of the $400$ positions, there are exactly $8$ which consist of two pawns in opposition on the $4$th and $5$th rank.\nThere are also another $4 \\times 20 = 80$ positions in which white moved a knight.\nFor all other $400 - 88 = 312$ positions, the already-moved pawn has the option of moving another square forward.\nThis gives another $312$ options for the $3$rd move.\nWe now need to take into account the possibility that White may be able to capture a Black pawn.\n{{finish|Each case needs to be investigated.}}\n\\end{proof}\n\n"}}, "1081": {"score": 0.7986018061637878, "content": {"text": "\\begin{definition}[Definition:Chess/Chessboard]\nA '''chessboard''' is an array (usually square) of alternating dark and light squares (conventionally referred to as '''black''' and '''white''', even though they may well not be those actual colours).\n{{ChessDiagram|\n|\n|  |  |  |  |  |  |  |  \n|  |  |  |  |  |  |  |  \n|  |  |  |  |  |  |  |  \n|  |  |  |  |  |  |  |  \n|  |  |  |  |  |  |  |  \n|  |  |  |  |  |  |  |  \n|  |  |  |  |  |  |  |  \n|  |  |  |  |  |  |  |  \n|\n}}\nThis may be referred to as an $m \\times n$ '''chessboard''', where:\n:$m$ is the number of ranks\n:$n$ is the number of files.\nFor the conventional '''chessboard''', as depicted above, $m = n = 8$.\n\\end{definition}"}}, "20020": {"score": 0.8280975818634033, "content": {"text": "\\section{Queen's Tour}\nTags: Queen's Tours\n\n\\begin{theorem}\nConsider a chessboard $\\CC$ of size $n \\times n$ such that $n > 2$.\nThen the shortest queen's tour on $\\CC$ is of $2 n - 2$ moves.\nFor $n < 5$ it is necessary for the queen to move outside the boundary of the chessboard in order for this to happen.\n\\end{theorem}\n\n\\begin{proof}\nFirst it is shown that at least $2 n - 2$ moves are needed.\nLet there be $R$ rows and $S$ columns which have none of the given moves on them.\nThe $R \\times S$-square segment of this chessboard has a $2 R + 2 S - 4$ squares on its edge if $R$ and $S$ are both greater than $1$.\nEach diagonal moves covers at most $2$ of these boundary square.\nThus in the set of moves there are at least:\n:$R + S - 2$ diagonal moves\n:$n - R$ horizontal moves\n:$n - S$ vertical moves.\nThus there are at least $2 n - 2$ moves needed to cover the entire chessboard.\n{{qed|lemma}}\nIt remains to demonstrate that no more than $2 n - 2$ moves are needed.\nThe proof proceeds by induction.\nLet us raise the following hypothesis:\nFor all $n \\in \\Z_{\\ge 3}$, let $\\map P n$ be the proposition:\n:A chessboard of size $n \\times n$ has a queen's tour $T$ of $2 n - 2$ moves such that $T$ exits from the top right square:\n:300px\n\\end{proof}\n\n"}}, "6783": {"score": 0.8010943531990051, "content": {"text": "\\begin{definition}[Definition:Pascal's Triangle/Edge Cells]\nConsider Pascal's Triangle:\n{{:Definition:Pascal's Triangle}}\nThe numbers in column $0$ and in diagonal $0$, containing all $1$s, are referred to as the '''edge cells'''.\n\\end{definition}"}}, "20232": {"score": 0.8115075826644897, "content": {"text": "\\section{Re-entrant Queen's Tour}\nTags: Queen's Tours\n\n\\begin{theorem}\nConsider a chessboard $\\CC$ of size $n \\times n$ such that $n > 3$.\nThen there exists a re-entrant queen's tour on $\\CC$ of $2 n - 2$ moves.\nFor $n < 6$ it is necessary for the queen to move outside the boundary of the chessboard in order for this to happen.\n\\end{theorem}\n\n\\begin{proof}\nFirst it is shown that at least $2 n - 2$ moves are needed.\nLet there be $R$ rows and $S$ columns which have none of the given moves on them.\nThe $R \\times S$-square segment of this chessboard has a $2 R + 2 S - 4$ squares on its edge if $R$ and $S$ are both greater than $1$.\nEach diagonal moves covers at most $2$ of these boundary square.\nThus in the set of moves there are at least:\n:$R + S - 2$ diagonal moves\n:$n - R$ horizontal moves\n:$n - S$ vertical moves.\nThus there are at least $2 n - 2$ moves needed to cover the entire chessboard.\n{{qed|lemma}}\nIt remains to demonstrate that no more than $2 n - 2$ moves are needed.\n{{ProofWanted}}\n\\end{proof}\n\n"}}}}, "TheoremQA_maxku/ipnetwork13-hammingdist.json": {"gold": {"14102": 1}, "retrieved": {"23447": {"score": 0.7827417254447937, "content": {"text": "\\section{Word Metric is Metric}\nTags: Group Theory, Word Metric, Examples of Metric Space, Examples of Metric Spaces\n\n\\begin{theorem}\nLet $\\struct {G, \\circ}$ be a group.\nLet $S$ be a generating set for $G$ which is closed under inverses (that is, $x^{-1} \\in S \\iff x \\in S$).\nLet $d_S$ be the associated word metric.\nThen $d_S$ is a metric on $G$.\n\\end{theorem}\n\n\\begin{proof}\nLet $g, h \\in G$.\nIt is given that $S$ is a generating set for $G$.\nIt follows that there exist $s_1, \\ldots, s_n \\in S$ such that $g^{-1} \\circ h = s_1 \\circ \\cdots \\circ s_n$.\nTherefore $\\map {d_S} {g, h} \\le n$, establishing that $\\R$ is a valid codomain for the mapping $d_S$ with domain $G \\times G$.\nThis is the form a mapping must have to be able to be a metric.\nNow checking the other defining properties for a metric in turn:\n\\end{proof}\n\n"}}, "10102": {"score": 0.7846753001213074, "content": {"text": "\\begin{definition}[Definition:Weight of Linear Codeword]\nLet $C$ be a codeword of a linear code.\nThe '''weight''' of $C$ is the number of non-zero terms of $C$.\n\\end{definition}"}}, "22015": {"score": 0.7916134595870972, "content": {"text": "\\section{Subset of Linear Code with Even Weight Codewords}\nTags: Linear Codes\n\n\\begin{theorem}\nLet $C$ be a linear code.\nLet $C^+$ be the subset of $C$ consisting of all the codewords of $C$ which have even weight.\nThen $C^+$ is a subgroup of $C$ such that either $C^+ = C$ or such that $\\order {C^+} = \\dfrac {\\order C} 2$.\n\\end{theorem}\n\n\\begin{proof}\nNote that the zero codeword is in $C^+$ as it has a weight of $0$ which is even.\nLet $c$ and $d$ be of even weight, where $c$ and $d$ agree in $k$ ordinates.\nLet $\\map w c$ denote the weight of $c$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map w {c + d}\n      | r = \\map w c - k + \\map w d - k\n      | c = \n}}\n{{eqn | r = \\map w c + \\map w d - 2 k\n      | c = \n}}\n{{end-eqn}}\nwhich is even.\nSince the negative of a vector $\\mathbf v$ in $\\Z_2$ equals $\\mathbf v$, it follows that the inverse of $c \\in C$ is also in $C$.\nIt follows from the Two-Step Subgroup Test that $C^+$ is a subgroup of $C$.\nLet $C \\ne C^+$.\nThen $C$ contains a codeword $c$ of odd weight.\nLet $C^-$ denote the subset of $C$ consisting of all the codewords of $C$ which have odd weight.\nAdding $c$ to each codeword of $C^+$ gives distinct codewords of odd weight, so:\n:$\\order {C^-} \\ge \\order {C^+}$\nSimilarly, adding $c$ to each codeword of $C^-$ gives distinct codewords of even weight, so:\n:$\\order {C^-} \\le \\order {C^+}$\nAs $C = C^+ \\cup C^-$ it follows that:\n:$\\order C = 2 \\order {C^+}$\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "5710": {"score": 0.7912763357162476, "content": {"text": "\\begin{definition}[Definition:Minimum Distance of Linear Code]\nLet $C$ be a linear code whose master code is $\\map V {n, p}$.\nThe '''minimum distance $d$ of $C$''' is defined as:\n:$\\map d C := \\ds \\min_{u, v \\mathop \\in C: u \\mathop \\ne v} \\set {\\map d {u, v} }$\nwhere $\\map d {u, v}$ denotes the distance between $u$ and $v$.\n\\end{definition}"}}, "10572": {"score": 0.7891222238540649, "content": {"text": "\\section{Minimum Distance of Linear Code is Smallest Weight of Non-Zero Codeword}\nTags: Linear Codes\n\n\\begin{theorem}\nLet $C$ be a linear $\\tuple {n, k}$-code whose master code is $\\map V {n, p}$.\nLet $\\map d C$ denote the minimum distance of $C$.\nThen:\n:$\\map d C = \\ds \\min_{u \\mathop \\in C} \\map w u$\nwhere $\\map w u$ denotes the weight of $u$.\n\\end{theorem}\n\n\\begin{proof}\nLet $f := \\ds \\min_{u \\mathop \\in C} \\map w u$.\nLet $\\mathbf 0$ denote the codeword in $\\map V {n, p}$ consisting of all zeroes.\nAs $C$ is a subspace of $\\map V {n, p}$, we have that $\\mathbf 0 \\in C$.\nLet $w$ be a codeword with weight $f$.\nThen:\n:$\\map d {w, \\mathbf 0} = f$\nso $f \\ge \\map d C$.\nLet $u, v \\in C$ such that $\\map d {u, v} = \\map d C$.\nWe have that $C$ is a linear code.\nTherefore:\n:$u - v \\in C$\nwhere $u - v$ denotes the difference between $u$ and $v$.\nBut $u - v$ has weight $\\map d C$.\nThus:\n:$\\map d C \\le f$\nand it follows that $\\map d C = f$.\n{{qed}}\n\\end{proof}\n\n"}}, "2559": {"score": 0.8406474590301514, "content": {"text": "\\begin{definition}[Definition:Distance between Linear Codewords]\nLet $u$ and $v$ be two codewords of a linear code.\nThe '''distance''' between $u$ and $v$ is the number of corresponding terms at which $u$ and $v$ are different.\n\\end{definition}"}}, "14522": {"score": 0.7956034541130066, "content": {"text": "\\section{Equal Set Differences iff Equal Intersections}\nTags: Set Theory, Intersection, Set Intersection, Equal Set Differences iff Equal Intersections, Set Difference\n\n\\begin{theorem}\n:$R \\setminus S = R \\setminus T \\iff R \\cap S = R \\cap T$\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | ll=(R \\setminus S) \\sqcup (R \\cap S)\n      | l = = R\n      | r=(R \\setminus T) \\sqcup (R \\cap T)\n}}\n{{end-eqn}}\nAssuming either $R \\setminus S = R \\setminus T$ or $R \\cap S = R \\cap T$ we can subtract those terms from both sides because the unions are disjoint.\nAssume for instance $R \\setminus S = R \\setminus T$. Subtracting this set we obtain\n{{begin-eqn}}\n{{eqn | l=[(R \\setminus S) \\sqcup (R \\cap S)] \\setminus (R \\setminus S)\n      | r=[(R \\setminus T) \\sqcup (R \\cap T)] \\setminus (R \\setminus T)\n}}\n{{eqn | l=[(R \\setminus S) \\setminus (R \\setminus S)] \\sqcup [(R \\cap S) \\setminus (R \\setminus S)]\n      | r=[(R \\setminus T) \\setminus (R \\setminus T)] \\sqcup [(R \\cap T) \\setminus (R \\setminus T)] \n}}\n{{eqn | l=(R \\cap S) \\setminus (R \\setminus S)\n      | r=(R \\cap T) \\setminus (R \\setminus T) \n}}\n{{eqn | l=(R \\cap S)\n      | r=(R \\cap T)\n}}\n{{end-eqn}}\n{{Explain|First we need a page defining $\\sqcup$, second we need a link to a result that says we can subtract those terms from the both sides because the unions are disjoint.}}\n{{Questionable|... and I'm not convinced it works as a proof, it's too handwavey.}}\n\\end{proof}\n\n"}}, "14901": {"score": 0.8572598099708557, "content": {"text": "\\section{Distance between Linear Codewords is Distance Function}\nTags: Linear Codes\n\n\\begin{theorem}\nLet $\\map V {n, p}$ be a master code.\nLet $d: V \\times V \\to \\Z$ be the mapping defined as:\n:$\\forall u, v \\in V: \\map d {u, v} =$ the distance between $u$ and $v$\nthat is, the number of corresponding terms at which $u$ and $v$ are different.\nThen $d$ defines a distance function in the sense of a metric space.\n\\end{theorem}\n\n\\begin{proof}\nIt is to be demonstrated that $d$ satisfies all the metric space axioms.\nLet $u, v, w \\in \\map V {n, p}$ be arbitrary.\n\\end{proof}\n\n"}}, "1987": {"score": 0.8022556900978088, "content": {"text": "\\begin{definition}[Definition:Coset Decoding Table]\nA '''coset decoding table''' is a technique for decoding a linear $\\tuple {n, k}$ code.\nLet $C$ be a linear $\\tuple {n, k}$ code whose master code is $\\map V {n, p}$.\nLet $T$ be an array constructed as follows:\n:The first row consists of the codewords of $C$, starting with the zero codeword first.\n{{explain|It has not yet been rigorously proved that $C$ does actually contain the zero codeword.}}\n:Each subsequent row is a left coset of $C$.\n:The entries of the first column of $T$ are the coset representatives, now called '''coset leaders'''.\n:The $r$th coset leader is allocated by choosing any element of $\\map V {n, p}$ of minimum weight which is not already included in the first $r - 1$ rows.\nThen $T$ is a '''coset decoding table'''.\nNote that it may not always be easy to find the $r$th coset leader.\n\\end{definition}"}}, "13057": {"score": 0.8031726479530334, "content": {"text": "\\section{Golay Ternary Code has Minimum Distance 5}\nTags: Golay Ternary Code\n\n\\begin{theorem}\nThe Golay ternary code has a minimum distance of $5$.\n\\end{theorem}\n\n\\begin{proof}\nLet $C$ denote the Golay ternary code.\nBy inspection of the standard generator matrix $G$ of $C$:\n:$G := \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 2 & 2 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 2 & 2 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 2 & 1 & 0 & 1 & 2 \\\\\n0 & 0 & 0 & 1 & 0 & 0 & 2 & 2 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 0 & 1 & 2 & 2 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 \n\\end{pmatrix}$\nit is seen that the smallest weight of all the codewords of $C$ that can be found in $G$ is $5$.\nSo it is immediately seen that the minimum distance of $C$ is at least $5$.\nIt remains to be shown that the minimum distance of $C$ is no more than $5$.\n{{ProofWanted}}\n\\end{proof}\n\n"}}}}, "TheoremQA_mingyin/convexity1.json": {"gold": {"11937": 1}, "retrieved": {"1928": {"score": 0.8743123412132263, "content": {"text": "\\begin{definition}[Definition:Convex Set (Vector Space)]\nLet $V$ be a vector space over $\\R$ or $\\C$.\nA subset $A \\subseteq V$ is said to be '''convex''' {{iff}}:\n:$\\forall x, y \\in A: \\forall t \\in \\closedint 0 1: t x + \\paren {1 - t} y \\in A$\n\\end{definition}"}}, "16571": {"score": 0.8743951916694641, "content": {"text": "\\section{Closed Unit Ball is Convex Set}\nTags: Vector Spaces, Closed Balls, Convex Sets (Vector Spaces)\n\n\\begin{theorem}\nLet $\\struct {X, \\norm {\\, \\cdot \\,} }$ be a normed vector space.\nLet $\\map {B_1^-} 0$ be a closed unit ball in $X$.\nThen $\\map {B_1^-} 0$ is convex.\n\\end{theorem}\n\n\\begin{proof}\nLet $x, y \\in \\map {B_1} 0$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\norm {\\paren {1 - \\alpha} x + \\alpha y}\n      | o = \\le\n      | r = \\norm {\\paren {1 - \\alpha} x} + \\norm {\\alpha y}\n      | c = {{NormAxiomVector|3}}\n}}\n{{eqn | r = \\size {1 - \\alpha} \\norm x + \\size \\alpha \\norm y\n      | c = {{NormAxiomVector|2}}\n}}\n{{eqn | r = \\paren {1 - \\alpha} \\norm x + \\alpha \\norm y\n      | c = {{Defof|Convex Set (Vector Space)}}: $0 \\le \\alpha \\le 1$\n}}\n{{eqn | o = \\le\n      | r = \\paren {1 - \\alpha} + \\alpha\n      | c = $x, y \\in \\map {B_1^-} 0$\n}}\n{{eqn | r = 1\n}}\n{{end-eqn}}\nTherefore, $\\paren {1 - \\alpha}x + \\alpha y \\in \\map {B_1^-} 0$.\nBy definition, $\\map {B_1^-} 0$ is convex.\n{{qed}}\n\\end{proof}\n\n"}}, "4443": {"score": 0.8823127746582031, "content": {"text": "\\begin{definition}[Definition:Inscribe]\nLet a geometric figure $A$ be constructed in the interior of another geometric figure $B$ such that:\n:$(1): \\quad$ $A$ and $B$ have points in common\n:$(2): \\quad$ No part of $A$ is outside $B$.\nThen $A$ is '''inscribed''' inside $B$.\n\\end{definition}"}}, "1929": {"score": 0.8808696866035461, "content": {"text": "\\begin{definition}[Definition:Convex Set (Vector Space)/Line Segment]\nLet $V$ be a vector space over $\\R$ or $\\C$.\nLet $x, y \\in V$.\nThe set:\n:$\\set {t x + \\paren {1 - t} y: t \\in \\closedint 0 1}$ \nis called the '''(straight) line segment joining $x$ and $y$'''.\nA convex set can thus be described as a set containing all '''straight line segments''' between its elements.\n\\end{definition}"}}, "699": {"score": 0.879486620426178, "content": {"text": "\\begin{definition}[Definition:Boundary (Geometry)/Containment]\nA geometric figure is said to be '''contained''' by its boundary or boundaries.\n{{DefinitionWanted}}\n\\end{definition}"}}, "22800": {"score": 0.9335262775421143, "content": {"text": "\\section{Triangle is Convex Set}\nTags: Vector Spaces\n\n\\begin{theorem}\nThe interior of a triangle embedded in $\\R^2$ is a convex set.\n\\end{theorem}\n\n\\begin{proof}\nDenote the triangle as $\\triangle$, and the interior of the boundary of $\\triangle$ as $\\Int \\triangle$.\nFrom Boundary of Polygon is Jordan Curve, it follows that the boundary of $\\triangle$ is equal to the image of a Jordan curve, so $\\Int \\triangle$ is well-defined.\nDenote the vertices of $\\triangle$ as $A_1, A_2, A_3$.\nFor $i \\in \\set {1, 2, 3}$, put $j = i \\bmod 3 + 1$, $k = \\paren {i + 1} \\bmod 3 + 1$, and:\n:$U_i = \\set {A_i + s t \\paren {A_j - A_i} + \\paren {1 - s} t \\paren {A_k - A_i} : s \\in \\openint 0 1, t \\in \\R_{>0} }$\nSuppose that the angle $\\angle A_i$ between is $A_j - A_i$ and $A_k - A_i$ is non-convex.\nAs $\\angle A_i$ is an internal angle in $\\triangle$, it follows from definition of polygon that $\\angle A_i$ cannot be zero or straight.\nThen $\\angle A_i$ is larger than a straight angle, which is impossible by Sum of Angles of Triangle Equals Two Right Angles.\nIt follows that $\\angle A_i$ is convex.\nFrom Characterization of Interior of Triangle, it follows that:\n:$\\ds \\Int \\triangle = \\bigcap_{i \\mathop = 1}^3 U_i$\nFrom Interior of Convex Angle is Convex Set, it follows for $i \\in \\set {1, 2, 3}$ that $U_i$ is a convex set.\nThe result now follows from Intersection of Convex Sets is Convex Set (Vector Spaces).\n{{qed}}\nCategory:Vector Spaces\n\\end{proof}\n\n"}}, "15566": {"score": 0.8827970027923584, "content": {"text": "\\section{Convex Set is Star Convex Set}\nTags: Vector Spaces, Convex Sets (Vector Spaces)\n\n\\begin{theorem}\nLet $V$ be a vector space over $\\R$ or $\\C$.\nLet $A \\subseteq V$ be a non-empty convex set.\nThen $A$ is a star convex set, and every point in $A$ is a star center.\n\\end{theorem}\n\n\\begin{proof}\nLet $a \\in A$.\nNote that there is at least one point in $A$, as $A$ is non-empty.\nIf $x \\in A$, then there is a line segment joining $a$ and $x$.\nBy definition of star convex set, it follows that $A$ is star convex, and $a$ is a star center.\n{{qed}}\nCategory:Vector Spaces\nCategory:Convex Sets (Vector Spaces)\n\\end{proof}\n\n"}}, "1917": {"score": 0.9344529509544373, "content": {"text": "\\begin{definition}[Definition:Convex Polygon]\nLet $P$ be a polygon.\n$P$ is a '''convex polygon''' {{iff}}:\n:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.\n\\end{definition}"}}, "12000": {"score": 0.9002712368965149, "content": {"text": "\\section{Interior of Convex Angle is Convex Set}\nTags: Vector Spaces, Convex Sets (Vector Spaces)\n\n\\begin{theorem}\nLet $\\mathbf v, \\mathbf w$ be two non-zero vectors in $\\R^2$, and let $p$ be a point in $\\R^2$.\nSuppose that the angle between $\\mathbf v$ and $\\mathbf w$ is a convex angle.\nThen the set \n:$U = \\left\\{ {p + st \\mathbf v + \\left({1-s}\\right) t \\mathbf w : s \\in \\left({0\\,.\\,.\\,1}\\right) , t \\in \\R_{>0} }\\right\\}$\nis a convex set.\n{{expand|It'd be really nice to have a picture of $U$ to support intuition and connect with the page title}}\n\\end{theorem}\n\n\\begin{proof}\nLet $p_1 ,p_2 \\in U$.\nThen for $i \\in \\left\\{ {1, 2}\\right\\}$, $p_i = p + s_i t_i \\mathbf v + \\left({1 - s_i}\\right) t_i \\mathbf w$ for some $s_i \\in \\left({0\\,.\\,.\\,1}\\right) , t_i \\in \\R_{>0}$.\nWLOG assume that $t_1 \\le t_2$.\nSuppose that $q \\in \\R^2$ lies on the line segment joining $p_1$ and $p_2$, so:\n{{begin-eqn}}\n{{eqn |l= q\n      |r= p + s_1 t_1 \\mathbf v + \\left({1 - s_1}\\right) t_1 \\mathbf w + s \\left({ p + s_2 t_2 \\mathbf v + \\left({1 - s_2}\\right) t_2 \\mathbf w - p - s_1 t_1 \\mathbf v - \\left({1 - s_1}\\right) t_1 \\mathbf w }\\right)\n      |c= for some $s \\in \\left({0\\,.\\,.\\,1}\\right)$\n}}\n{{eqn |r= p + \\left({ \\left({1 - s}\\right) s_1 t_1 + s s_2 t_2}\\right) \\mathbf v +  \\left({ \\left({1 - s}\\right) \\left({1 - s_1}\\right) t_1 + s \\left({1 - s_2}\\right) t_2}\\right) \\mathbf w\n}}\n{{eqn |r= p + \\dfrac{\\left({1 - s}\\right) s_1 t_1 + s s_2 t_2}{r}r \\mathbf v + \\dfrac{t_1 + st_2 - st_1 - \\left({1 - s}\\right) s_1 t_1 - s s_2 t_2}{r} r \\mathbf w\n      |c= where $r = t_1 + s \\left({t_2 - t_1}\\right)$\n}}\n{{eqn |r= p + \\dfrac{\\left({1 - s}\\right) s_1 t_1 + s s_2 t_2}{r}r \\mathbf v + \\left({ 1 - \\dfrac{\\left({1 - s}\\right) s_1 t_1 + s s_2 t_2}{r} }\\right) r \\mathbf w\n}}\n{{end-eqn}}\nAs $t_1 \\le t_2$, it follows that $r \\in \\R_{>0}$.\nWe have $\\dfrac{ \\left({1 - s}\\right) s_1 t_1 + s s_2 t_2}{r}> 0$, and:\n{{begin-eqn}}\n{{eqn |l= 1 - \\dfrac{\\left({1 - s}\\right) s_1 t_1 + s s_2 t_2}{r}\n      |r= \\dfrac{ \\left({1 - s}\\right) \\left({1 - s_1}\\right) t_1 + s \\left({1 - s_2}\\right) t_2}{r}\n}}\n{{eqn |o= >\n      |r= 0\n}}\n{{end-eqn}}\nIt follows that $\\dfrac{ \\left({1 - s}\\right) s_1 t_1 + s s_2 t_2}{r} \\in \\left({0\\,.\\,.\\,1}\\right)$.\nThen $q \\in U$.\nBy definition of convex set, it follows that $U$ is convex.\n{{qed}}\nCategory:Vector Spaces\nCategory:Convex Sets (Vector Spaces)\n\\end{proof}\n\n"}}, "1918": {"score": 0.9184110760688782, "content": {"text": "\\begin{definition}[Definition:Convex Polyhedron]\nLet $P$ be a polyhedron.\n$P$ is a '''convex polyhedron''' {{iff}}:\n:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.\n\\end{definition}"}}}}, "TheoremQA_jianyu_xu/Ramsey_6.json": {"gold": {"20117": 1, "7627": 1}, "retrieved": {"13492": {"score": 0.8929495215415955, "content": {"text": "\\section{Finite Tree has Leaf Nodes}\nTags: Finite Tree has Leaf Nodes, Tree Theory, Graph Theory, Trees\n\n\\begin{theorem}\nEvery non-edgeless finite tree has at least two leaf nodes.\n\\end{theorem}\n\n\\begin{proof}\nWe use the Method of Infinite Descent.\nSuppose $T$ is a tree which has no nodes of degree $1$.\nFirst note that no tree has all even nodes.\nThat is because by Characteristics of Eulerian Graph, it would then be an Eulerian graph, and by definition, trees do not have circuits.\nFrom the Handshake Lemma, we know that $T$ must therefore have at least two odd nodes whose degree is at least $3$.\nAs $T$ is finite, this number must itself be finite.\nOf those nodes, there must be two (call them $u$ and $v$) which can be joined by a path $P$ containing no odd nodes apart from $u$ and $v$.\n(Otherwise you can pick as one of the two nodes one of those in the interior of $P$.)\nConsider that path $P$ from $u$ to $v$. \nAs a tree has no circuits, all nodes of $P$ are distinct, or at least part of $P$ will describe a cycle.\nNow consider the subgraph $S$ formed by removing the edges comprising $P$ from $T$, but leaving the nodes where they are.\nThe nodes $u$ and $v$ at either end of $P$ will no longer be odd, as they have both had one edge removed from them.\nAll the nodes on $P$ other than $u$ and $v$ will stay even.\nThe graph $S$ may become disconnected, and may even contain isolated nodes.\nHowever, except for these isolated nodes (which became that way because of being nodes of degree $2$ on $P$), and however many components $S$ is now in, all the nodes of $S$ are still either even or odd with degree of $3$ or higher.\nThat is because by removing $P$, the only odd nodes we have affected are $u$ and $v$, which are now even.\nNow, if the nodes in any component of $S$ are all even, that component is Eulerian.\nHence $S$ contains a circuit, and is therefore not a tree.\nFrom Subgraph of Tree, it follows that $T$ can not be a tree after all.\nHowever, if the nodes in any component $T'$ of $S$ are not all even, then there can't be as many odd nodes in it as there are in $T$ (because we have reduced the number by $2$).\nAlso, because of the method of construction of $T'$, all of its odd nodes are of degree of at least $3$.\nBy applying the Method of Infinite Descent, it follows that $T$ must contain a circuit, and is therefore not a tree.\nSo every tree must have at least one node of degree $1$.\nNow, suppose that $T$ is a tree with exactly $1$ node of degree $1$. Call this node $u$.\nFrom the Handshake Lemma, we know that $T$ must therefore have at least one odd node whose degree is at least $3$.\nLet $P$ be a path from $u$ to any such odd node such that $P$ passes only through even nodes, as we did above.\nAgain, let us remove all the edges of $P$.\nBy a similar argument to the one above, we will once again have reduced the tree to one in which any remaining odd nodes all have degree of at least $3$.\nThen we are in a position to apply the argument above.\nHence $T$ must have at least two nodes of degree $1$.\n{{qed}}\n{{Proofread}}\n\\end{proof}\n\n"}}, "15419": {"score": 0.8939852714538574, "content": {"text": "\\section{Cycle does not Contain Subcycles}\nTags: Graph Theory\n\n\\begin{theorem}\nLet $G$ be a cycle graph.\nThen the only cycle graph that is a subgraph of $G$ is $G$ itself.\n\\end{theorem}\n\n\\begin{proof}\n{{AimForCont}} that $G$ contains a subgraph $C$ such that:\n:$C$ is a cycle graph\n:$C \\ne G$ is non-empty.\nThen there exists some vertex $v$ that is not in $C$.\nLet $u$ be any vertex of $C$.\nSince $G$ is a cycle graph, it is connected.\nTherefore there is a walk from $u$ to $v$ in $G$.\nThere must be some vertex $x$ that is the last vertex in $C$ along that walk.\nTherefore, $x$ is adjacent to a vertex not in $C$.\nThus it has a degree of at least $3$.\nBut $G$ is a cycle graph and every vertex in a cycle graph has degree $2$.\nThe result follows by Proof by Contradiction.\n{{Qed}}\nCategory:Graph Theory\n\\end{proof}\n\n"}}, "21566": {"score": 0.8995197415351868, "content": {"text": "\\section{Smallest Simple Graph with One Vertex Adjacent to All Others}\nTags: Graph Theory\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph of order $n$.\nLet $G$ be the simple graph with the smallest size such that one vertex is adjacent to all other vertices of $G$.\nThen $G$ is the star graph of order $n$ and is of size $n - 1$.\n\\end{theorem}\n\n\\begin{proof}\nIn order for the one vertex in question to be adjacent to all the others, it needs to be incident to $n - 1$ edges.\nThis is the smallest number of edges required.\nHence $G$ is a star graph.\nFrom Size of Star Graph, the size of $G$ is thus $n - 1$.\n{{qed}}\n\\end{proof}\n\n"}}, "20117": {"score": 0.8955176472663879, "content": {"text": "\\section{Ramsey's Theorem}\nTags: Ramsey Theory, Named Theorems, Combinatorics\n\n\\begin{theorem}\nIn any coloring of the edges of a sufficiently large complete graph, one will find monochromatic complete subgraphs.\nFor 2 colors, Ramsey's theorem states that for any pair of positive integers $\\tuple {r, s}$, there exists a least positive integer $\\map R {r, s}$ such that for any complete graph on $\\map R {r, s}$ vertices, whose edges are colored red or blue, there exists either a complete subgraph on $r$ vertices which is entirely red, or a complete subgraph on $s$ vertices which is entirely blue.\nMore generally, for any given number of colors $c$, and any given integers $n_1, \\ldots, n_c$, there is a number $\\map R {n_1, \\ldots, n_c}$ such that:\n:if the edges of a complete graph of order $\\map R {n_1, \\ldots, n_c}$ are colored with $c$ different colours, then for some $i$ between $1$ and $c$, it must contain a complete subgraph of order $n_i$ whose edges are all color $i$.\nThis number $\\map R {n_1, \\ldots, n_c}$ is called the Ramsey number for $n_1, \\ldots, n_c$.\nThe special case above has $c = 2$ (and $n_1 = r$ and $n_2 = s$).\nHere $\\map R {r, s}$ signifies an integer that depends on both $r$ and $s$. It is understood to represent the smallest integer for which the theorem holds.\n\\end{theorem}\n\n\\begin{proof}\nFirst we prove the theorem for the 2-color case, by induction on $r + s$.\nIt is clear from the definition that\n:$\\forall n \\in \\N: \\map R {n, 1} = \\map R {1, n} = 1$\nbecause the complete graph on one node has no edges.\nThis is the base case.\nWe prove that $R \\left({r, s}\\right)$ exists by finding an explicit bound for it.\nBy the inductive hypothesis, $\\map R {r - 1, s}$ and $\\map R {r, s - 1}$ exist.\n\\end{proof}\n\n"}}, "18389": {"score": 0.89484703540802, "content": {"text": "\\section{Ore Graph is Connected}\nTags: Graph Theory, Ore Graphs\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be an Ore graph.\nThen $G$ is connected.\n\\end{theorem}\n\n\\begin{proof}\nLet $G$ be an Ore graph.\n{{AimForCont}} $G$ is not connected.\nThen it has at least two components.\nCall these components $C_1$ and $C_2$.\nThus, there exist non-adjacent vertices $u$ and $v$ such that $u$ is in $C_1$ and $v$ is in $C_2$.\nLet $m_1$ and $m_2$ be the number of vertices in $C_1$ and $C_2$ respectively.\nIt is clear that:\n:$m_1 + m_2 \\le n$\nBy definition of Ore graph, $G$ is simple.\nThus it follows that:\n:$\\map {\\deg_G} u \\le m_1 - 1$ and $\\map {\\deg_G} v \\le m_2 - 1$\nThus:\n{{begin-eqn}}\n{{eqn | l = \\map {\\deg_G} u + \\map {\\deg_G} v\n      | o = \\le\n      | r = m_1 - 1 + m_2 - 1\n      | c = \n}}\n{{eqn | r = m_1 + m_2 - 2\n      | c = \n}}\n{{eqn | o = <\n      | r = m_1 + m_2\n      | c = \n}}\n{{eqn | o = \\le\n      | r = n\n      | c = \n}}\n{{end-eqn}}\nThat is:\n:$\\map {\\deg_G} u + \\map {\\deg_G} v < n$\nBut by definition of Ore graph:\n:$\\map {\\deg_G} u + \\map {\\deg_G} v \\ge n$\nBy Proof by Contradiction, it follows that our assumption that $G$ is not connected was false.\nHence the result.\n{{Qed}}\n\\end{proof}\n\n"}}, "21376": {"score": 0.9131611585617065, "content": {"text": "\\section{Simple Graph whose Vertices all Incident but Edges not Adjacent}\nTags: Simple Graphs\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph such that:\n:every vertex is incident with at least one edge\n:no two edges are adjacent to each other.\nThen $G$ has an even number of vertices.\n\\end{theorem}\n\n\\begin{proof}\nSuppose there exists a set of $3$ vertices that are connected.\nThen at least one of these vertices has at least $2$ edges.\nThat would mean that at least $2$ edges were incident with the same vertex.\nThat is, that at least $2$ edges were adjacent to each other.\nSo, for a simple graph to fulfil the conditions, vertices can exist only in $2$-vertex components.\nSo such a simple graph must have an even number of vertices.\n{{qed}}\n\\end{proof}\n\n"}}, "21375": {"score": 0.901847243309021, "content": {"text": "\\section{Simple Graph whose Vertices Incident to All Edges}\nTags: Graph Theory\n\n\\begin{theorem}\nLet $G = \\struct {V, E}$ be a simple graph whose vertices are incident to all its edges.\nThen $G$ is either:\n:the star graph $S_2$, which is also the complete graph $K_2$\n:an edgeless graph of any order.\n\\end{theorem}\n\n\\begin{proof}\nIf $G$ has no edges, then all the vertices are incident to all the edges vacuously.\nSo any of the edgeless graphs $N_n$ for order $n \\in \\Z_{\\ge 0}$ fulfils the criterion.\nSuppose $G$ has more than $2$ vertices $v_1, v_2, v_3$ and at least one edge.\n{{WLOG}}, let one edge be $v_1 v_2$.\nBut $v_3$ cannot be incident to edge $v_1 v_2$.\nSo $G$ can have no more than $2$ vertices.\nFurthermore, there can be only one edge joining those two vertices.\nThe result follows from inspection of $K_2$ and $S_2$.\n{{qed}}\n\\end{proof}\n\n"}}, "21374": {"score": 0.9202666878700256, "content": {"text": "\\section{Simple Graph where All Vertices and All Edges are Adjacent}\nTags: Simple Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph in which:\n:every vertex is adjacent to every other vertex\nand:\n:every edge is adjacent to every other edge.\nThen $G$ is of order no greater than $3$.\n\\end{theorem}\n\n\\begin{proof}\nIt is seen that examples exist of simple graphs which fulfil the criteria where the order of $G$ is no greater than $3$:\n:400px\nThe cases where the order of $G$ is $1$ or $2$ are trivial.\nWhen the order of $G$ is $3$, the criteria can be verified by inspection.\nLet the order of $G = \\struct {V, E}$ be $4$ or more.\nLet $v_1, v_2, v_3, v_4 \\in V$.\nSuppose every vertex is adjacent to every other vertex.\nAs $v_1$ is adjacent to $v_2$, there exists the edge $v_1 v_2$.\nAs $v_3$ is adjacent to $v_4$, there exists the edge $v_3 v_4$.\nBut $v_1 v_2$ and $v_3 v_4$ both join two distinct pairs of vertices.\nThus $v_1 v_2$ and $v_3 v_4$ are not adjacent, by definition.\nSo when there are $4$ or more vertices in $G$, it cannot fulfil the criteria.\n{{qed}}\n\\end{proof}\n\n"}}, "11719": {"score": 0.9070208072662354, "content": {"text": "\\section{Isomorphism Classes for Order 4 Size 3 Simple Graphs}\nTags: Graph Isomorphisms\n\n\\begin{theorem}\nThere are $3$ equivalence classes for simple graphs of order $4$ and size $3$ under isomorphism:\n:400px\n\\end{theorem}\n\n\\begin{proof}\n{{tidy}}\n{{MissingLinks}}\nThe fact that the $3$ graphs given are not isomorphic follows from Vertex Condition for Isomorphic Graphs.\nThe vertices have degrees as follows:\n:Graph $1$: $2, 2, 1, 1$\n:Graph $2$: $3, 1, 1, 1$\n:Graph $3$: $2, 2, 2, 0$\nThe fact that there are no more isomorphism classes of such graphs can be proved constructively.\nLet the $4$ vertices be named $A, B, C$ and $D$.\nLemma: There must be intersections among the edges.\nProof: If there were no intersection at all, it requires at least $3 \\times 2 = 6$ vertices.\n{{WLOG}}, let $2$ of the edges be $AB$ and $AC$.\nTo place the last edge, there are $\\dbinom 4 2 = 6$ potential choices:\n:$AB$: this makes the graph not simple\n:$AC$: this makes the graph not simple\n:$AD$: this is isomorphic to graph $2$\n:$BC$: this is isomorphic to graph $3$\n:$BD$: this is isomorphic to graph $1$\n:$CD$: this is isomorphic to graph $1$.\nHence, by Proof by Cases, these $3$ are the only isomorphism classes.\n{{qed}}\n\\end{proof}\n\n"}}, "23824": {"score": 0.9082427620887756, "content": {"text": "\\section{No Simple Graph is Perfect}\nTags: Simple Graphs, Graph Theory, Perfect Graphs\n\n\\begin{theorem}\nLet $G$ be a simple graph whose order is $2$ or greater.\nThen $G$ is not perfect.\n\\end{theorem}\n\n\\begin{proof}\nRecall that a perfect graph is one where each vertex is of different degree.\nWe note in passing that the simple graph consisting of one vertex trivially fulfils the condition for perfection.\n{{AimForCont}} $G$ is a simple graph of order $n$ where $n \\ge 2$ such that $G$ is perfect.\nFirst, suppose that $G$ has no isolated vertices.\nBy the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n$.\nThat means it must connect to at least $n$ other vertices.\nBut there are only $n - 1$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\nNow suppose $G$ has an isolated vertex.\nThere can be only one, otherwise there would be two vertices of degree zero, and so $G$ would not be perfect.\nAgain by the Pigeonhole Principle, for all vertices to have different degrees, one of them must be of degree at least $n - 1$.\nBut of the remaining $n - 1$ vertices, one of them is of degree zero.\nSo it cannot be adjacent to any vertex.\nSo there are only $n - 2$ other vertices to connect to.\nTherefore $G$ cannot be perfect.\n{{qed}}\n\\end{proof}\n\n"}}}}, "TheoremQA_xinyi/maximum_entropy_2.json": {"gold": {"2371": 1}, "retrieved": {"3633": {"score": 0.8498049378395081, "content": {"text": "\\begin{definition}[Definition:Gamma Distribution]\nLet $X$ be a continuous random variable on a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $\\Img X = \\R_{\\ge 0}$.\n$X$ is said to have a '''Gamma distribution''' if it has probability density function: \n:$\\map {f_X} x = \\dfrac {\\beta^\\alpha x^{\\alpha - 1} e^{-\\beta x} } {\\map \\Gamma \\alpha}$\nfor $\\alpha, \\beta > 0$, where $\\Gamma$ is the Gamma function.\nThis is written: \n:$X \\sim \\map \\Gamma {\\alpha, \\beta}$\n\\end{definition}"}}, "23274": {"score": 0.8520276546478271, "content": {"text": "\\section{Variance of Beta Distribution}\nTags: Beta Distribution, Variance of Beta Distribution, Variance\n\n\\begin{theorem}\nLet $X \\sim \\map \\Beta {\\alpha, \\beta}$ for some $\\alpha, \\beta > 0$, where $\\Beta$ is the Beta distribution. \nThen:\n:$\\var X = \\dfrac {\\alpha \\beta} {\\paren {\\alpha + \\beta}^2 \\paren {\\alpha + \\beta + 1} }$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of the Beta distribution, $X$ has probability density function:\n:$\\map {f_X} x = \\dfrac {x^{\\alpha - 1} \\paren {1 - x}^{\\beta - 1} } {\\map \\Beta {\\alpha, \\beta} }$\nFrom Variance as Expectation of Square minus Square of Expectation: \n:$\\displaystyle \\var X = \\int_0^1 x^2 \\map {f_X} X \\rd x - \\paren {\\expect X}^2$\nSo: \n{{begin-eqn}}\n{{eqn\t| l = \\var X\n\t| r = \\frac 1 {\\map \\Beta {\\alpha, \\beta} } \\int_0^1 x^{\\alpha + 1} \\paren {1 - x}^{\\beta - 1} \\rd x - \\frac {\\alpha^2} {\\paren {\\alpha + \\beta}^2}\n\t| c = Expectation of Beta Distribution\n}}\n{{eqn\t| r = \\frac {\\map \\Beta {\\alpha + 2, \\beta} } {\\map \\Beta {\\alpha, \\beta} } - \\frac {\\alpha^2} {\\paren {\\alpha + \\beta}^2}\n\t| c = {{Defof|Beta Function|index = 1}}\n}}\n{{eqn\t| r = \\frac {\\map \\Gamma {\\alpha + 2} \\, \\map \\Gamma \\beta} {\\map \\Gamma {\\alpha + \\beta + 2} } \\cdot \\frac {\\map \\Gamma {\\alpha + \\beta} } {\\map \\Gamma \\alpha \\, \\map \\Gamma \\beta} - \\frac {\\alpha^2} {\\paren {\\alpha + \\beta}^2}\n\t| c = {{Defof|Beta Function|index = 3}}\n}}\n{{eqn\t| r = \\frac {\\alpha \\paren {\\alpha + 1} } {\\paren {\\alpha + \\beta} \\paren {\\alpha + \\beta + 1} } \\cdot \\frac {\\map \\Gamma \\alpha \\, \\map \\Gamma \\beta \\, \\map \\Gamma {\\alpha + \\beta} } {\\map \\Gamma \\alpha \\, \\map \\Gamma \\beta \\, \\map \\Gamma {\\alpha + \\beta} } - \\frac {\\alpha^2} {\\paren {\\alpha + \\beta}^2}\n\t| c = Gamma Difference Equation\n}}\n{{eqn\t| r = \\frac {\\paren {\\alpha^2 + \\alpha} \\paren {\\alpha + \\beta} } {\\paren {\\alpha + \\beta}^2 \\paren {\\alpha + \\beta + 1} } - \\frac {\\alpha^2 \\paren {\\alpha + \\beta + 1} } {\\paren {\\alpha + \\beta}^2 \\paren {\\alpha + \\beta + 1} }\n}}\n{{eqn\t| r = \\frac {\\alpha ^3 + \\alpha^2 \\beta + \\alpha^2 + \\alpha \\beta - \\alpha^3 - \\alpha^2 \\beta - \\alpha^2} {\\paren {\\alpha + \\beta}^2 \\paren {\\alpha + \\beta + 1} }\n}}\n{{eqn\t| r = \\frac {\\alpha \\beta} {\\paren {\\alpha + \\beta}^2 \\paren {\\alpha + \\beta + 1} }\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Variance\nCategory:Beta Distribution\n515976\n440865\n2021-04-14T21:10:43Z\nRobkahn131\n3897\n515976\nwikitext\ntext/x-wiki\n\\end{proof}\n\n"}}, "2371": {"score": 0.8672321438789368, "content": {"text": "\\begin{definition}[Definition:Differential Entropy]\n'''Differential entropy''' extends the concept of entropy to continuous random variables.\nLet $X$ be a continuous random variable.\nLet $X$ have probability density function $f_X$. \nThen the '''differential entropy''' of $X$, $\\map h X$ measured in nats, is given by: \n:$\\ds \\map h X = -\\int_{-\\infty}^\\infty \\map {f_X} x \\ln \\map {f_X} x \\rd x$\nWhere $\\map {f_X} x = 0$, we take $\\map {f_X} x \\ln \\map {f_X} x = 0$ by convention.\n\\end{definition}"}}, "13800": {"score": 0.865635335445404, "content": {"text": "\\section{Exponential of Negative of Exponential Random Variable has Beta Distribution}\nTags: Exponential Distribution, Beta Distribution\n\n\\begin{theorem}\nLet $\\beta$ be a positive real number.\nLet $X \\sim \\Exponential \\beta$ where $\\Exponential \\beta$ is the exponential distribution with parameter $\\beta$. \nThen:\n:$e^{-X} \\sim \\BetaDist {\\dfrac 1 \\beta} 1$\n\\end{theorem}\n\n\\begin{proof}\nNote that if:\n:$Y \\sim \\BetaDist {\\dfrac 1 \\beta} 1$\nthen the probability density function of $Y$, $f_Y$ is given by:\n{{begin-eqn}}\n{{eqn\t| l = \\map {f_Y} y\n\t| r = \\frac {y^{\\frac 1 \\beta - 1} \\paren {1 - y}^{1 - 1} } {\\map \\Beta {\\frac 1 \\beta, 1} }\n\t| c = {{Defof|Beta Distribution}}\n}}\n{{eqn\t| r = \\frac {y^{\\frac 1 \\beta - 1} } {\\frac {\\map \\Gamma {\\frac 1 \\beta} \\map \\Gamma 1} {\\map \\Gamma {\\frac 1 \\beta + 1} } }\n\t| c = {{Defof|Beta Function}}\n}}\n{{eqn\t| r = \\frac 1 {\\frac {\\map \\Gamma {\\frac 1 \\beta} } {\\frac 1 \\beta \\map \\Gamma {\\frac 1 \\beta} } } y^{\\frac 1 \\beta}\n\t| c = Gamma Difference Equation\n}}\n{{eqn\t| r = \\frac 1 \\beta y^{\\frac 1 \\beta}\n}}\n{{end-eqn}}\nfor each $y > 0$. \nLet:\n:$Z = e^{-X}$\nIt suffices to show that $Z$ has the same probability density function as $Y$.\nWe have: \n{{begin-eqn}}\n{{eqn\t| l = \\map \\Pr {Z \\le z}\n\t| r = \\map \\Pr {e^{-X} \\le z}\n}}\n{{eqn\t| r = \\map \\Pr {-X \\le \\ln z}\n}}\n{{eqn\t| r = \\map \\Pr {X \\ge -\\ln z}\n}}\n{{eqn\t| r = \\map \\exp {-\\frac {-\\ln z} {\\beta} }\n\t| c = {{Defof|Exponential Distribution}}\n}}\n{{eqn\t| r = z^{\\frac 1 \\beta}\n}}\n{{end-eqn}}\nBy Derivative of Power, the probability density function of $Z$, $f_Z$ is therefore given by: \n:$\\map {f_Z} z = \\dfrac 1 \\beta z^{\\frac 1 \\beta - 1}$\nfor each $z > 0$.\nSo:\n:$f_Y = f_Z$\n{{qed}}\nCategory:Exponential Distribution\nCategory:Beta Distribution\n\\end{proof}\n\n"}}, "13875": {"score": 0.8599751591682434, "content": {"text": "\\section{Expectation of Beta Distribution}\nTags: Expectation of Beta Distribution, Beta Distribution, Expectation\n\n\\begin{theorem}\nLet $X \\sim \\BetaDist \\alpha \\beta$ for some $\\alpha, \\beta > 0$, where $\\operatorname{Beta}$ denotes the beta distribution. \nThen:\n:$\\expect X = \\dfrac \\alpha {\\alpha + \\beta}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of the Beta distribution, $X$ has probability density function:\n:$\\displaystyle f_X \\left({x}\\right) = \\frac { x^{\\alpha - 1} \\left({1 - x}\\right)^{\\beta - 1} } {\\Beta \\left({\\alpha, \\beta}\\right)}$\nFrom the definition of the expected value of a continuous random variable: \n:$\\displaystyle \\mathbb E \\left[{X}\\right] = \\int_0^1 x f_X \\left({x}\\right) \\rd x$\nSo:\n{{begin-eqn}}\n{{eqn\t| l = \\mathbb E \\left[{X}\\right]\n\t| r = \\frac 1 {\\Beta \\left({\\alpha, \\beta}\\right)} \\int_0^1 x^\\alpha \\left({1 - x}\\right)^{\\beta - 1} \\rd x\n}}\n{{eqn\t| r = \\frac {\\Beta \\left({\\alpha + 1, \\beta}\\right)} {\\Beta \\left({\\alpha, \\beta}\\right)} \n\t| c = {{Defof|Beta Function/Definition 1|Beta Function}}\n}}\n{{eqn\t| r = \\frac {\\Gamma \\left({\\alpha + 1}\\right) \\Gamma \\left({\\beta}\\right)} {\\Gamma \\left({\\alpha + \\beta + 1}\\right)} \\cdot \\frac {\\Gamma \\left({\\alpha + \\beta}\\right)} {\\Gamma \\left({\\alpha}\\right) \\Gamma \\left({\\beta}\\right)}\n\t| c = {{Defof|Beta Function/Definition 3|Beta Function}}\n}}\n{{eqn\t| r = \\frac \\alpha {\\alpha + \\beta} \\cdot \\frac {\\Gamma \\left({\\alpha}\\right) \\Gamma \\left({\\beta}\\right) \\Gamma \\left({\\alpha + \\beta}\\right)} {\\Gamma \\left({\\alpha}\\right) \\Gamma \\left({\\beta}\\right) \\Gamma \\left({\\alpha + \\beta}\\right)}\n\t| c = Gamma Difference Equation\n}}\n{{eqn\t| r = \\frac \\alpha {\\alpha + \\beta}\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Expectation\nCategory:Beta Distribution\n351285\n351120\n2018-04-25T15:04:19Z\nCaliburn\n3218\n351285\nwikitext\ntext/x-wiki\n\\end{proof}\n\n"}}, "15018": {"score": 0.8850268721580505, "content": {"text": "\\section{Differential Entropy of Continuous Uniform Distribution}\nTags: Uniform Distribution, Differential Entropy, Continuous Uniform Distribution\n\n\\begin{theorem}\nLet $X \\sim \\ContinuousUniform a b$ for some $a, b \\in \\R$, $a \\ne b$, where $\\operatorname U$ is the continuous uniform distribution. \nThen the differential entropy of $X$, $\\map h X$, is given by: \n:$\\map h X = \\map \\ln {b - a}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of the continuous uniform distribution, $X$ has probability density function: \n:$\\map {f_X} x = \\begin{cases} \\dfrac 1 {b - a} & a \\le x \\le b \\\\ 0 & \\text{otherwise} \\end{cases}$\nFrom the definition of differential entropy: \n:$\\ds \\map h X = - \\int_{-\\infty}^\\infty \\map {f_X} x \\map \\ln {\\map {f_X} x} \\rd x$\nSo: \n{{begin-eqn}}\n{{eqn\t| l = \\map h X\n\t| r = -\\int_a^b \\frac 1 {b - a} \\map \\ln {\\frac 1 {b - a} } \\rd x\n}}\n{{eqn\t| r = \\frac {\\map \\ln {b - a} } {b - a} \\int_a^b \\rd x\n\t| c = Logarithm of Reciprocal\n}}\n{{eqn\t| r = \\frac {\\map \\ln {b - a} } {b - a} \\sqbrk x_a^b \n\t| c = Primitive of Constant, Fundamental Theorem of Calculus\n}}\n{{eqn\t| r = \\frac {\\paren {b - a} \\map \\ln {b - a} } {b - a}\n}}\n{{eqn\t| r = \\map \\ln {b - a}\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Continuous Uniform Distribution\nCategory:Differential Entropy\n\\end{proof}\n\n"}}, "3666": {"score": 0.8695729970932007, "content": {"text": "\\begin{definition}[Definition:Generalized Inverse Gaussian Distribution]\n<!-- Inside here reside a considerable number of results to be extracted.\nThe original of this can probably still be found on [https://groups.google.com/forum/#!topic/sci.math/zNONo7hyOkQ].\nThis page was found on Wikipedia about to be deleted. I rescued it and put it here so it would not be lost.\n{{Probability distribution|\n   name       =Generalized inverse Gaussian|\n   type       =density|\n   pdf_image  =|\n   cdf_image  =|\n   parameters =''a'' > 0, ''b'' > 0, ''p'' real|\n   support    =''x'' > 0|\n   pdf        =$f(x) = \\frac{(a/b)^{p/2}}{2 K_p(\\sqrt{ab})} x^{(p-1)} e^{-(ax + b/x)/2}$|\n   cdf        =|\n   mean       =$\\frac{\\sqrt{b}\\ K_{p+1}(\\sqrt{a b}) }{ \\sqrt{a}\\ K_{p}(\\sqrt{a b})}$|\n   median     =|\n   mode       =$\\frac{(p-1)+\\sqrt{(p-1)^2+ab}}{a}$|\n   variance   =$\\left(\\frac{b}{a}\\right)\\left[\\frac{K_{p+2}(\\sqrt{ab})}{K_p(\\sqrt{ab})}-\\left(\\frac{K_{p+1}(\\sqrt{ab})}{K_p(\\sqrt{ab})}\\right)^2\\right]$|\n   skewness   =|\n   kurtosis   =|\n   entropy    =|\n   mgf        =$\\left(\\frac{a}{a-2t}\\right)^{\\frac{p}{2}}\\frac{K_p(\\sqrt{b(a-2t})}{K_p(\\sqrt{ab})}$|\n   char       =$\\left(\\frac{a}{a-2it}\\right)^{\\frac{p}{2}}\\frac{K_p(\\sqrt{b(a-2it})}{K_p(\\sqrt{ab})}$|\n }}\n-->\nThe '''generalized inverse Gaussian distribution''' ('''GIG''')  is a three-parameter family of continuous probability distributions with probability density function:\n:$\\forall x > 0: \\map f x = \\dfrac {\\paren {a / b}^{p/2} } {2 \\map {K_p} {\\sqrt{a b} } } x^{\\paren {p - 1} } e^{-\\paren {a x + b / x} / 2}$\nwhere:\n:$K_p$ is a modified Bessel function of the second kind\n:$a > 0, b > 0, p$ are real.\nCategory:Definitions/Examples of Probability Distributions\n\\end{definition}"}}, "15019": {"score": 0.9024010300636292, "content": {"text": "\\section{Differential Entropy of Exponential Distribution}\nTags: Exponential Distribution, Differential Entropy\n\n\\begin{theorem}\nLet $X$ be a continuous random variable of the exponential distribution with parameter $\\beta$ for some $\\beta \\in \\R_{> 0}$. \nThen the differential entropy of $X$, $\\map h X$, is given by: \n:$\\map h X = 1 + \\map \\ln \\beta$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of the exponential distribution, $X$ has probability density function:\n:$\\map {f_X} x = \\dfrac 1 \\beta e^{-\\frac x \\beta}$\nFrom the definition of differential entropy: \n:$\\ds \\map h X = -\\int_0^\\infty \\map {f_X} x \\map \\ln {\\map {f_X} x} \\rd x$\nSo:\n{{begin-eqn}}\n{{eqn\t| l = \\map h X\n\t| r = -\\frac 1 \\beta \\int_0^\\infty e^{-\\frac x \\beta} \\map \\ln {\\frac 1 \\beta e^{-\\frac x \\beta} } \\rd x\n}}\n{{eqn\t| r = \\frac 1 \\beta \\int_0^\\infty e^{-\\frac x \\beta} \\map \\ln {\\beta e^{\\frac x \\beta} } \\rd x\n\t| c = Reciprocal of Logarithm\n}}\n{{eqn\t| r = \\frac {\\map \\ln \\beta} \\beta \\int_0^\\infty e^{-\\frac x \\beta} \\rd x + \\frac 1 {\\beta^2} \\int_0^\\infty x e^{-\\frac x \\beta} \\rd x\n\t| c = Logarithm of Product, {{Defof|Natural Logarithm}}\n}}\n{{eqn\t| r = \\frac {\\map \\ln \\beta} \\beta \\sqbrk {-\\beta e^{-\\frac x \\beta} }_0^\\infty + \\frac 1 {\\beta^2} \\paren {\\sqbrk {-\\beta x e^{-\\frac x \\beta} }_0^\\infty + \\beta \\int_0^\\infty e^{-\\frac x \\beta} \\rd x}\n\t| c = Primitive of Exponential Function, Integration by Parts\n}}\n{{eqn\t| r = \\frac {\\beta \\map \\ln \\beta} \\beta + \\frac 1 \\beta \\int_0^\\infty e^{-\\frac x \\beta} \\rd x\n\t| c = Exponential Tends to Zero and Infinity, Limit at Infinity of Polynomial over Real Exponential\n}}\n{{eqn\t| r = \\map \\ln \\beta + \\frac 1 \\beta \\sqbrk {-\\beta e^{-\\frac x \\beta} }_0^\\infty\n\t| c = Primitive of Exponential Function\n}}\n{{eqn\t| r = \\map \\ln \\beta + \\frac \\beta \\beta\n\t| c = Exponential Tends to Zero and Infinity, Exponential of Zero\n}}\n{{eqn\t| r = 1 + \\map \\ln \\beta\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Differential Entropy\nCategory:Exponential Distribution\n\\end{proof}\n\n"}}, "5305": {"score": 0.8726206421852112, "content": {"text": "\\begin{definition}[Definition:Log Normal Distribution]\nLet $X$ be a continuous random variable on a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $\\Img X = \\R_{>0}$.\n$X$ is said to have a '''log normal distribution''' {{iff}} it has probability density function: \n:$\\ds \\map {f_X} x = \\dfrac 1 {\\sigma \\sqrt {2 \\pi} x } \\map \\exp {-\\dfrac {\\paren {\\map \\ln x - \\mu}^2} {2 \\sigma^2} }$\nfor $\\mu \\in \\R, \\sigma \\in \\R_{> 0}$.\n\\end{definition}"}}, "15020": {"score": 0.8808473944664001, "content": {"text": "\\section{Differential Entropy of Gaussian Distribution}\nTags: Gaussian Distribution, Differential Entropy\n\n\\begin{theorem}\nLet $X \\sim \\Gaussian \\mu {\\sigma^2}$ for some $\\mu \\in \\R, \\sigma \\in \\R_{> 0}$, where $N$ is the Gaussian distribution.\nThen the differential entropy $\\map h X$ of $X$ is given by: \n:$\\map h X = \\map \\ln {\\sigma \\sqrt {2 \\pi} } + \\dfrac 1 2$\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of the Gaussian distribution, $X$ has probability density function:\n:$\\map {f_X} x = \\dfrac 1 {\\sigma \\sqrt {2 \\pi} } \\, \\map \\exp {-\\dfrac {\\paren {x - \\mu}^2} {2 \\sigma^2} }$\nFrom the definition of differential entropy: \n:$\\ds \\map h X = -\\int_{-\\infty}^\\infty \\map {f_X} x \\ln \\map {f_X} x \\rd x$\nSo:\n{{begin-eqn}}\n{{eqn\t| l = \\map h X\n\t| r = - \\frac 1 {\\sigma \\sqrt{2 \\pi} } \\int_{-\\infty}^\\infty \\map \\exp {-\\frac {\\paren {x - \\mu}^2} {2 \\sigma^2} } \\map \\ln {\\frac 1 {\\sigma \\sqrt {2 \\pi} } \\map \\exp {-\\frac {\\paren {x - \\mu}^2} {2 \\sigma^2} } } \\rd x\n}}\n{{eqn\t| r = \\frac 1 {\\sigma \\sqrt {2 \\pi} } \\int_{-\\infty}^\\infty \\map \\exp {-\\frac {\\paren {x - \\mu}^2} {2 \\sigma^2} } \\map \\ln {\\sigma \\sqrt {2 \\pi} \\, \\map \\exp {\\frac {\\paren {x - \\mu}^2} {2 \\sigma^2} } } \\rd x\n\t| c = Logarithm of Reciprocal\n}}\n{{eqn\t| r = \\frac {\\sqrt 2 \\sigma} { \\sigma \\sqrt {2 \\pi} } \\int_{-\\infty}^\\infty \\map \\exp {-t^2} \\map \\ln {\\sigma \\sqrt {2 \\pi} \\, \\map \\exp {t^2} } \\rd t\n\t| c = substituting $t = \\dfrac {x - \\mu} {\\sqrt 2 \\sigma}$\n}}\n{{eqn\t| r = \\frac 1 {\\sqrt \\pi} \\int_{-\\infty}^\\infty \\paren {\\map \\ln {\\sigma \\sqrt {2 \\pi} } + \\map \\ln {\\map \\exp {t^2} } } \\, \\map \\exp {-t^2} \\rd t\n\t| c = Sum of Logarithms\n}}\n{{eqn\t| r = \\frac {\\map \\ln {\\sigma \\sqrt {2 \\pi} } } {\\sqrt \\pi} \\int_{-\\infty}^\\infty \\map \\exp {-t^2} \\rd t + \\frac 1 {\\sqrt \\pi} \\int_{-\\infty}^\\infty t^2 \\map \\exp {-t^2} \\rd t\n}}\n{{eqn\t| r = \\frac {\\sqrt \\pi \\, \\map \\ln {\\sigma \\sqrt {2 \\pi} } } {\\sqrt \\pi} + \\frac 1 {\\sqrt \\pi} \\paren {\\sqbrk {-\\frac t 2 \\, \\map \\exp {-t^2} }_{-\\infty}^\\infty + \\frac 1 2 \\int_{-\\infty}^\\infty \\map \\exp {-t^2} \\rd t}\n\t| c = Gaussian Integral, Integration by Parts, Fundamental Theorem of Calculus\n}}\n{{eqn\t| r = \\map \\ln {\\sigma \\sqrt {2 \\pi} } + \\frac 1 {2 \\sqrt \\pi} \\int_{-\\infty}^\\infty \\map \\exp {-t^2} \\rd t\n\t| c = Exponential Tends to Zero and Infinity\n}}\n{{eqn\t| r = \\map \\ln {\\sigma \\sqrt {2 \\pi} } + \\frac {\\sqrt \\pi} {2 \\sqrt \\pi}\n\t| c = Gaussian Integral\n}}\n{{eqn\t| r = \\map \\ln {\\sigma \\sqrt {2 \\pi} } + \\frac 1 2\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Gaussian Distribution\nCategory:Differential Entropy\n\\end{proof}\n\n"}}}}, "TheoremQA_elainewan/math_calculus_12.json": {"gold": {"15210": 1}, "retrieved": {"15195": {"score": 0.8314478397369385, "content": {"text": "\\section{Derivative at Maximum or Minimum}\nTags: Differential Calculus, Derivative at Maximum or Minimum\n\n\\begin{theorem}\nLet $f$ be a real function which is differentiable on the open interval $\\openint a b$.\nLet $f$ have a local minimum or local maximum at $\\xi \\in \\openint a b$.\nThen:\n:$\\map {f'} \\xi = 0$\n\\end{theorem}\n\n\\begin{proof}\nBy definition of derivative at a point:\n:$\\dfrac {\\map f x - \\map f \\xi} {x - \\xi} \\to \\map {f'} \\xi$ as $x \\to \\xi$\nSuppose $\\map {f'} \\xi > 0$.\nThen from Behaviour of Function Near Limit it follows that:\n:$\\exists I = \\openint {\\xi - h} {\\xi + h}: \\dfrac {\\map f x - \\map f \\xi} {x - \\xi} > 0$\nprovided that $x \\in I$ and $x \\ne \\xi$.\nNow let $x_1$ be any number in the open interval $\\openint {\\xi - h} \\xi$.\nThen:\n:$x_1 - \\xi < 0$\nand hence from:\n:$\\dfrac {\\map f {x_1} - \\map f \\xi} {x_1 - \\xi} > 0$\nit follows that:\n:$\\map f {x_1} < \\map f \\xi$\nThus $f$ can not have a local minimum at $\\xi$.\nNow let $x_2$ be any number in the open interval $\\openint \\xi {\\xi + h}$.\nThen:\n:$x_2 - \\xi > 0$\nand hence from:\n:$\\dfrac {\\map f {x_2} - \\map f \\xi} {x_2 - \\xi} > 0$\nit follows that:\n:$\\map f {x_2} > \\map f \\xi$\nThus $f$ can not have a local maximum at $\\xi$ either.\nA similar argument can be applied to $-f$ to handle the case where $\\map {f'} \\xi < 0$.\nThe only other possibility is that $\\map {f'} \\xi = 0$, hence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "21298": {"score": 0.8329562544822693, "content": {"text": "\\section{Shape of Sine Function}\nTags: Sine Function, Analysis\n\n\\begin{theorem}\nThe sine function is:\n:$(1): \\quad$ strictly increasing on the interval $\\closedint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$\n:$(2): \\quad$ strictly decreasing on the interval $\\closedint {\\dfrac \\pi 2} {\\dfrac {3 \\pi} 2}$\n:$(3): \\quad$ concave on the interval $\\closedint 0 \\pi$\n:$(4): \\quad$ convex on the interval $\\closedint \\pi {2 \\pi}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the discussion of Sine and Cosine are Periodic on Reals, we have that:\n: $\\sin \\paren {x + \\dfrac \\pi 2} = \\cos x$\nThe result then follows directly from the Shape of Cosine Function.\n\\end{proof}\n\n"}}, "11899": {"score": 0.8351512551307678, "content": {"text": "\\section{Interval between Local Maxima for Underdamped Free Vibration}\nTags: Mathematical Physics\n\n\\begin{theorem}\nConsider a physical system $S$ whose behaviour can be described with the second order ODE in the form:\n:$(1): \\quad \\dfrac {\\d^2 x} {\\d t^2} + 2 b \\dfrac {\\d x} {\\d t} + a^2 x = 0$\nfor $a, b \\in \\R_{>0}$.\nLet $b < a$, so as to make $S$ underdamped.\n:600px\nLet $T$ be the period of oscillation of $S$.\nThen the successive local maxima of $x$ occur for $t = 0, T, 2T, \\ldots$\n\\end{theorem}\n\n\\begin{proof}\nLet the position of $S$ be described in the canonical form:\n:$(1): \\quad x = \\dfrac {x_0 \\, a} \\alpha e^{-b t} \\map \\cos {\\alpha t - \\theta}$\nwhere:\n:$\\alpha = \\sqrt {a^2 - b^2}$.\n:$\\theta = \\map \\arctan {\\dfrac b \\alpha}$\nFrom Period of Oscillation of Underdamped System is Regular, the period of oscillation $T$ is given by:\n:$T = \\dfrac {2 \\pi} {a^2 - b^2}$\nDifferentiating {{{WRT|Differentiation}} $t$:\n{{begin-eqn}}\n{{eqn | l = x'\n      | r = -b \\dfrac {x_0 \\, a} \\alpha e^{-b t} \\map \\cos {\\alpha t - \\theta} - \\dfrac {x_0 \\, a} \\alpha e^{-b t} \\alpha \\map \\sin {\\omega t - \\theta}\n      | c = \n}}\n{{eqn | r = -\\dfrac {x_0 \\, a} \\alpha e^{-b t} \\paren {b \\map \\cos {\\alpha t - \\theta} + \\alpha \\map \\sin {\\alpha t - \\theta} }\n      | c = \n}}\n{{end-eqn}}\nFrom Derivative at Maximum or Minimum, the local maxima and local minima of $x$ occur at $x' = 0$:\n{{begin-eqn}}\n{{eqn | l = x'\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = -\\dfrac {x_0 \\, a} \\alpha e^{-b t} \\paren {b \\map \\cos {\\alpha t - \\theta} + \\alpha \\map \\sin {\\alpha t - \\theta} }\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = b \\map \\cos {\\alpha t - \\theta}\n      | r = -\\alpha \\map \\sin {\\alpha t - \\theta}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map \\tan {\\alpha t - \\theta}\n      | r = -\\frac b \\alpha\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac {\\tan \\alpha t - \\tan \\theta} {1 + \\tan \\alpha t \\tan \\theta}\n      | r = -\\frac b \\alpha\n      | c = Tangent of Difference\n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac {\\tan \\alpha t - \\dfrac b \\alpha} {1 + \\tan \\alpha t \\dfrac b \\alpha}\n      | r = -\\frac b \\alpha\n      | c = Definition of $\\theta$\n}}\n{{eqn | ll= \\leadsto\n      | l = \\tan \\alpha t - \\dfrac b \\alpha \n      | r = -\\frac b \\alpha - \\tan \\alpha t \\dfrac {b^2} {\\alpha^2}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\tan \\alpha t \\paren {1 + \\dfrac {b^2} {\\alpha^2} }\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\tan \\alpha t\n      | r = 0\n      | c = as $1 + \\dfrac {b^2} {\\alpha^2} > 0$\n}}\n{{eqn | ll= \\leadsto\n      | l = \\alpha t\n      | r = 0, \\pi, 2 \\pi, \\ldots\n      | c = Tangent of Zero, Tangent of Angle plus Straight Angle\n}}\n{{end-eqn}}\nIt remains to be determined which of these points at which $x' = 0$ are local maxima.\nThis occurs when $x > 0$.\nFrom Cosine of Angle plus Full Angle:\n:$\\cos x = \\map \\cos {2 \\pi + x}$\nWe have that at $x = x_0$ at $t = 0$.\nIt is given that $x_0 > 0$.\nSo at $t = 0, 2 \\pi, 4 \\pi, \\ldots$ we have that:\n:$\\cos \\alpha t > 0$\nSimilarly, from Cosine of Angle plus Straight Angle:\n:$\\cos x = -\\map \\cos {\\pi + x}$\nSo at $t = \\pi, 3 \\pi, 5 \\pi, \\ldots$ we have that:\n:$\\cos \\alpha t < 0$\nThus we have that:\n:$\\alpha L = 2 \\pi$\nwhere $L$ is the value of $t$ between consecutive local maxima of $x$.\nThus:\n:$L = \\dfrac {2 \\pi} {\\alpha} = \\dfrac {2 \\pi} {a^2 - b^2} = T$\nas required.\n{{qed}}\n\\end{proof}\n\n"}}, "22562": {"score": 0.834595799446106, "content": {"text": "\\section{Tangent Inequality}\nTags:  Trigonometry, Tangent Function, Inequalities\n\n\\begin{theorem}\n:$x < \\tan x$\nfor all $x$ in the interval $\\left({0 \\,.\\,.\\, \\dfrac {\\pi} 2}\\right)$.\n\\end{theorem}\n\n\\begin{proof}\nLet $f \\left({x}\\right) = \\tan x - x$.\nBy Derivative of Tangent Function, $f' \\left({x}\\right) = \\sec^2 x - 1$.\nBy Shape of Secant Function, $\\sec^2 x > 1$ for $x \\in \\left({0 \\,.\\,.\\, \\dfrac {\\pi} 2}\\right)$.\nHence $f' \\left({x}\\right) > 0$.\nFrom Derivative of Monotone Function, $f \\left({x}\\right)$ is strictly increasing in this interval.\nSince $f \\left({0}\\right) = 0$, it follows that $f \\left({x}\\right) > 0$ for all $x$ in $x \\in \\left({0 \\,.\\,.\\, \\dfrac {\\pi} 2}\\right)$.\n{{qed}}\nCategory:Tangent Function\nCategory:Inequalities\n\\end{proof}\n\n"}}, "21295": {"score": 0.8344367742538452, "content": {"text": "\\section{Shape of Cosine Function}\nTags: Analysis, Cosine Function\n\n\\begin{theorem}\nThe cosine function is:\n:$(1): \\quad$ strictly decreasing on the interval $\\closedint 0 \\pi$\n:$(2): \\quad$ strictly increasing on the interval $\\closedint \\pi {2 \\pi}$\n:$(3): \\quad$ concave on the interval $\\closedint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$\n:$(4): \\quad$ convex on the interval $\\closedint {\\dfrac \\pi 2} {\\dfrac {3 \\pi} 2}$\n\\end{theorem}\n\n\\begin{proof}\nFrom the discussion of Sine and Cosine are Periodic on Reals, we know that:\n:$\\cos x \\ge 0$ on the closed interval $\\closedint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$\nand:\n:$\\cos x > 0$ on the open interval $\\openint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$\nFrom the same discussion, we have that:\n:$\\map \\sin {x + \\dfrac \\pi 2} = \\cos x$\nSo immediately we have that $\\sin x \\ge 0$ on the closed interval $\\closedint 0 \\pi$, $\\sin x > 0$ on the open interval $\\openint 0 \\pi$.\nBut $\\map {D_x} {\\cos x} = -\\sin x$ from Derivative of Cosine Function.\nThus from Derivative of Monotone Function, $\\cos x$ is strictly decreasing on $\\closedint 0 \\pi$.\nFrom Derivative of Sine Function it follows that:\n:$\\map {D_{xx} } {\\cos x} = -\\cos x$\nOn $\\closedint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$ where $\\cos x \\ge 0$, therefore, $\\map {D_{xx} } {\\cos x} \\le 0$.\nFrom Second Derivative of Concave Real Function is Non-Positive it follows that $\\cos x$ is concave on $\\closedint {-\\dfrac \\pi 2} {\\dfrac \\pi 2}$.\nThe rest of the result follows similarly.\n\\end{proof}\n\n"}}, "21788": {"score": 0.8690357804298401, "content": {"text": "\\section{Steiner's Calculus Problem}\nTags: Euler's Number\n\n\\begin{theorem}\nLet $f: \\R_{>0} \\to \\R$ be the real function defined as:\n:$\\forall x \\in \\R_{>0}: \\map f x = x^{1/x}$\nThen $\\map f x$ reaches its maximum at $x = e$ where $e$ is Euler's number .\n\\end{theorem}\n\n\\begin{proof}\n{{begin-eqn}}\n{{eqn | l = \\map {f'} x\n      | r = \\frac \\d {\\d x} x^{1/x}\n}}\n{{eqn | r = \\frac \\d {\\d x} e^{\\ln x / x}\n}}\n{{eqn | r = e^{\\ln x / x} \\paren {\\frac 1 {x^2} - \\frac {\\ln x} {x^2} }\n}}\n{{eqn | r = \\frac {x^{1/x} } {x^2} \\paren {1 - \\ln x}\n}}\n{{end-eqn}}\n$\\dfrac {x^{1/x} } {x^2}$ is always greater than $0$.\nTherefore:\n:$\\map {f'} x > 0$ for $\\ln x < 1$\n:$\\map {f'} x = 0$ for $\\ln x = 1$\n:$\\map {f'} x < 0$ for $\\ln x > 1$\nBy Derivative at Maximum or Minimum, maximum is obtained when $\\ln x = 1$,\nthat is, when $x = e$.\n{{qed}}\n{{Namedfor|Jakob Steiner|cat = Steiner}}\n\\end{proof}\n\n"}}, "10783": {"score": 0.8520060777664185, "content": {"text": "\\section{Maximum Rate of Change of X Coordinate of Cycloid}\nTags: Cycloids\n\n\\begin{theorem}\nLet a circle $C$ of radius $a$ roll without slipping along the x-axis of a cartesian plane at a constant speed such that the center moves with a velocity $\\mathbf v_0$ in the direction of increasing $x$.\nConsider a point $P$ on the circumference of this circle.\nLet $\\tuple {x, y}$ be the coordinates of $P$ as it travels over the plane.\nThe maximum rate of change of $x$ is $2 \\mathbf v_0$, which happens when $P$ is at the top of the circle $C$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Rate of Change of Cartesian Coordinates of Cycloid, the rate of change of $x$ is given by:\n:$\\dfrac {\\d x} {\\d t} = \\mathbf v_0 \\paren {1 - \\cos \\theta}$\nThis is a maximum when $1 - \\cos \\theta$ is a maximum.\nThat happens when $\\cos \\theta$ is at a minimum.\nThat happens when $\\cos \\theta = -1$.\nThat happens when $\\theta = \\pi, 3 \\pi, \\ldots$\nThat is, when $\\theta = \\paren {2 n + 1} \\pi$ where $n \\in \\Z$.\nThat is, when $P$ is at the top of the circle $C$.\nWhen $\\cos \\theta = -1$ we have:\n{{begin-eqn}}\n{{eqn | l = \\frac {\\d x} {\\d t}\n      | r = \\mathbf v_0 \\paren {1 - \\paren {-1} }\n      | c = \n}}\n{{eqn | r = 2 \\mathbf v_0\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "10775": {"score": 0.882152795791626, "content": {"text": "\\section{Maximum Abscissa for Loop of Folium of Descartes}\nTags: Folium of Descartes\n\n\\begin{theorem}\nConsider the folium of Descartes defined in parametric form as:\n:$\\begin {cases} x = \\dfrac {3 a t} {1 + t^3} \\\\ y = \\dfrac {3 a t^2} {1 + t^3} \\end {cases}$\n:500px\nThe point on the loop at which the $x$ value is at a maximum occurs when $t = \\sqrt [3] {\\dfrac 1 2}$, corresponding to the point $P$ defined as:\n:$P = \\tuple {2^{2/3} a, 2^{1/3} a}$\n\\end{theorem}\n\n\\begin{proof}\nWe calculate the derivative of $x$ {{WRT|Differentiation}} $t$:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {\\d x} {\\d t}\n      | r = \\map {\\dfrac \\d {\\d t} } {\\dfrac {3 a t} {1 + t^3} }\n      | c = \n}}\n{{eqn | r = \\dfrac {\\paren {1 + t^3} \\times 3 a - 3 a t \\paren {3 t^2} } {\\paren {1 + t^3}^2}\n      | c = Quotient Rule for Derivatives\n}}\n{{eqn | r = \\dfrac {3 a - 6 a t^3} {\\paren {1 + t^3}^2}\n      | c = simplifying\n}}\n{{end-eqn}}\nThus $x$ is stationary when:\n{{begin-eqn}}\n{{eqn | l = 3 a - 6 a t^3\n      | r = 0\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = t\n      | r = \\paren {\\dfrac 1 2}^{1/3}\n      | c = \n}}\n{{end-eqn}}\nFrom Behaviour of Parametric Equations for Folium of Descartes according to Parameter, it is clear from the geometry that $x$ is a local maximum for this value of $t$.\nThen we have:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = \\dfrac {3 a \\times \\paren {1/2}^{1/3} } {1 + \\paren {\\paren {1/2}^{1/3} }^3}\n      | c = \n}}\n{{eqn | r = \\dfrac {3 a \\times \\paren {1/2}^{1/3} } {1 + 1/2}\n      | c = \n}}\n{{eqn | r = 2 a \\times \\paren {\\dfrac 1 2}^{1/3}\n      | c = \n}}\n{{eqn | r = \\paren {\\dfrac {2^3} 2}^{1/3} a\n      | c = \n}}\n{{eqn | r = 2^{2/3} a\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = y\n      | r = \\dfrac {3 a \\times \\paren {\\paren {1/2}^{1/3} }^2} {1 + \\paren {\\paren {1/2}^{1/3} }^3}\n      | c = \n}}\n{{eqn | r = \\dfrac {3 a \\times \\paren {1/2}^{2/3} } {1 + 1/2}\n      | c = \n}}\n{{eqn | r = 2 a \\times \\paren {\\dfrac 1 2}^{2/3}\n      | c = \n}}\n{{eqn | r = \\paren {\\dfrac {2^3} {2^2} }^{1/3} a\n      | c = \n}}\n{{eqn | r = 2^{1/3} a\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\nCategory:Folium of Descartes\n\\end{proof}\n\n"}}, "10785": {"score": 0.8573976755142212, "content": {"text": "\\section{Maximum Rate of Change of Y Coordinate of Cycloid}\nTags: Cycloids\n\n\\begin{theorem}\nLet a circle $C$ of radius $a$ roll without slipping along the x-axis of a cartesian plane at a constant speed such that the center moves with a velocity $\\mathbf v_0$ in the direction of increasing $x$.\nConsider a point $P$ on the circumference of this circle.\nLet $\\tuple {x, y}$ be the coordinates of $P$ as it travels over the plane.\nThe maximum rate of change of $y$ is $\\mathbf v_0$, which happens when $\\theta = \\dfrac \\pi 2 + 2 n \\pi$ where $n \\in \\Z$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Rate of Change of Cartesian Coordinates of Cycloid, the rate of change of $y$ is given by:\n:$\\dfrac {\\d y} {\\d t} = \\mathbf v_0 \\sin \\theta$.\nThis is a maximum when $\\sin \\theta$ is a maximum.\nThat happens when $\\sin \\theta = 1$.\nThat happens when $\\theta = \\dfrac \\pi 2 + 2 n \\pi$ where $n \\in \\Z$.\nWhen $\\sin \\theta = 1$ we have:\n:$\\dfrac {\\d y} {\\d t} = \\mathbf v_0$\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "7513": {"score": 0.8610955476760864, "content": {"text": "\\begin{definition}[Definition:Quadratic Function]\nA '''quadratic function''' is an expression of the form:\n:$\\map Q x := a_0 + a_1 x + a_2 x^2$\nwhere $a_0, a_1, a_2$ are constants.\nThe domain of $x$ is usually defined as the real numbers $\\R$ or the \ncomplex numbers $\\C$.\n\\end{definition}"}}}}, "TheoremQA_elainewan/math_algebra_7_2.json": {"gold": {"2724": 1, "2723": 1}, "retrieved": {"2721": {"score": 0.848992645740509, "content": {"text": "\\begin{definition}[Definition:Eigenvalue/Real Square Matrix]\nLet $\\mathbf A$ be a square matrix of order $n$ over $\\R$. \nLet $\\lambda \\in \\R$. \n$\\lambda$ is an '''eigenvalue''' of $A$ if there exists a non-zero vector $\\mathbf v \\in \\R^n$ such that: \n:$\\mathbf A \\mathbf v = \\lambda \\mathbf v$\n\\end{definition}"}}, "9505": {"score": 0.8508864641189575, "content": {"text": "\\begin{definition}[Definition:Trace (Linear Algebra)/Linear Operator]\nLet $V$ be a vector space.\nLet $A: V \\to V$ be a linear operator of $V$.\nThe '''trace''' of $A$ is the trace of the matrix of $A$ with respect to some basis.\nCategory:Definitions/Linear Algebra\n\\end{definition}"}}, "14713": {"score": 0.8693427443504333, "content": {"text": "\\section{Eigenvalue of Matrix Powers}\nTags: Matrix Algebra\n\n\\begin{theorem}\nLet $A$ be a square matrix.\nLet $\\lambda$ be an eigenvalue of $A$ and $\\mathbf v$ be the corresponding eigenvector.\nThen:\n:$A^n \\mathbf v = \\lambda^n \\mathbf v$\nholds for each positive integer $n$.\n\\end{theorem}\n\n\\begin{proof}\nProof by induction:\nFor all $n \\in \\N_{> 0}$, let $\\map P n$ be the proposition:\n:$A^n \\mathbf v = \\lambda^n \\mathbf v$\n\\end{proof}\n\n"}}, "11160": {"score": 0.8576740622520447, "content": {"text": "\\section{Linear Operator on the Plane}\nTags: Linear Operators, Linear Algebra, Analytic Geometry\n\n\\begin{theorem}\nLet $\\phi$ be a linear operator on the real vector space of two dimensions $\\R^2$.\nThen $\\phi$ is completely determined by an ordered tuple of $4$ real numbers.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\phi$ be a linear operator on $\\R^2$.\nLet $\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22} \\in \\R$ be the real numbers which satisfy the equations:\n{{begin-eqn}}\n{{eqn | l = \\map \\phi {e_1}\n      | r = \\alpha_{11} e_1 + \\alpha_{21} e_2\n      | c = \n}}\n{{eqn | l = \\map \\phi {e_2}\n      | r = \\alpha_{12} e_1 + \\alpha_{22} e_2\n      | c = \n}}\n{{end-eqn}}\nwhere $\\tuple {e_1, e_2}$ is the standard ordered basis of $\\R^2$.\nThen, by linearity:\n{{begin-eqn}}\n{{eqn | l = \\map \\phi {\\lambda_1, \\lambda_2}\n      | r = \\map \\phi {\\lambda_1 e_1 + \\lambda_2 e_2}\n      | c = \n}}\n{{eqn | r = \\lambda_1 \\map \\phi {e_1} + \\lambda_2 \\map \\phi {e_2}\n      | c = \n}}\n{{eqn | r = \\paren {\\lambda_1 \\alpha_{11} + \\lambda_2 \\alpha_{12} } e_1 + \\paren {\\lambda_1 \\alpha_{21} + \\lambda_2 \\alpha_{22} } e_2\n      | c = \n}}\n{{eqn | r = \\paren {\\lambda_1 \\alpha_{11} + \\lambda_2 \\alpha_{12}, \\lambda_1 \\alpha_{21} + \\lambda_2 \\alpha_{22} }\n      | c = \n}}\n{{end-eqn}}\nConversely, if $\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22} \\in \\R$ are ''any'' real numbers, then we can define the mapping $\\phi$ as:\n:$\\map \\phi {\\lambda_1, \\lambda_2} = \\tuple {\\lambda_1 \\alpha_{11} + \\lambda_2 \\alpha_{12}, \\lambda_1 \\alpha_{21} + \\lambda_2 \\alpha_{22} }$\nwhich is easily verified as being a linear operator on $\\R^2$:\n{{begin-eqn}}\n{{eqn | l = b \\cdot \\map \\phi {\\lambda_1, \\lambda_2} + c \\cdot \\map \\phi {\\lambda_3, \\lambda_4}\n      | r = b \\tuple {\\lambda_1 \\alpha_{11} + \\lambda_2 \\alpha_{12}, \\lambda_1 \\alpha_{21} + \\lambda_2 \\alpha_{22} } + c \\tuple {\\lambda_3 \\alpha_{11} + \\lambda_4 \\alpha_{12}, \\lambda_3 \\alpha_{21} + \\lambda_4 \\alpha_{22} }\n      | c = \n}}\n{{eqn | r = \\tuple {b \\lambda_1 \\alpha_{11} + b \\lambda_2 \\alpha_{12}, b \\lambda_1 \\alpha_{21} + b \\lambda_2 \\alpha_{22} } + \\tuple {c \\lambda_3 \\alpha_{11} + c \\lambda_4 \\alpha_{12}, c \\lambda_3 \\alpha_{21} + c \\lambda_4 \\alpha_{22} }\n      | c = \n}}\n{{eqn | r = \\tuple {b \\lambda_1 \\alpha_{11} + b \\lambda_2 \\alpha_{12} + c \\lambda_3 \\alpha_{11} + c \\lambda_4 \\alpha_{12}, b \\lambda_1 \\alpha_{21} + b \\lambda_2 \\alpha_{22} + c \\lambda_3 \\alpha_{21} + c \\lambda_4 \\alpha_{22} }\n      | c = \n}}\n{{eqn | r = \\tuple {\\paren {b \\lambda_1 + c \\lambda_3} \\alpha_{11} + \\tuple {b \\lambda_2 + c \\lambda_4} \\alpha_{12}, \\paren {b \\lambda_1 + c \\lambda_3} \\alpha_{21} + \\paren {c \\lambda_2 + c \\lambda_4} \\alpha_{22} }\n      | c = \n}}\n{{eqn | r = \\tuple {b \\lambda_1 + c \\lambda_3, b \\lambda_2 + c \\lambda_4}\n      | c = \n}}\n{{end-eqn}}\nThus, by Condition for Linear Transformation, $\\phi$ is a linear operator on $\\R^2$.\nThus each linear operator on $\\R^2$ is completely determined by the ordered tuple:\n:$\\tuple {\\alpha_{11}, \\alpha_{12}, \\alpha_{21}, \\alpha_{22} }$\nof real numbers.\n{{Qed}}\n\\end{proof}\n\n"}}, "14717": {"score": 0.8568642735481262, "content": {"text": "\\section{Eigenvalues of Normal Operator have Orthogonal Eigenspaces}\nTags: Linear Transformations on Hilbert Spaces\n\n\\begin{theorem}\nLet $\\HH$ be a Hilbert space.\nLet $\\mathbf T: \\HH \\to \\HH$ be a normal operator.\nLet $\\lambda_1, \\lambda_2$ be distinct eigenvalues of $\\mathbf T$.\nThen:\n:$\\map \\ker {\\mathbf T - \\lambda_1} \\perp \\map \\ker {\\mathbf T - \\lambda_2}$\nwhere:\n:$\\ker$ denotes kernel\n:$\\perp$ denotes orthogonality.\n\\end{theorem}\n\n\\begin{proof}\n{{improve}}\nRequisite knowledge: $\\mathbf T^*$ is the adjoint of $\\mathbf T$ and is defined by the fact that for any $\\mathbf u, \\mathbf w \\in \\HH$, we have\n:$\\innerprod {\\mathbf {T u} } {\\mathbf w} = \\innerprod {\\mathbf u} {\\mathbf T^* \\mathbf w}$\nIt is important to note the existence and uniqueness of adjoint operators.\n{{explain|Link to the required proofs and/or definitions as given above.}}\n'''Claim''': We know that for $\\mathbf v \\in \\HH$:\n:$\\mathbf {T v} = \\lambda \\mathbf v \\iff \\mathbf T^* \\mathbf v = \\overline \\lambda \\mathbf v$\nThis is true because for all normal operators, by definition:\n:$\\mathbf T^* \\mathbf T = \\mathbf T {\\mathbf T^*}$\nand so:\n{{begin-eqn}}\n{{eqn | l = \\norm {\\mathbf {T v} }^2\n      | r = \\innerprod {\\mathbf{T v} } {\\mathbf{T v} }\n      | c = {{Defof|Inner Product Norm}}\n}}\n{{eqn | r = \\innerprod {\\mathbf T^* \\mathbf {T v} } {\\mathbf v}\n}}\n{{eqn | r = \\innerprod {\\mathbf T \\mathbf T^* \\mathbf v} {\\mathbf v}\n      | c = {{Defof|Normal Operator}}\n}}\n{{eqn | r = \\innerprod {\\mathbf T^* \\mathbf v} {\\mathbf T^* \\mathbf v}\n}}\n{{eqn | r = \\norm {\\mathbf T^* \\mathbf v}^2\n      | c = {{Defof|Inner Product Norm}}\n}}\n{{end-eqn}}\nSince $\\mathbf T$ is normal, $\\mathbf T - \\lambda \\mathbf I$ is also normal.\n{{explain|Link to a page demonstrating the above}}\nThus:\n{{begin-eqn}}\n{{eqn | l = \\mathbf {T v}\n      | r = \\lambda \\mathbf v\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\bszero\n      | r = \\norm {\\paren {\\mathbf T - \\lambda \\mathbf I} \\mathbf v}\n}}\n{{eqn | r = \\norm {\\paren {\\mathbf T - \\lambda \\mathbf I}^* \\mathbf v}\n}}\n{{eqn | r = \\norm {\\mathbf T^* \\mathbf v - \\overline \\lambda \\mathbf v}\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\mathbf T^* \\mathbf v\n      | r = \\overline \\lambda \\mathbf v\n}}\n{{end-eqn}}\nLet $\\mathbf v_1$ and $\\mathbf v_2$ be non-zero eigenvectors of $\\mathbf T$ with corresponding eigenvalues $\\lambda_1$ and $\\lambda_2$, respectively.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\lambda_1 \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n      | r = \\innerprod {\\lambda_1 \\mathbf v_1} {\\mathbf v_2}\n}}\n{{eqn | r = \\innerprod {\\mathbf T \\mathbf v_1} {\\mathbf v_2}\n}}\n{{eqn | r = \\innerprod {\\mathbf v_1} {\\mathbf T^* \\mathbf v_2}\n}}\n{{eqn | r = \\innerprod {\\mathbf v_1} {\\overline{\\lambda_2} \\mathbf v_2}\n}}\n{{eqn | r = \\lambda_2 \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\bszero\n      | r = \\paren {\\lambda_1 - \\lambda_2} \\innerprod {\\mathbf v_1} {\\mathbf v_2}\n}}\n{{end-eqn}}\nSince $\\lambda_1 \\ne \\lambda_2$, this is only possible if $\\innerprod {\\mathbf v_1} {\\mathbf v_2} = 0$, which means the eigenvectors of our normal operator are orthogonal.\n{{qed}}\n\\end{proof}\n\n"}}, "2719": {"score": 0.9010978937149048, "content": {"text": "\\begin{definition}[Definition:Eigenspace]\nLet $K$ be a field.\nLet $V$ be a vector space over $K$. \nLet $A : V \\to V$ be a linear operator.\nLet $I : V \\to V$ be the identity mapping on $V$. \nLet $\\lambda \\in K$ be an eigenvalue of $A$. \nLet $\\map \\ker {A - \\lambda I}$ be the kernel of $A - \\lambda I$.\nWe say that $\\map \\ker {A - \\lambda I}$ is the '''eigenspace''' corresponding to the eigenvalue $\\lambda$.\n\\end{definition}"}}, "2724": {"score": 0.8697099089622498, "content": {"text": "\\begin{definition}[Definition:Eigenvector/Real Square Matrix]\nLet $\\mathbf A$ be a square matrix of order $n$ over $\\R$. \nLet $\\lambda \\in \\R$ be an eigenvalue of $\\mathbf A$. \nA non-zero vector $\\mathbf v \\in \\R^n$ is an '''eigenvector corresponding to $\\lambda$''' {{iff}}: \n:$\\mathbf A \\mathbf v = \\lambda \\mathbf v$\n\\end{definition}"}}, "2723": {"score": 0.9097055196762085, "content": {"text": "\\begin{definition}[Definition:Eigenvector/Linear Operator]\nLet $K$ be a field.\nLet $V$ be a vector space over $K$. \nLet $A : V \\to V$ be a linear operator.\nLet $\\lambda \\in K$ be an eigenvalue of $A$.\nA non-zero vector $v \\in V$ is an '''eigenvector corresponding to $\\lambda$''' {{iff}}:\n:$v \\in \\map \\ker {A - \\lambda I}$\nwhere: \n:$I : V \\to V$ is the identity mapping on $V$\n:$\\map \\ker {A - \\lambda I}$ denotes the kernel of $A - \\lambda I$.\nThat is, {{iff}}: \n:$A v = \\lambda v$\n\\end{definition}"}}, "21776": {"score": 0.8705072402954102, "content": {"text": "\\section{Stabilizer of Subspace stabilizes Orthogonal Complement}\nTags: Hilbert Spaces\n\n\\begin{theorem}\nLet $H$ be a finite-dimensional real or complex Hilbert space (that is, inner product space).\nLet $t: H \\to H$ be a normal operator on $H$.\nLet $t$ stabilize a subspace $V$.\nThen $t$ also stabilizes its orthogonal complement $V^\\perp$.\n\\end{theorem}\n\n\\begin{proof}\nLet $p: H \\to V$ be the orthogonal projection of $H$ onto $V$.\nThen the orthogonal projection of $H$ onto $V^\\perp$ is $\\mathbf 1 - p$, where $\\mathbf 1$ is the identity map of $H$.\nThe fact that $t$ stabilizes $V$ can be expressed as:\n:$\\paren {\\mathbf 1 - p} t p = 0$\nor:\n:$p t p = t p$\nThe goal is to show that:\n:$p t \\paren {\\mathbf 1 - p} = 0$\nWe have that $\\tuple {a, b} \\mapsto \\map \\tr {a b^*}$ is an inner product on the space of endomorphisms of $H$.\nHere, $b^*$ denotes the adjoint operator of $b$.\nThus it will suffice to show that $\\map \\tr {x x^*} = 0$ for $x = p t \\paren {\\mathbf 1 - p}$.\nThis follows from a direct computation, using properties of the trace and orthogonal projections:\n{{begin-eqn}}\n{{eqn | l = x x^*\n      | r = p t \\paren {\\mathbf 1 - p}^2 t^* p\n      | c = \n}}\n{{eqn | r = p t \\paren {\\mathbf 1 - p} t^* p\n      | c = \n}}\n{{eqn | r = p t t^* p - p t p t^* p\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\map \\tr {x x^*}\n      | r = \\map \\tr {p t t^* p} - \\map \\tr {p t p t^* p}\n      | c = \n}}\n{{eqn | r = \\map \\tr {p^2 t t^*} - \\map \\tr {p^2 t p t^*}\n      | c = \n}}\n{{eqn | r = \\map \\tr {p t t^*} - \\map \\tr {\\paren {p t p} t^*}\n      | c = \n}}\n{{eqn | r = \\map \\tr {p t t^*} - \\map \\tr {t p t^*}\n      | c = \n}}\n{{eqn | r = \\map \\tr {p t t^*} - \\map \\tr {p t^* t}\n      | c = \n}}\n{{eqn | r = \\map \\tr {p \\paren {t t^* - t^*t} }\n      | c = \n}}\n{{eqn | r = \\map \\tr 0\n      | c = \n}}\n{{eqn | r = 0\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Hilbert Spaces\n\\end{proof}\n\n"}}, "2720": {"score": 0.8908191323280334, "content": {"text": "\\begin{definition}[Definition:Eigenvalue/Linear Operator]\nLet $K$ be a field.\nLet $V$ be a vector space over $K$. \nLet $A : V \\to V$ be a linear operator.\n$\\lambda \\in K$ is an '''eigenvalue''' of $A$ {{iff}}:\n:$\\map \\ker {A - \\lambda I} \\ne \\set {0_V}$\nwhere: \n:$0_V$ is the zero vector of $V$\n:$I : V \\to V$ is the identity mapping on $V$\n:$\\map \\ker {A - \\lambda I}$ denotes the kernel of $A - \\lambda I$.\nThat is, $\\lambda \\in K$ is an '''eigenvalue of $A$''' {{iff}} the kernel of $A$ is non-trivial.\n{{explain|Link to \"non-trivial\"}}\n\\end{definition}"}}}}, "TheoremQA_xinyi/dag_3.json": {"gold": {"23699": 1}, "retrieved": {"4894": {"score": 0.8604989051818848, "content": {"text": "\\begin{definition}[Definition:Labeled Tree]\nA '''labeled tree''' is a finite rooted tree in which each node has attached to it a '''label'''.\nThese '''labels''' may be any symbol or other object that one can think of, but usually consist of one letter or symbol.\n{{expand|A picture like [http://mathworld.wolfram.com/images/eps-gif/PrueferCode_700.gif this], exemplifying the below}}\nThe parent function is usually depicted by placing the ancestor above the child and connecting them with a line.\nThe various children of a node may also be considered to be in a particular order.\nThis is usually depicted by placing them in a left-to-right order.\nBecause of the intuitive clarity of these relations, '''labeled trees''' may be considered collations as basic as strings are.\nHowever, because of the space limitations that most printed media have to take into account, it is usual that one quickly resorts to representing trees by strings in one way or another.\nCategory:Definitions/Formal Systems\n\\end{definition}"}}, "16790": {"score": 0.860560953617096, "content": {"text": "\\section{Cayley's Formula}\nTags: Named Theorems, Graph Theory, Combinatorics\n\n\\begin{theorem}\nThe number of distinct labeled trees with $n$ nodes is $n^{n - 2}$.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from Bijection between Pr\u00fcfer Sequences and Labeled Trees.\nThis shows that there is a bijection between the set of labeled trees with $n$ nodes and the set of all Pr\u00fcfer sequences of the form:\n:$\\tuple {\\mathbf a_1, \\mathbf a_2, \\ldots, \\mathbf a_{n - 2} }$\nwhere each of the $\\mathbf a_i$'s is one of the integers $1, 2, \\ldots, n$, allowing for repetition.\nSince there are exactly $n$ possible values for each integer $\\mathbf a_i$, the total number of such sequences is $\\ds \\prod_{i \\mathop = 1}^{n - 2} n$.\nThe result follows from Equivalence of Mappings between Sets of Same Cardinality.\n{{qed}}\n{{Namedfor|Arthur Cayley|cat = Cayley}}\n\\end{proof}\n\n"}}, "8121": {"score": 0.8679787516593933, "content": {"text": "\\begin{definition}[Definition:Rooted Tree/Sibling]\nLet $T$ be a rooted tree with root $r_T$.\nTwo children of the same node of $T$ are called '''siblings'''.\nThat is, '''siblings''' are nodes which both have the same parent.\nCategory:Definitions/Rooted Trees\n\\end{definition}"}}, "8120": {"score": 0.8634997606277466, "content": {"text": "\\begin{definition}[Definition:Rooted Tree/Root Node]\nLet $T$ be a rooted tree.\nThe '''root node''' of $T$ is the node of $T$ which is distinguished from the others by being the ancestor node of every node of $T$.\n\\end{definition}"}}, "8110": {"score": 0.862962007522583, "content": {"text": "\\begin{definition}[Definition:Rooted Tree/Ancestor Node/Proper]\nLet $T$ be a rooted tree with root $r_T$.\nLet $t$ be a node of $T$.\nA '''proper ancestor node''' of $t$ is an ancestor node of $t$ that is not $t$ itself.\n\\end{definition}"}}, "8116": {"score": 0.8808940052986145, "content": {"text": "\\begin{definition}[Definition:Rooted Tree/Child Node/Grandchild Node]\nLet $T$ be a rooted tree with root $r_T$.\nLet $t$ be a node of $T$.\nA child of a child node of a node $t$ can be referred to as a '''grandchild node''' of $t$.\nIn terms of the parent mapping $\\pi$ of $T$, a '''grandchild node''' of $t$ is a node $s$ such that:\n:$\\pi \\left({\\pi \\left({s}\\right)}\\right) = t$\nCategory:Definitions/Rooted Trees\n\\end{definition}"}}, "8115": {"score": 0.8685178160667419, "content": {"text": "\\begin{definition}[Definition:Rooted Tree/Child Node]\nLet $T$ be a rooted tree with root $r_T$.\nLet $t$ be a node of $T$.\nThe '''child nodes''' of $t$ are the elements of the set:\n:$\\left\\{{s \\in T: \\pi \\left({s}\\right) = t}\\right\\}$\nwhere $\\pi \\left({s}\\right)$ denotes the parent mapping of $s$.\nThat is, the '''children''' of $t$ are all the nodes of $T$ of which $t$ is the parent.\n\\end{definition}"}}, "8118": {"score": 0.8875792026519775, "content": {"text": "\\begin{definition}[Definition:Rooted Tree/Parent Node]\nLet $T$ be a rooted tree whose root is $r_T$.\nLet $t$ be a node of $T$.\nFrom Path in Tree is Unique, there is only one path from $t$ to $r_T$.\nLet $\\pi: T \\setminus \\left\\{{r_T}\\right\\} \\to T$ be the mapping defined by:\n:$\\pi \\left({t}\\right) := \\text{the node adjacent to $t$ on the path to $r_T$}$\nThen $\\pi \\left({t}\\right)$ is known as the '''parent node''' of $t$.\nThe mapping $\\pi$ is called the '''parent mapping'''.\n\\end{definition}"}}, "17301": {"score": 0.8689706921577454, "content": {"text": "\\section{Bijection between Pr\u00fcfer Sequences and Labeled Trees}\nTags: Tree Theory, Trees, Graph Theory, Combinatorics\n\n\\begin{theorem}\nThere is a one-to-one correspondence between Pr\u00fcfer sequences and labeled trees.\nThat is, every labeled tree has a unique Pr\u00fcfer sequence that defines it, and every Pr\u00fcfer sequence defines just one labeled tree.\n\\end{theorem}\n\n\\begin{proof}\nLet $T$ be the set of all labeled trees of order $n$.\nLet $P$ be the set of all Pr\u00fcfer sequence of length $n-2$.\nLet $\\phi: T \\to P$ be the mapping that maps each tree to its Pr\u00fcfer sequence.\n* From Pr\u00fcfer Sequence from Labeled Tree, $\\phi$ is clearly well-defined, as every element of $T$ maps uniquely to an element of $P$.\n* However, from Labeled Tree from Pr\u00fcfer Sequence, $\\phi^{-1}: P \\to T$ is also clearly well-defined, as every element of $P$ maps to a unique element of $T$.\nHence the result.\n{{questionable|How is it immediate that the two constructions are mutually inverse?}}\n{{qed}}\nCategory:Tree Theory\nCategory:Combinatorics\n\\end{proof}\n\n"}}, "8109": {"score": 0.8738959431648254, "content": {"text": "\\begin{definition}[Definition:Rooted Tree/Ancestor Node]\nLet $T$ be a rooted tree with root $r_T$.\nLet $t$ be a node of $T$.\nAn '''ancestor node''' of $t$ is a node in the path from $t$ to $r_T$.\nThis path is indeed unique, by Path in Tree is Unique.\n\\end{definition}"}}}}, "TheoremQA_elainewan/math_calculus_3_8.json": {"gold": {"15210": 1}, "retrieved": {"15142": {"score": 0.8437931537628174, "content": {"text": "\\section{Derivative of Tangent Function}\nTags: Differential Calculus, Tangent Function, Derivative of Tangent Function, Derivatives of Trigonometric Functions\n\n\\begin{theorem}\n:$\\map {\\dfrac \\d {\\d x} } {\\tan x} = \\sec^2 x = \\dfrac 1 {\\cos^2 x}$\nwhen $\\cos x \\ne 0$.\n\\end{theorem}\n\n\\begin{proof}\nFrom the definition of the tangent function:\n: $\\tan x = \\dfrac {\\sin x} {\\cos x}$\nFrom Derivative of Sine Function:\n: $D_x \\left({\\sin x}\\right) = \\cos x$\nFrom Derivative of Cosine Function:\n: $D_x \\left({\\cos x}\\right) = -\\sin x$\nThen:\n{{begin-eqn}}\n{{eqn | l=D_x \\left({\\tan x}\\right)\n      | r=\\frac {\\cos x \\cos x - \\sin x \\left({-\\sin x}\\right)} {\\cos^2 x}\n      | c=Quotient Rule for Derivatives\n}}\n{{eqn | r=\\frac {\\cos^2 x + \\sin^2 x} {\\cos^2 x}\n      | c=\n}}\n{{eqn | r=\\frac 1 {\\cos^2 x}\n      | c=Sum of Squares of Sine and Cosine\n}}\n{{end-eqn}}\nThis is valid only when $\\cos x \\ne 0$.\nThe result follows from the Secant is Reciprocal of Cosine.\n{{qed}}\n\\end{proof}\n\n"}}, "10015": {"score": 0.8456838130950928, "content": {"text": "\\begin{definition}[Definition:Velocity]\nThe '''velocity''' $\\mathbf v$ of a body $M$ is defined as the first derivative of the displacement $\\mathbf s$ of $M$ from a given point of reference {{WRT|Differentiation}} time $t$:\n:$\\mathbf v = \\dfrac {\\d \\mathbf s} {\\d t}$\nColloquially, it is described as the ''rate of change of position''.\nIt is important to note that as displacement is a vector quantity, then it follows by definition of derivative of a vector that so is velocity.\n\\end{definition}"}}, "20137": {"score": 0.8500095009803772, "content": {"text": "\\section{Rate of Change of Cartesian Coordinates of Cycloid}\nTags: Cycloids\n\n\\begin{theorem}\nLet a circle $C$ of radius $a$ roll without slipping along the x-axis of a cartesian plane at a constant speed such that the center moves with a velocity $\\mathbf v_0$ in the direction of increasing $x$.\nConsider a point $P$ on the circumference of this circle.\nLet $\\tuple {x, y}$ be the coordinates of $P$ as it travels over the plane.\nThen the rate of change of $x$ and $y$ can be expresssed as:\n{{begin-eqn}}\n{{eqn | l = \\frac {\\d x} {\\d t}\n      | r = \\mathbf v_0 \\paren {1 - \\cos \\theta}\n      | c = \n}}\n{{eqn | l = \\frac {\\d y} {\\d t}\n      | r = \\mathbf v_0 \\sin \\theta\n      | c = \n}}\n{{end-eqn}}\nwhere $\\theta$ is the angle turned by $C$ after time $t$.\n\\end{theorem}\n\n\\begin{proof}\nLet the center of $C$ be $O$.\n{{WLOG}}, let $P$ be at the origin at time $t = t_0$.\nBy definition, $P$ traces out a cycloid.\nFrom Equation of Cycloid, $P = \\tuple {x, y}$ is described by:\n:$(1): \\quad \\begin{cases} x & = a \\paren {\\theta - \\sin \\theta} \\\\\ny & = a \\paren {1 - \\cos \\theta} \\end{cases}$\nLet $\\tuple {x_c, y_c}$ be the coordinates of $O$ at time $t$.\nWe have that:\n:$y_c$ is constant: $y_c = a$\nFrom Body under Constant Acceleration: Velocity after Time\n:$x_c = \\mathbf v_0 t$\nas the acceleration of $O$ is zero.\nBut $x_c$ is equal to the length of the arc of $C$ that has rolled along the $x$-axis.\nBy Arc Length of Sector:\n:$x_c = a \\theta$\nand so:\n{{begin-eqn}}\n{{eqn | n = 2\n      | l = \\theta\n      | r = \\frac {\\mathbf v_0 t} a\n      | c = \n}}\n{{eqn | n = 3\n      | ll= \\leadsto\n      | l = \\frac {\\d \\theta} {\\d t}\n      | r = \\frac {\\mathbf v_0} a\n      | c = \n}}\n{{end-eqn}}\nThus:\n{{begin-eqn}}\n{{eqn | l = x\n      | r = a \\paren {\\frac {\\mathbf v_0 t} a - \\sin \\theta}\n      | c = substituting for $\\theta$ from $(2)$\n}}\n{{eqn | l = x\n      | r = \\mathbf v_0 t - a \\sin \\theta\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac {\\d x} {\\d t}\n      | r = \\mathbf v_0 - a \\cos \\theta \\frac {\\d \\theta} {\\d t}\n      | c = Chain Rule for Derivatives\n}}\n{{eqn | r = \\mathbf v_0 - a \\cos \\theta \\frac {\\mathbf v_0} a\n      | c = from $(3)$\n}}\n{{eqn | r = \\mathbf v_0 \\paren {1 - \\cos \\theta}\n      | c = \n}}\n{{end-eqn}}\nand:\n{{begin-eqn}}\n{{eqn | l = y\n      | r = a \\paren {1 - \\cos \\theta}\n      | c = \n}}\n{{eqn | ll= \\leadsto\n      | l = \\frac {\\d y} {\\d t}\n      | r = a \\sin \\theta \\frac {\\d \\theta} {\\d t}\n      | c = Chain Rule for Derivatives\n}}\n{{eqn | r = a \\sin \\theta \\frac {\\mathbf v_0} a\n      | c = from $(3)$\n}}\n{{eqn | r = \\mathbf v_0 \\sin \\theta\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "7656": {"score": 0.8499317169189453, "content": {"text": "\\begin{definition}[Definition:Rate]\nThe '''rate''' of a physical process is its (first) derivative with respect to time.\nLoosely speaking, it means '''how fast something progresses''', with a wider scope than change in physical displacement.\n\\end{definition}"}}, "21494": {"score": 0.8462331295013428, "content": {"text": "\\section{Slope of Curve at Point equals Derivative}\nTags: Differential Calculus, Analytic Geometry\n\n\\begin{theorem}\nLet $\\CC$ be a curve embedded in the Cartesian plane described using the equation:\n:$y = \\map f x$\nwhere $f$ is a real function.\nLet there exist a unique tangent $\\TT$ to $\\CC$ at a point $P = \\tuple {x_0, y_0}$ on $\\CC$.\nThen the slope of $\\CC$ at $P$ is equal to the derivative of $f$ at $P$.\n\\end{theorem}\n\n\\begin{proof}\nWe have been given that there exists a unique tangent $\\TT$ to $\\CC$ at $P$.\nBy definition of tangent, $\\TT$ has a slope $M$ given by:\n:$m = \\ds \\lim_{h \\mathop \\to 0} \\frac {\\map f {x_0 + h} - \\map f {x_0} } h$\nThis is the definition of the derivative of $f$ at $P$.\n{{qed}}\n\\end{proof}\n\n"}}, "10785": {"score": 0.8616145253181458, "content": {"text": "\\section{Maximum Rate of Change of Y Coordinate of Cycloid}\nTags: Cycloids\n\n\\begin{theorem}\nLet a circle $C$ of radius $a$ roll without slipping along the x-axis of a cartesian plane at a constant speed such that the center moves with a velocity $\\mathbf v_0$ in the direction of increasing $x$.\nConsider a point $P$ on the circumference of this circle.\nLet $\\tuple {x, y}$ be the coordinates of $P$ as it travels over the plane.\nThe maximum rate of change of $y$ is $\\mathbf v_0$, which happens when $\\theta = \\dfrac \\pi 2 + 2 n \\pi$ where $n \\in \\Z$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Rate of Change of Cartesian Coordinates of Cycloid, the rate of change of $y$ is given by:\n:$\\dfrac {\\d y} {\\d t} = \\mathbf v_0 \\sin \\theta$.\nThis is a maximum when $\\sin \\theta$ is a maximum.\nThat happens when $\\sin \\theta = 1$.\nThat happens when $\\theta = \\dfrac \\pi 2 + 2 n \\pi$ where $n \\in \\Z$.\nWhen $\\sin \\theta = 1$ we have:\n:$\\dfrac {\\d y} {\\d t} = \\mathbf v_0$\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "10783": {"score": 0.8576340675354004, "content": {"text": "\\section{Maximum Rate of Change of X Coordinate of Cycloid}\nTags: Cycloids\n\n\\begin{theorem}\nLet a circle $C$ of radius $a$ roll without slipping along the x-axis of a cartesian plane at a constant speed such that the center moves with a velocity $\\mathbf v_0$ in the direction of increasing $x$.\nConsider a point $P$ on the circumference of this circle.\nLet $\\tuple {x, y}$ be the coordinates of $P$ as it travels over the plane.\nThe maximum rate of change of $x$ is $2 \\mathbf v_0$, which happens when $P$ is at the top of the circle $C$.\n\\end{theorem}\n\n\\begin{proof}\nFrom Rate of Change of Cartesian Coordinates of Cycloid, the rate of change of $x$ is given by:\n:$\\dfrac {\\d x} {\\d t} = \\mathbf v_0 \\paren {1 - \\cos \\theta}$\nThis is a maximum when $1 - \\cos \\theta$ is a maximum.\nThat happens when $\\cos \\theta$ is at a minimum.\nThat happens when $\\cos \\theta = -1$.\nThat happens when $\\theta = \\pi, 3 \\pi, \\ldots$\nThat is, when $\\theta = \\paren {2 n + 1} \\pi$ where $n \\in \\Z$.\nThat is, when $P$ is at the top of the circle $C$.\nWhen $\\cos \\theta = -1$ we have:\n{{begin-eqn}}\n{{eqn | l = \\frac {\\d x} {\\d t}\n      | r = \\mathbf v_0 \\paren {1 - \\paren {-1} }\n      | c = \n}}\n{{eqn | r = 2 \\mathbf v_0\n      | c = \n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "23330": {"score": 0.8820136189460754, "content": {"text": "\\section{Velocity Vector in Polar Coordinates}\nTags: Polar Coordinates\n\n\\begin{theorem}\nConsider a particle $p$ moving in the plane.\nLet the position of $p$ at time $t$ be given in polar coordinates as $\\left\\langle{r, \\theta}\\right\\rangle$.\nThen the velocity $\\mathbf v$ of $p$ can be expressed as:\n:$\\mathbf v = r \\dfrac {\\d \\theta} {\\d t} \\mathbf u_\\theta + \\dfrac {\\d r} {\\d t} \\mathbf u_r$\nwhere:\n:$\\mathbf u_r$ is the unit vector in the direction of the radial coordinate of $p$\n:$\\mathbf u_\\theta$ is the unit vector in the direction of the angular coordinate of $p$\n\\end{theorem}\n\n\\begin{proof}\nLet the radius vector $\\mathbf r$ from the origin to $p$ be expressed as:\n:$(1): \\quad \\mathbf r = r \\mathbf u_r$\n:600px\nFrom Derivatives of Unit Vectors in Polar Coordinates:\n{{begin-eqn}}\n{{eqn | n = 2\n      | l = \\dfrac {\\d \\mathbf u_r} {\\d \\theta}\n      | r = \\mathbf u_\\theta\n      | c = \n}}\n{{eqn | n = 3\n      | l = \\dfrac {\\d \\mathbf u_\\theta} {\\d \\theta}\n      | r = -\\mathbf u_r\n      | c = \n}}\n{{end-eqn}}\nThe velocity of $p$ is by definition the rate of change in its position:\n{{begin-eqn}}\n{{eqn | l = \\mathbf v\n      | r = \\dfrac {\\d \\mathbf r} {\\d t}\n      | c = \n}}\n{{eqn | r = r \\dfrac {\\d \\mathbf u_r} {\\d t} + \\mathbf u_r \\dfrac {\\d r} {\\d t}\n      | c = from $(1)$ and Product Rule for Derivatives\n}}\n{{eqn | r = r \\dfrac {\\d \\mathbf u_r} {\\d \\theta} \\dfrac {\\d \\theta} {\\d t} + \\mathbf u_r \\dfrac {\\d r} {\\d t}\n      | c = Chain Rule for Derivatives\n}}\n{{eqn | r = r \\dfrac {\\d \\theta} {\\d t} \\mathbf u_\\theta + \\dfrac {\\d r} {\\d t} \\mathbf u_r\n      | c = substituting from $(2)$ and $(3)$\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "15159": {"score": 0.8577253222465515, "content": {"text": "\\section{Derivatives of Unit Vectors in Polar Coordinates}\nTags: Polar Coordinates\n\n\\begin{theorem}\nConsider a particle $p$ moving in the plane.\nLet the position of $p$ be given in polar coordinates as $\\left\\langle{r, \\theta}\\right\\rangle$.\nLet:\n:$\\mathbf u_r$ be the unit vector in the direction of the radial coordinate of $p$\n:$\\mathbf u_\\theta$ be the unit vector in the direction of the angular coordinate of $p$\nThen the derivative of $\\mathbf u_r$ and $\\mathbf u_\\theta$ {{WRT}} $\\theta$ can be expressed as:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {\\d \\mathbf u_r} {\\d \\theta}\n      | r = \\mathbf u_\\theta\n}}\n{{eqn | l = \\dfrac {\\d \\mathbf u_\\theta} {\\d \\theta}\n      | r = -\\mathbf u_r\n}}\n{{end-eqn}}\n\\end{theorem}\n\n\\begin{proof}\nBy definition of sine and cosine:\n{{begin-eqn}}\n{{eqn | n = 1\n      | l = \\mathbf u_r\n      | r = \\mathbf i \\cos \\theta + \\mathbf j \\sin \\theta\n}}\n{{eqn | n = 2\n      | l = \\mathbf u_\\theta\n      | r = -\\mathbf i \\sin \\theta + \\mathbf j \\cos \\theta\n}}\n{{end-eqn}}\nwhere $\\mathbf i$ and $\\mathbf j$ are the unit vectors in the $x$-axis and $y$-axis respectively.\n:600px\nDifferentiating $(1)$ and $(2)$ {{WRT|Differentiation}} $\\theta$ gives:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {\\d \\mathbf u_r} {\\d \\theta}\n      | r = -\\mathbf i \\sin \\theta + \\mathbf j \\cos \\theta\n      | c = \n}}\n{{eqn | r = \\mathbf u_\\theta\n      | c = \n}}\n{{eqn | l = \\dfrac {\\d \\mathbf u_\\theta} {\\d \\theta}\n      | r = -\\mathbf i \\cos \\theta - \\mathbf j \\sin \\theta\n      | c = \n}}\n{{eqn | r = -\\mathbf u_r\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "7658": {"score": 0.8610084652900696, "content": {"text": "\\begin{definition}[Definition:Rate of Change]\nThe '''rate of change''' of a physical quantity is its (first) derivative with respect to an independent variable which is usually, but not exclusively, time.\nLoosely speaking, it means '''how fast something changes''', with a wider scope than change in physical displacement.\n\\end{definition}"}}}}, "TheoremQA_jianyu_xu/Catalan_1.json": {"gold": {"17913": 1}, "retrieved": {"3956": {"score": 0.828590989112854, "content": {"text": "\\begin{definition}[Definition:Hat-Check Distribution]\nLet $X$ be a discrete random variable on a probability space $\\struct {\\Omega, \\Sigma, \\Pr}$.\nLet $\\Img X = \\set {0, 2, 3, \\ldots, n}$\nLet $X$ represent the number of elements in a a totally ordered set with $n$ elements that are not in the correct order.\nThen $X$ has the '''Hat-Check distribution with parameter $n$''' (where $n > 0$) {{iff}}:\n\\end{definition}"}}, "23451": {"score": 0.8290126919746399, "content": {"text": "\\section{X Choose n leq y Choose n + z Choose n-1 where n leq y leq x leq y+1 and n-1 leq z leq y}\nTags: Binomial Coefficients\n\n\\begin{theorem}\nLet $n \\in \\Z_{\\ge 0}$ be a positive integer.\nLet $x, y \\in \\R$ be real numbers which satisfy:\n:$n \\le y \\le x \\le y + 1$\nLet $z$ be the unique real number $z$ such that:\n:$\\dbinom x {n + 1} = \\dbinom y {n + 1} + \\dbinom z n$\nwhere $n - 1 \\le z \\le y$.\nIts uniqueness is proved at Uniqueness of Real $z$ such that $\\dbinom x {n + 1} = \\dbinom y {n + 1} + \\dbinom z n$.\nThen:\n:$\\dbinom x n \\le \\dbinom y n + \\dbinom z {n - 1}$\n\\end{theorem}\n\n\\begin{proof}\nIf $z \\ge n$, then from Ordering of Binomial Coefficients:\n:$\\dbinom z {n + 1} \\le \\dbinom y {n + 1}$\nOtherwise $n - 1 \\le z \\le n$, and:\n:$\\dbinom z {n + 1} \\le 0 \\le \\dbinom y {n + 1}$\nIn either case:\n:$(1): \\quad \\dbinom z {n + 1} \\le \\dbinom y {n + 1}$\nTherefore:\n{{begin-eqn}}\n{{eqn | l = \\dbinom {z + 1} {n + 1}\n      | r = \\dbinom z {n + 1} + \\dbinom z n\n      | c = Pascal's Rule\n}}\n{{eqn | o = \\le\n      | r = \\dbinom y {n + 1} + \\dbinom z n\n      | c = \n}}\n{{eqn | r = \\dbinom x {n + 1}\n      | c = by hypothesis\n}}\n{{end-eqn}}\nand so $x \\ge z + 1$.\nNow we are to show that every term of the summation:\n:$\\ds \\binom x {n + 1} - \\binom y {n + 1} = \\sum_{k \\mathop \\ge 0} \\dbinom {z - k} {n - k} t_k$\nwhere:\n:$t_k = \\dbinom {x - z - 1 + k} {k + 1} - \\dbinom {y - z - 1 + k} {k + 1}$\nis negative.\nBecause $z \\ge n - 1$, the binomial coefficient $\\dbinom {z - k} {n - k}$ is non-negative.\nBecause $x \\ge z + 1$, the binomial coefficient $\\dbinom {x - z - 1 + k} {k + 1}$ is also non-negative.\nTherefore:\n:$z \\le y \\le x$\nimplies that:\n:$\\dbinom {y - z - 1 + k} {k + 1} \\le \\dbinom {x - z - 1 + k} {k + 1}$\nWhen $x = y$ and $z = n - 1$ the result becomes:\n:$\\dbinom x n \\le \\dbinom x n + \\dbinom {n - 1} {n - 1}$\nwhich reduces to:\n:$\\dbinom x n \\le \\dbinom x n + 1$\nwhich is true.\nOtherwise:\n{{begin-eqn}}\n{{eqn | l = \\dbinom x n - \\dbinom y n - \\dbinom z {n - 1}\n      | r = \\sum_{k \\mathop \\ge 0} \\dbinom {z - k} {n - 1 - k} \\left({t_k - \\delta_{k 0} }\\right)\n      | c = where $\\delta_{k 0}$ is the Kronecker delta\n}}\n{{eqn | r = \\sum_{k \\mathop \\ge 0} \\dfrac {n - k} {z - n + 1} \\dbinom {z - k} {n - k} \\left({t_k - \\delta_{k 0} }\\right)\n      | c = Factors of Binomial Coefficient\n}}\n{{end-eqn}}\nThis is less than or equal to:\n{{begin-eqn}}\n{{eqn | l = \\sum_{k \\mathop \\ge 0} \\dfrac {n - 1} {z - n + 1} \\dbinom {z - k} {n - k} \\left({t_k - \\delta_{k 0} }\\right)\n      | r = \\dfrac {n - 1} {z - n + 1} \\left({\\dbinom x {n + 1} - \\dbinom y {n + 1} - \\dbinom z n}\\right)\n      | c = \n}}\n{{eqn | r = 0\n      | c = because $t_0 - 1 = x - y - 1 \\le 0$\n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\n\\end{proof}\n\n"}}, "16790": {"score": 0.8345142602920532, "content": {"text": "\\section{Cayley's Formula}\nTags: Named Theorems, Graph Theory, Combinatorics\n\n\\begin{theorem}\nThe number of distinct labeled trees with $n$ nodes is $n^{n - 2}$.\n\\end{theorem}\n\n\\begin{proof}\nFollows directly from Bijection between Pr\u00fcfer Sequences and Labeled Trees.\nThis shows that there is a bijection between the set of labeled trees with $n$ nodes and the set of all Pr\u00fcfer sequences of the form:\n:$\\tuple {\\mathbf a_1, \\mathbf a_2, \\ldots, \\mathbf a_{n - 2} }$\nwhere each of the $\\mathbf a_i$'s is one of the integers $1, 2, \\ldots, n$, allowing for repetition.\nSince there are exactly $n$ possible values for each integer $\\mathbf a_i$, the total number of such sequences is $\\ds \\prod_{i \\mathop = 1}^{n - 2} n$.\nThe result follows from Equivalence of Mappings between Sets of Same Cardinality.\n{{qed}}\n{{Namedfor|Arthur Cayley|cat = Cayley}}\n\\end{proof}\n\n"}}, "20981": {"score": 0.8327472805976868, "content": {"text": "\\section{Sequence of Binomial Coefficients is Strictly Increasing to Half Upper Index}\nTags: Binomial Coefficients\n\n\\begin{theorem}\nLet $n \\in \\Z_{>0}$ be a strictly positive integer.\nLet $\\dbinom n k$ be the binomial coefficient of $n$ over $k$ for a positive integer $k \\in \\Z_{\\ge 0}$.\nLet $S_n = \\sequence {x_k}$ be the sequence defined as:\n:$x_k = \\dbinom n k$\nThen $S_n$ is strictly increasing exactly where $0 \\le k < \\dfrac n 2$.\n\\end{theorem}\n\n\\begin{proof}\nWhen $k \\ge 0$, we have:\n{{begin-eqn}}\n{{eqn | l = \\binom n {k + 1}\n      | r = \\frac {n!} {\\paren {k + 1}! \\paren {n - k - 1}!}\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{eqn | r = \\frac {n - k} {n - k} \\frac {n!} {\\paren {k + 1}! \\paren {n - k - 1}!}\n      | c = \n}}\n{{eqn | r = \\frac {n - k} {\\paren {k + 1} \\paren {n - k} } \\frac {n!} {k! \\paren {n - k - 1}!}\n      | c = extracting $k + 1$ from its factorial\n}}\n{{eqn | r = \\frac {n - k} {k + 1} \\frac {n!} {k! \\paren {n - k}!}\n      | c = inserting $n - k$ into its factorial\n}}\n{{eqn | r = \\frac {n - k} {k + 1} \\binom n k\n      | c = {{Defof|Binomial Coefficient}}\n}}\n{{end-eqn}}\nIn order for $S_n$ to be strictly increasing, it is necessary for $\\dfrac {n - k} {k + 1} > 1$.\nSo:\n{{begin-eqn}}\n{{eqn | l = \\dfrac {n - k} {k + 1}\n      | o = >\n      | r = 1\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = n - k\n      | o = >\n      | r = k + 1\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = n\n      | o = >\n      | r = 2 k + 1\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = n\n      | o = >\n      | r = 2 \\paren {k + 1} - 1\n      | c = \n}}\n{{end-eqn}}\nThus $\\dbinom n {k + 1} > \\dbinom n k$ {{iff}} $k + 1$ is less than half of $n$.\nHence the result.\n{{Qed}}\nCategory:Binomial Coefficients\n\\end{proof}\n\n"}}, "7458": {"score": 0.8311672806739807, "content": {"text": "\\begin{definition}[Definition:Pr\u00fcfer Sequence]\nA '''Pr\u00fcfer sequence''' of order $n$ is a (finite) sequence of integers:\n:$\\left({\\mathbf a_1, \\mathbf a_2, \\ldots, \\mathbf a_{n-2}}\\right)$\nsuch that $\\forall i: 1 \\le i \\le n-2: 1 \\le \\mathbf a_i \\le n$.\nThat is, it is a (finite) sequence of $n - 2$ integers between $1$ and $n$.\n\\end{definition}"}}, "15959": {"score": 0.8456918001174927, "content": {"text": "\\section{Condition for Increasing Binomial Coefficients}\nTags: Condition for Increasing Binomial Coefficients, Binomial Coefficients\n\n\\begin{theorem}\nLet $n \\in \\Z_{> 0}$ be a (strictly) positive integer.\nLet $\\dbinom n k$ denote a binomial coefficient for $k \\in \\N$.\nThen:\n:$\\dbinom n k < \\dbinom n {k + 1} \\iff 0 \\le k < \\dfrac {n - 1} 2$\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction on $n$.\nFor all $n \\in \\Z_{> 0}$, let $\\map P n$ be the proposition:\n:$\\dbinom n k < \\dbinom n {k + 1} \\iff 0 \\le k < \\dfrac {n - 1} 2$\nFirst we investigate the edge case.\nLet $n = 1$.\nThen we have:\n{{begin-eqn}}\n{{eqn | l = \\dbinom 1 0\n      | r = 1\n      | c = Binomial Coefficient with Zero\n}}\n{{eqn | l = \\dbinom 1 1\n      | r = 1\n      | c = Binomial Coefficient with Self\n}}\n{{end-eqn}}\nThus we see:\n:there are no $k$ such that $0 \\le k < \\dfrac {1 - 1} 2 = 0$\nand:\n:there are no $k$ such that $\\dbinom 1 k < \\dbinom 1 {k + 1}$\nThus $\\map P 1$ is seen to hold.\n\\end{proof}\n\n"}}, "17363": {"score": 0.8347561955451965, "content": {"text": "\\section{Bell Number as Summation over Lower Index of Stirling Numbers of the Second Kind}\nTags: Stirling Numbers, Bell Numbers\n\n\\begin{theorem}\nLet $B_n$ be the Bell number for $n \\in \\Z_{\\ge 0}$.\nThen:\n:$B_n = \\ds \\sum_{k \\mathop = 0}^n {n \\brace k}$\nwhere $\\ds {n \\brace k}$ denotes a Stirling number of the second kind.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of Bell numbers:\n:$B_n$ is the number of partitions of a (finite) set whose cardinality is $n$.\nFirst consider the case where $n > 0$.\nFrom Number of Set Partitions by Number of Components, the number of partitions of $S$ into $k$ components is $\\ds {n \\brace k}$.\nThus the total number of all partitions of $S$ is the sum of $\\ds {n \\brace k}$ where $k$ runs from $1$ to $n$.\nHence:\n:$B_n = \\ds \\sum_{k \\mathop = 1}^n {n \\brace k}$\nBut $\\ds {n \\brace 0} = 0$ for all $n \\in \\Z_{> 0}$, and so:\n:$B_n = \\ds \\sum_{k \\mathop = 0}^n {n \\brace k}$\nNow consider the edge case where $n = 0$, that is, $S = \\O = \\set{}$.\nThere is by conventional definition one partition of $\\O$, that is: $\\set \\O$\nIn this case, we have:\n:$\\ds {0 \\brace 0} = 1$\nand so:\n:$B_0 = \\ds \\sum_{k \\mathop = 0}^0 {n \\brace k} = {0 \\brace 0} = 1$\n{{qed}}\nCategory:Bell Numbers\nCategory:Stirling Numbers\n\\end{proof}\n\n"}}, "17913": {"score": 0.8575509786605835, "content": {"text": "\\section{Number of Distinct Parenthesizations on Word}\nTags: Parenthesization, Catalan Numbers\n\n\\begin{theorem}\nLet $w_n$ denote an arbitrary word of $n$ elements.\nThe number of distinct parenthesizations of $w_n$ is the Catalan number $C_{n - 1}$:\n:$C_{n - 1} = \\dfrac 1 n \\dbinom {2 \\paren {n - 1} } {n - 1}$\n\\end{theorem}\n\n\\begin{proof}\nLet $w_n$ denote an arbitrary word of $n$ elements.\nLet $a_n$ denote the number of ways $W_n$ elements may be parenthesized.\nFirst note that we have:\n{{begin-eqn}}\n{{eqn | l = a_1\n      | r = 1\n      | c = \n}}\n{{eqn | l = a_2\n      | r = 1\n      | c = \n}}\n{{eqn | l = a_3\n      | r = 2\n      | c = that is, $b_1 \\paren {b_2 b_3}$ and $\\paren {b_1 b_2} b_3$\n}}\n{{end-eqn}}\nand from Parenthesization of Word of $4$ Elements:\n:$a_4 = 5$\nConsider a word $w_{n + 1}$ of $n + 1$ elements.\nThen $w_{n + 1}$ can be formed as any one of:\n:$w_1$ concatenated with $w_n$\n:$w_2$ concatenated with $w_{n - 1}$\n:$\\dotsc$ and so on until:\n:$w_n$ concatenated with $w_1$\nThus the $i$th row in the above sequence is the number of parenthesizations of $w_{n + 1}$ in which the two outermost parenthesizations contain $i$ and $n - i + 1$ terms respectively.\nWe have that:\n:there are $a_i$ parenthesizations of $w_i$\n:there are $a_{n - i + 1}$ parenthesizations of $w_{n - i + 1}$\nHence the total number of parenthesizations of $w_{n + 1}$ is the sum of all these parenthesizations for $1 \\le i \\le n$.\nThat is:\n:$(1): \\quad a_{n + 1} = a_1 a_n + a_2 a_{n - 1} + \\dotsb + a_n a_1$\nLet us start with the generating function:\n:$\\ds \\map {G_A} z = \\sum_{n \\mathop = 1}^\\infty a_n z^n$\nThen:\n{{begin-eqn}}\n{{eqn | l = \\map {G_A} z\n      | r = z + \\sum_{n \\mathop = 2}^\\infty \\paren {a_1 a_n + a_2 a_{n - 1} + \\dotsb + a_n a_1} z^n\n      | c = from $(1)$\n}}\n{{eqn | r = z + \\sum_{n \\mathop = 1}^\\infty a_n z^n \\sum_{n \\mathop = 1}^\\infty a_n z^n\n      | c = \n}}\n{{eqn | r = z + \\paren {\\map {G_A} z}^2\n      | c = \n}}\n{{end-eqn}}\nThus $\\map {G_A} z$ satisfies the quadratic equation:\n:$\\paren {\\map {G_A} z}^2 - \\map {G_A} z + z = 0$\nBy the Quadratic Formula, this gives:\n:$\\map {G_A} z = \\dfrac {1 \\pm \\sqrt {1 - 4 z} } 2$\nSince $\\map {G_A} 0 = 0$, we can eliminate the positive square root and arrive at:\n:$(2): \\quad \\map {G_A} z = \\dfrac 1 2 - \\dfrac {\\sqrt {1 - 4 z} } 2$\nExpanding $\\sqrt {1 - 4 z}$ using the Binomial Theorem:\n:$\\ds \\map {G_A} z = \\dfrac 1 2 - \\dfrac 1 2 \\sum_{n \\mathop = 0}^\\infty \\paren {-1}^n \\dbinom {\\frac 1 2} n 4^n z^n$\nwhere:\n:$\\dbinom {\\frac 1 2} 0 = 1$\nand:\n:$\\dbinom {\\frac 1 2} n = \\dfrac {\\frac 1 2 \\paren {\\frac 1 2 - 1} \\dotsm \\paren {\\frac 1 2 - n + 1} } {n!}$\nAs a result:\n:$\\ds (3): \\quad \\map {G_A} z = -\\dfrac 1 2 \\sum_{n \\mathop = 1}^\\infty \\paren {-1}^n \\dbinom {\\frac 1 2} n 4^n z^n$\nWe can expand $(3)$ as a Taylor series about $0$.\nAs such a series, when it exists, is unique, the coefficients must be $a_n$.\nHence:\n{{begin-eqn}}\n{{eqn | l = a_n\n      | r = -\\dfrac 1 2 \\paren {-1}^n \\dbinom {\\frac 1 2} n 4^n\n      | c = \n}}\n{{eqn | r = \\paren {-1}^{n - 1} \\dfrac 1 2 \\dfrac {\\frac 1 2 \\paren {\\frac 1 2 - 1} \\dotsm \\paren {\\frac 1 2 - n + 1} } {n!} 4^n\n      | c = \n}}\n{{eqn | r = \\paren {-1}^n \\dfrac 1 2 \\dfrac {\\paren {-1} \\paren {1 - 2} \\dotsm \\paren {1 - 2 \\paren {n - 1 } } 2^n} {n!}\n      | c = \n}}\n{{eqn | r = \\dfrac 1 2 \\dfrac {1 \\times 3 \\times \\dotsb \\times \\paren {2 n - 3} 2^n} {n!}\n      | c = \n}}\n{{eqn | r = \\dfrac 1 2 \\dfrac {1 \\times 3 \\times \\dotsb \\times \\paren {2 n - 3} n! 2^n} {n! n!}\n      | c = multiplying top and bottom by $n!$\n}}\n{{eqn | r = \\dfrac 1 2 \\dfrac {1 \\times 2 \\times 3 \\times 4 \\times \\dotsb \\times \\paren {2 n - 4} \\paren {2 n - 3} \\paren {2 n - 2} \\paren {2 n} } {\\paren {n!}^2}\n      | c = \n}}\n{{eqn | r = \\dfrac 1 2 \\dfrac {1 \\times 2 \\times 3 \\times 4 \\times \\dotsb \\times \\paren {2 n - 2} } {\\paren {\\paren {n - 1}!}^2}\n      | c = \n}}\n{{eqn | r = \\frac 1 n \\binom {2 n - 2} {n - 1}\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "17948": {"score": 0.8376221060752869, "content": {"text": "\\section{Number of Set Partitions by Number of Components}\nTags: Set Partitions, Number of Set Partitions, Stirling Numbers, Combinatorics, Number of Set Partitions by Number of Components\n\n\\begin{theorem}\nLet $S$ be a (finite) set whose cardinality is $n$.\nLet $\\map f {n, k}$ denote the number of different ways $S$ can be partitioned into $k$ components.\nThen:\n:$\\ds \\map f {n, k} = {n \\brace k}$\nwhere $\\ds {n \\brace k}$ denotes a Stirling number of the second kind.\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction on $n$.\nFor all $n \\in \\Z_{\\ge 0}$, let $\\map P n$ be the proposition:\n:$\\ds \\map f {n, k} = {n \\brace k}$\n$\\map P 0$ is the degenerate case:\n:$\\ds \\map f {0, k} = \\delta_{0 k} = {0 \\brace k}$\nThat is: the empty set can be partitioned one and only one way: into $0$ subsets.\nThus $\\map P 0$ is seen to hold.\nThe remainder of the proof considers $n \\in \\Z_{> 0}$.\nFirst we note that when $k < 1$ or $k > n$:\n:$\\ds \\map f {n, k} = 0 = {n \\brace k}$\nHence, throughout, we consider only such $k$ as $1 \\le k \\le n$.\nWe define the representative set of cardinality $n$ to be:\n:$S_n := \\set {1, 2, \\ldots, n}$\n\\end{proof}\n\n"}}, "20393": {"score": 0.838978111743927, "content": {"text": "\\section{Recurrence Relation for Bell Numbers}\nTags: Bell Numbers\n\n\\begin{theorem}\nLet $B_n$ be the Bell number for $n \\in \\Z_{\\ge 0}$.\nThen:\n:$B_{n + 1} = \\ds \\sum_{k \\mathop = 0}^n \\dbinom n k B_k$\nwhere $\\dbinom n k$ are binomial coefficients.\n\\end{theorem}\n\n\\begin{proof}\nBy definition of Bell numbers:\n:$B_{n + 1}$ is the number of partitions of a (finite) set whose cardinality is $n + 1$.\nLet $k \\in \\set {k \\in \\Z: 0 \\le k \\le n}$.\nLet us form a partition of a (finite) set $S$ with cardinality $n + 1$ such that one component has $n + 1 - k > 0$ elements.\nWe can do this by first choosing $1$ element from $S$. We put this element into that single component.\nThen choose $k$ more elements from $S$, and let the remaining $n - k$ elements be put into the same component as the first element.\nFrom Cardinality of Set of Subsets and the definition of binomial coefficient, there are $\\dbinom n k$ ways to do this.\nFor the chosen $k$ elements, there are $B_k$ ways to partition them.\nThus there are $\\dbinom n k B_k$ possible partitions for $S$:\n:$\\dbinom n k$ of selecting $n - k$ elements to form one component with the one singled-out element\n:for each of these, $B_k$ ways to partition the remaining $k$ elements.\nSumming the number of ways to do this over all possible $k$:\n:$\\ds B_{n + 1} = \\sum_{k \\mathop = 0}^n \\dbinom n k B_k$\n{{qed}}\nCategory:Bell Numbers\n\\end{proof}\n\n"}}}}, "TheoremQA_mingyin/cauchy-integral-theorem1.json": {"gold": {"16899": 1}, "retrieved": {"15632": {"score": 0.9038476943969727, "content": {"text": "\\section{Contour Integral of Gamma Function}\nTags: Gamma Function, Complex Analysis\n\n\\begin{theorem}\nLet $\\Gamma$ denote the gamma function.\nLet $y$ be a positive number.\nThen for any positive number $c$:\n:$\\ds \\frac 1 {2 \\pi i} \\int_{c - i \\infty}^{c + i \\infty} \\map \\Gamma t y^{-t} \\rd t = e^{-y}$\n\\end{theorem}\n\n\\begin{proof}\nLet $L$ be the rectangular contour with the vertices $c \\pm i R$, $- N - \\dfrac 1 2 \\pm i R$.\nWe will take the Contour Integral of $\\map \\Gamma t y^{-t}$ about the rectangular contour $L$.\nNote from Poles of Gamma Function, that the poles of this function are located at the non-positive integers.\nThus, by the Residue Theorem, we have:\n:$\\ds \\oint_L \\map \\Gamma t y^{-t} \\rd z = 2 \\pi i \\sum_{k \\mathop = 0}^N \\map {\\operatorname{Res} } {-k}$\nThus, we obtain:\n:$\\ds \\lim_{N \\mathop \\to \\infty} \\lim_{R \\mathop \\to \\infty} \\oint_L \\map \\Gamma t y^{-t} \\rd z = 2 \\pi i \\sum_{k \\mathop = 0}^\\infty \\map {\\operatorname{Res} } {-k}$\nFrom Residues of Gamma Function, we see that:\n:$\\map {\\operatorname{Res} } {-k} = \\dfrac {\\paren {-1}^k y^k} {k!}$ \nWhich gives us: \n{{begin-eqn}}\n{{eqn\t| l = \\lim_{N \\mathop \\to \\infty} \\lim_{R \\mathop \\to \\infty} \\oint_L \\map \\Gamma t y^{-t} \\rd t\n\t| r = 2 \\pi i \\sum_{k \\mathop = 0}^\\infty \\map {\\operatorname{Res} } {-k}\n}}\n{{eqn\t| r = 2 \\pi i \\sum_{k \\mathop = 0}^\\infty \\frac {\\paren {-1}^k y^k} {k!}\n}}\n{{eqn\t| r = 2 \\pi i e^{-y}\n\t| c = Power Series Expansion for Exponential Function\n}}\n{{end-eqn}}\nWe aim to show that the all but the {{RHS}} of the rectangular contour go to $0$ as we take these limits, as our result follows readily from this.\n{{explain|This should be made clearer}}\nThe top and bottom portions of the contour can be parameterized by:\n:$\\map \\gamma t = c \\pm i R - t$\nwhere $0 < t < c + N + \\dfrac 1 2$. \n{{explain|A labelled diagram here would help}}\nThe modulus of the contour integral is therefore given by:\n{{begin-eqn}}\n{{eqn | l = \\cmod {\\int_0^{c + N + \\frac 1 2} \\map \\Gamma {\\map \\gamma t} y^{- \\map \\gamma t} \\map {\\gamma'} t \\rd t}\n      | r = \\cmod {\\int_0^{c + N + \\frac 1 2} \\map \\Gamma {c \\pm i R - t} y^{-\\paren {c \\pm i R - t} } \\rd t}\n}}\n{{eqn | r = \\cmod {y^{-\\paren {c \\pm i R} } } \\cmod {\\int_0^{c + N + \\frac 1 2} \\map \\Gamma {c \\pm i R - t} y^t \\rd t}\n}}\n{{eqn | r = y^{-c} \\cmod {\\int_0^{c + N + \\frac 1 2} \\map \\Gamma {c \\pm i R - t} y^t \\rd t}\n}}\n{{end-eqn}}\nFrom Bound on Complex Values of Gamma Function, we have that: \n{{begin-eqn}}\n{{eqn | l = \\cmod {\\map \\Gamma {c \\pm i R - t} y^t}\n      | o = \\le\n      | r = \\frac {\\cmod {c - t + i} } {\\cmod {c - t + i R} } \\cmod {\\map \\Gamma {c - t + i} y^t}\n      | c = (1)\n}}\n{{end-eqn}}\nfor all $\\cmod R \\ge 1$. Because $\\cmod R \\ge 1$, we have that\n{{begin-eqn}}\n{{eqn | l = \\frac {\\cmod {c - t + i} } {\\cmod {c - t + i R} }\n      | o = \\le\n      | r = 1\n}}\n{{end-eqn}}\n:Combining the two inequalities we obtain:\n{{begin-eqn}}\n{{eqn | l = \\cmod {\\map \\Gamma {c \\pm i R - t} y^t}\n      | o = \\le\n      | r = \\cmod {\\map \\Gamma {c - t + i} y^t}\n      | c = (2)\n}}\n{{end-eqn}}\nWe see that:\n:$\\ds \\int_0^{c + N + \\frac 1 2} \\cmod {\\map \\Gamma {c- t + i} y^t} \\rd t < \\infty$\nas the poles of Gamma are at the nonpositive integers, which means that the integral is a definite integral of a continuous function. \n{{explain|I know you're referring to convergence here, but you should make it clearer.}} \nThe above is enough to allow for the interchange of limits by the Dominated Convergence Theorem, thus we have:\n \n{{begin-eqn}}\n{{eqn |l = \\lim_{N \\mathop \\to \\infty} \\lim_{R \\mathop \\to \\infty} y^{-c} \\cmod {\\int_0^{c + N + \\frac 1 2} \\map \\Gamma {c \\pm i R - t} y^t \\rd t}\n      |r = \\lim_{N \\mathop \\to \\infty} y^{-c} \\cmod {\\int_0^{c + N + \\frac 1 2} \\lim_{R \\mathop \\to \\infty} \\map \\Gamma {c \\pm i R - t} y^t \\rd t}\n}}\n{{end-eqn}}\nBut using Equation $(1)$ from above we see:\n{{begin-eqn}}\n{{eqn | l = 0 \n      | o = \\le\n      | r = \\lim_{R \\mathop \\to \\infty} \\cmod {\\map \\Gamma {c \\pm i R - t} y^t}\n}}\n{{eqn | o = \\le\n      | r = \\lim_{R \\mathop \\to \\infty} \\frac {\\cmod {c - t + i} } {\\cmod {c - t + i R} } \\cmod {\\map \\Gamma {c - t + i} y^t}\n}}\n{{eqn | r = 0\n}}\n{{end-eqn}}\nThus by the Squeeze Theorem we have:\n:$\\ds \\lim_{R \\mathop \\to \\infty} \\cmod {\\map \\Gamma {c \\pm i R - t} } = 0$\nWhich means we have:\n{{begin-eqn}}\n{{eqn | l = \\lim_{N \\mathop \\to \\infty} \\lim_{R \\mathop \\to \\infty} y^{-c} \\cmod {\\int_0^{c + N + \\frac 1 2} \\map \\Gamma {c \\pm i R - t} y^t \\rd t} \n      | r = \\lim_{N \\mathop \\to \\infty} y^{-c} \\cmod {\\int_0^{c + N + \\frac 1 2}  \\lim_{R \\mathop \\to \\infty} \\map \\Gamma {c \\pm i R - t} y^t \\rd t}\n}}\n{{eqn | r = \\lim_{N \\mathop \\to \\infty} y^{-c} \\cmod {\\int_0^{c + N + \\frac 1 2} 0 y^t \\rd t}\n}}\n{{eqn | r = \\lim_{N \\mathop \\to \\infty} 0 \n}}\n{{eqn | r = 0\n}}\n{{end-eqn}} \nThus we have that the top and bottom of the contour go to $0$ in the limit. \nThe {{LHS}} of the contour may be parameterized by:\n:$\\map \\gamma t = N - \\dfrac 1 2 - it$\nwhere $t$ runs from $-R$ to $R$. \nThus the absolute value of integral of the {{LHS}} is given as:\n{{begin-eqn}}\n{{eqn | l = \\cmod {\\int_{-R}^R \\map \\Gamma {\\map \\gamma t} y^{- \\map \\gamma t} \\map {\\gamma'} t \\rd t}\n      | r = \\cmod {\\int_{-R}^R \\map \\Gamma {- N - \\frac 1 2 - i t} y^{- \\paren {- N - \\frac 1 2 - i t} } \\paren {-i} \\rd t}\n}}\n{{eqn | r = \\cmod {\\int_{-R}^R \\frac {\\map \\Gamma {- N + \\frac 3 2 - i t} } {\\paren {- N - \\frac 1 2 - i t} \\paren {- N + \\frac 1 2 - i t} } y^{- \\paren {- N - \\frac 1 2 - i t} } \\paren {-i} \\rd t}\n      | c = Gamma Difference Equation\n}}\n{{eqn | o = \\le\n      | r = \\int_{-R}^R \\cmod {\\frac {\\map \\Gamma {- N + \\frac 3 2 - i t} } {\\paren {- N - \\frac 1 2 - i t} \\paren {- N + \\frac 1 2 - i t} } y^{- \\paren {- N - \\frac 1 2 - i t} } \\paren {-i} } \\rd t\n      | c = Modulus of Complex Integral\n}}\n{{eqn | r = \\int_{-R}^R \\cmod {\\frac {\\map \\Gamma {- N + \\frac 3 2 - i t} } {\\paren {- N - \\frac 1 2 - i t} \\paren {- N + \\frac 1 2 - i t} } y^{N + \\frac 1 2} } \\rd t \n}}\n{{eqn | o = \\le\n      | r = \\int_{-R}^R \\frac {\\cmod {\\map \\Gamma {- N + \\frac 3 2} } } {\\cmod {\\paren {- N - \\frac 1 2 -i t} \\paren {- N + \\frac 1 2 - i t} } } y^{N + \\frac 1 2} \\rd t\n      | c = See equation (2) above\n}}\n{{eqn | r = \\cmod {\\map \\Gamma {- N + \\frac 3 2} } y^{N + \\frac 1 2} \\int_{-R}^R \\frac 1 {\\cmod {\\paren {- N - \\frac 1 2 -i t} \\paren {- N + \\frac 1 2 - i t} } } \\rd t\n}}\n{{eqn | o = \\le\n      | r = \\cmod {\\map \\Gamma {- N + \\frac 3 2} } y^{N + \\frac 1 2} \\int_{-R}^R \\frac 1 {\\cmod {\\paren {- N + \\frac 1 2 - i t} }^2} \\rd t\n}}\n{{eqn | r = \\cmod {\\map \\Gamma {- N + \\frac 3 2} } y^{N + \\frac 1 2} \\int_{-R}^R \\frac 1 {\\paren {- N + \\frac 1 2}^2 + t^2} \\rd t\n      | c = {{Defof|Complex Modulus}}\n}}\n{{eqn | r = \\cmod {\\map \\Gamma {- N + \\frac 3 2} } y^{N + \\frac 1 2} \\frac {\\map \\arctan {\\frac R {- N + \\frac 1 2} } - \\map \\arctan {\\frac {-R} {- N + \\frac 1 2} } }{- N + \\frac 1 2}\n      | c = Derivative of Arctangent Function/Corollary\n}}\n{{end-eqn}}\nThus we have:\n{{begin-eqn}}\n{{eqn | l = 0\n      | o = \\le\n      | r = \\lim_{N \\mathop \\to \\infty} \\lim_{R \\mathop \\to \\infty} \\cmod {\\int_{-R}^R \\map \\Gamma {- N - \\frac 1 2 - i t} y^{- \\paren {- N - \\frac 1 2 - i t} } \\paren {-i} \\rd t}\n}}\n{{eqn | o = \\le\n      | r = \\lim_{N \\mathop \\to \\infty} \\lim_{R \\to \\infty} \\cmod {\\map \\Gamma {- N + \\frac 3 2} } y^{N + \\frac 1 2} \\frac {\\map \\arctan {\\frac R {-N+ \\frac 1 2} } - \\map \\arctan {\\frac {-R} {- N + \\frac 1 2} } } {- N + \\frac 1 2}\n}}\n{{eqn | r = \\lim_{N \\mathop \\to \\infty} \\cmod {\\map \\Gamma {- N + \\frac 3 2} } y^{N + \\frac 1 2} \\frac \\pi {- N + \\frac 1 2}\n}}\n{{eqn | r = \\lim_{N \\mathop \\to \\infty} \\frac {2^{2 N - 2} \\paren {N - 1}!} {\\paren {2 N - 2}!} \\sqrt \\pi y^{N + \\frac 1 2} \\frac \\pi {- N + \\frac 1 2}\n      | c = Gamma Function of Negative Half-Integer\n}}\n{{eqn | r = \\lim_{N \\mathop \\to \\infty} \\frac{2^{2 N - 2} \\sqrt {2 \\pi \\paren {N - 1} } \\paren {\\frac {N - 1} e}^{N - 1} } {\\sqrt{2 \\pi \\paren {2 N - 2} } \\paren {\\frac {2 \\paren {N - 1} } e}^{2 N - 2} } \\sqrt \\pi y^{N + \\frac 1 2} \\frac \\pi {- N + \\frac 1 2} \n      | c = Stirling's Formula\n}}\n{{eqn | r = \\lim_{N \\mathop \\to \\infty} \\frac{2^{2 N - 2} \\paren {\\frac {N - 1} e}^{N - 1} } {2^{2 N -2} \\sqrt{2} \\paren {\\frac {N - 1} e}^{2 N - 2} } \\sqrt \\pi y^{N + \\frac 1 2} \\frac \\pi {-N+\\frac 1 2} \n}}\n{{eqn | r = \\lim_{N \\mathop \\to \\infty} \\frac 1 {\\sqrt{2} \\paren {\\frac {N - 1} e}^{N - 1} } \\sqrt \\pi y^{N + \\frac 1 2} \\frac \\pi {- N + \\frac 1 2} \n}}\n{{eqn | r = 0\n}}\n{{end-eqn}}\nWhich gives us:\n{{begin-eqn}}\n{{eqn | l = \\lim_{N \\mathop \\to \\infty} \\lim_{R \\mathop \\to \\infty} \\cmod {\\int_{-R}^R \\map \\Gamma {- N - \\frac 1 2 - i t} y^{- \\paren {- N - \\frac 1 2 - i t} } \\paren {-i} \\rd t}\n      | r = 0\n      | c = Squeeze Theorem\n}}\n{{end-eqn}}\nThus we have the left, top, and bottom of the rectangular contour go to 0 in the limit, which gives us:\n{{begin-eqn}}\n{{eqn | l = \\frac 1 {2 \\pi i} \\int_{c - i \\infty}^{c + i \\infty} \\map \\Gamma t y^{-t} \\rd t\n      | r = \\frac 1 {2 \\pi i} \\lim_{N \\mathop \\to \\infty} \\lim_{R \\mathop \\to \\infty} \\oint_L \\map \\Gamma t y^{-t} \\rd t\n}}\n{{eqn | r = e^{-y} \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Gamma Function\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "20560": {"score": 0.9083545207977295, "content": {"text": "\\section{Residue at Simple Pole}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $f: \\C \\to \\C$ be a function meromorphic on some region, $D$, containing $a$. \nLet $f$ have a simple pole at $a$. \nThen the residue of $f$ at $a$ is given by:\n:$\\ds \\Res f a = \\lim_{z \\mathop \\to a} \\paren {z - a} \\map f z$\n\\end{theorem}\n\n\\begin{proof}\nBy Existence of Laurent Series, there exists a Laurent series:\n:$\\ds \\map f z = \\sum_{n \\mathop = -\\infty}^\\infty c_n \\paren {z - a}^n$\nwhich is convergent in $D \\setminus \\set a$, where $\\sequence {c_n}$ is a doubly infinite sequence of complex coefficients. \nWe are given that $f$ has only a simple pole at $a$.\nThus $c_n = 0$ for $n < -1$.\nSo we can write: \n:$\\ds \\map f z = \\sum_{n \\mathop = 0}^\\infty c_n \\paren {z - a}^n + \\frac {c_{-1} } {z - a}$\nThen:\n{{begin-eqn}}\n{{eqn\t| l = \\lim_{z \\mathop \\to a} \\paren {z - a} \\map f z\n\t| r = \\lim_{z \\mathop \\to a} \\paren {z - a} \\paren {\\sum_{n \\mathop = 0}^\\infty c_n \\paren {z - a}^n + \\frac {c_{-1} } {z - a} }\n}}\n{{eqn\t| r = \\lim_{z \\mathop \\to a} \\paren {\\sum_{n \\mathop = 0}^\\infty c_n \\paren {z - a}^{n + 1} + c_{-1} }\n}}\n{{eqn\t| r = \\sum_{n \\mathop = 0}^\\infty c_n \\paren {a - a}^{n + 1} + c_{-1}\n}}\n{{eqn\t| r = 0 \\sum_{n \\mathop = 0}^\\infty c_n + c_{-1} \n}}\n{{eqn\t| r = c_{-1}\n}}\n{{eqn\t| r = \\Res f a\n\t| c = {{Defof|Residue (Complex Analysis)|Residue}}\n}}\n{{end-eqn}}\n{{qed}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "22359": {"score": 0.9122518301010132, "content": {"text": "\\section{Summation Formula (Complex Analysis)}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $N \\in \\N$ be an arbitrary natural number.\nLet $C_N$ be the square embedded in the complex plane $\\C$ with vertices $\\paren {N + \\dfrac 1 2} \\paren {\\pm 1 \\pm i}$.\nLet $f$ be a meromorphic function on $\\C$ with finitely many poles. \nSuppose that: \n:$\\ds \\int_{C_N} \\paren {\\pi \\cot \\pi z} \\map f z \\rd z \\to 0$\nas $N \\to \\infty$. \nLet $X$ be the set of poles of $f$.\nThen: \n \n:$\\ds \\sum_{n \\mathop \\in \\Z \\mathop \\setminus X} \\map f n = - \\sum_{z_0 \\mathop \\in X} \\Res {\\pi \\map \\cot {\\pi z} \\map f z} {z_0}$\n \nIf $X \\cap \\Z = \\O$, this becomes: \n:$\\ds \\sum_{n \\mathop = -\\infty}^\\infty \\map f n = -\\sum_{z_0 \\mathop \\in X} \\Res {\\pi \\map \\cot {\\pi z} \\map f z} {z_0}$\n\\end{theorem}\n\n\\begin{proof}\nBy Summation Formula: Lemma, there exists a constant $A$ such that: \n:$\\cmod {\\map \\cot {\\pi z} } < A$ \nfor all $z$ on $C_N$.\nSince $f$ has only finitely many poles, we can take $N$ large enough so that no poles of $f$ lie on $C_N$. \nLet $X_N$ be the set of poles of $f$ contained in the region bounded by $C_N$. \nFrom Poles of Cotangent Function, $\\map \\cot {\\pi z}$ has poles at $z \\in \\Z$. \nLet $A_N = \\set {n \\in \\Z : -N \\le n \\le N}$\nWe then have: \n{{begin-eqn}}\n{{eqn\t| l = \\oint_{C_N} \\pi \\map \\cot {\\pi z} \\map f z \\rd z\n\t| r = 2 \\pi i \\sum_{z_0 \\mathop \\in X_N \\mathop \\cap A_N} \\Res {\\pi \\map \\cot {\\pi z} \\map f z} {z_0}\n\t| c = Residue Theorem \n}}\n{{eqn\t| r = 2 \\pi i \\paren {\\sum_{n \\mathop \\in A_N \\mathop \\setminus X_N} \\Res {\\pi \\map \\cot {\\pi z} \\map f z} n + \\sum_{z_0 \\mathop \\in X_N} \\Res {\\pi \\map \\cot {\\pi z} \\map f z} {z_0} }\n}}\n{{end-eqn}}\nWe then have, for each integer $n$: \n{{begin-eqn}}\n{{eqn\t| l = \\Res {\\pi \\map \\cot {\\pi z} \\map f z} n\n\t| r = \\lim_{z \\mathop \\to n} \\paren {\\paren {z - n} \\pi \\map \\cot {\\pi z} \\map f z}\n\t| c = Residue at Simple Pole\n}}\n{{eqn\t| r = \\map f n \\lim_{z \\mathop \\to n} \\paren {\\frac {z - n} z + 2 \\sum_{k \\mathop = 1}^\\infty \\frac {z \\paren {z - n} } {z^2 - k^2} }\n\t| c = Mittag-Leffler Expansion for Cotangent Function\n}}\n{{eqn\t| r = \\map f n \\cdot 2 \\lim_{z \\mathop \\to n} \\paren {\\frac z {z + n} }\n}}\n{{eqn\t| r = \\map f n \\frac {2 n} {2 n}\n}}\n{{eqn\t| r = \\map f n\n}}\n{{end-eqn}}\nNote that by hypothesis:\n:$\\ds \\int_{C_N} \\paren {\\pi \\cot \\pi z} \\map f z \\rd z \\to 0$\nSo, taking $N \\to \\infty$: \n:$\\ds 0 = 2 \\pi i \\paren {\\sum_{n \\mathop \\in \\Z \\mathop \\setminus X} \\map f n + \\sum_{z_0 \\mathop \\in X} \\Res {\\pi \\map \\cot {\\pi z} \\map f z} {z_0} }$\nwhich gives:\n:$\\ds \\sum_{n \\mathop \\in \\Z \\mathop \\setminus X} \\map f n = -\\sum_{z_0 \\mathop \\in X} \\Res {\\pi \\map \\cot {\\pi z} \\map f z} {z_0}$\n{{qed}}\n\\end{proof}\n\n"}}, "13913": {"score": 0.9112234711647034, "content": {"text": "\\section{Existence of Laurent Series}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $z_0 \\in \\C$ be a complex number.\nLet $R \\in \\R_{>0}$ be a real number.\nLet $\\map {B'} {z_0, R}$ be the open punctured disk at $z_0$ of radius $R$.\nLet $f: \\map {B'} {z_0, R} \\to \\C$ be holomorphic.\nThen there exists a sequence $\\sequence {a_n}_{n \\mathop \\in \\Z}$ such that:\n:$\\ds \\map f z = \\sum_{n = -\\infty}^\\infty a_n \\paren {z - z_0}^n$\nfor all $z \\in B'(z_0, R)$.\n\\end{theorem}\n\n\\begin{proof}\n{{Proofread}}\nChoose circles $C_1$ and $C_3$ centered at $z_0$ and connect them by path $C_2$ such that $z$ is inside $C_1 + C_2 - C_3 - C_2$ as shown below:\n:390px\nThis curve and its interior are contained in $B'$, so by Cauchy's Integral Formula:\n{{begin-eqn}}\n{{eqn | l = \\map f z\n      | r = \\dfrac 1 {2 \\pi i} \\int_{C_1 + C_2 - C_3 - C_2} \\dfrac {\\map f w} {\\paren {w - z} } \\rd w\n      | c = \n}}\n{{eqn | r = \\dfrac 1 {2 \\pi i} \\int_{C_1 - C_3} \\dfrac {\\map f w} {\\paren {w - z} } \\rd w\n      | c = \n}}\n{{eqn | r = \\dfrac 1 {2 \\pi i} \\int_{C_1} \\dfrac {\\map f w} {\\paren {w - z} } \\rd w - \\dfrac 1 {2 \\pi i} \\int_{C_3} \\dfrac {\\map f w} {\\paren {w - z} } \\rd w\n      | c = \n}}\n{{end-eqn}}\nSince $C_1$ contains $z$ we have for all $w$ on $C_1$:\n:$\\dfrac {\\cmod {z - z_0} } {\\cmod {w - z_0} } < 1$\nTherefore:\n{{begin-eqn}}\n{{eqn | l = \\frac 1 {2 \\pi i} \\int_{C_1} \\frac {\\map f w} {\\paren {w - z} } \\rd w\n      | r = \\frac 1 {2 \\pi i} \\int_{C_1} \\frac {\\map f w} {\\paren {w - z + z_0 - z_0} } \\rd w \n      | c = adding and subtracting $z_0$\n}}\n{{eqn | r = \\frac 1 {2 \\pi i} \\int_{C_1} \\frac {\\map f w} {\\paren {w - z_0} \\paren {1 - \\frac {z - z_0} {w - z_0} } } \\rd w \n      | c = Factoring out $w - z_0$\n}}\n{{eqn | r = \\frac 1 {2 \\pi i} \\int_{C_1} \\frac {\\map f w} {\\paren {w - z_0} } \\cdot \\frac 1 {\\paren {1 - \\frac {z - z_0} {w - z_0} } } \\rd w \n      | c = \n}}\n{{eqn | r = \\frac 1 {2 \\pi i} \\int_{C_1} \\frac {\\map f w} {\\paren {w - z_0} } \\cdot \\sum_{n \\mathop = 0}^\\infty \\paren {\\frac {z - z_0} {w - z_0} }^n \\rd w \n      | c = Sum of Infinite Geometric Sequence\n}}\n{{eqn | r = \\frac 1 {2 \\pi i} \\int_{C_1} \\sum_{n \\mathop = 0}^\\infty \\frac {\\map f w} {\\paren {w - z_0}^{n + 1} } \\cdot \\paren {z - z_0}^n \\rd w \n      | c = \n}}\n{{eqn | r = \\sum_{n \\mathop = 0}^\\infty \\paren {\\frac 1 {2 \\pi i} \\int_{C_1} \\frac {\\map f w} {\\paren {w - z_0}^{n + 1} } \\rd w} \\paren {z - z_0}^n \n      | c = \n}}\n{{eqn | r = \\sum_{n \\mathop = 0}^\\infty a_n \\paren {z - z_0}^n \n      | c = letting $a_n$ equal the value in the large parentheses above, for all $n \\ge 0$\n}}\n{{end-eqn}}\nSimilarly, since $z$ lies outside $C_3$ we have for all $w$ on $C_3$:\n:$\\dfrac {\\cmod {w - z_0} } {\\cmod {z - z_0} } < 1$\nTherefore:\n{{begin-eqn}}\n{{eqn | l = \\frac 1 {2 \\pi i} \\int_{C_3} \\frac {\\map f w} {\\paren {w - z} } \\rd w\n      | r = \\frac 1 {2 \\pi i} \\int_{C_3} \\frac {\\map f w} {\\paren {w - z + z_0 - z_0} } \\rd w \n      | c = Adding and subtracting $z_0$\n}}\n{{eqn | r = \\frac 1 {2 \\pi i} \\int_{C_3} -\\frac {\\map f w} {\\paren {z - z_0} \\paren {1 - \\frac {w - z_0} {z - z_0} } } \\rd w \n      | c = factoring out $z - z_0$\n}}\n{{eqn | r = -\\frac 1 {2 \\pi i} \\int_{C_3} \\frac {\\map f w} {\\paren {z - z_0} } \\cdot \\frac 1 {\\paren {1 - \\frac {w - z_0} {z - z_0} } } \\rd w \n      | c = \n}}\n{{eqn | r = -\\frac 1 {2 \\pi i} \\int_{C_3} \\frac {\\map f w} {\\paren {z - z_0} } \\cdot \\sum_{n \\mathop = 0}^\\infty \\paren {\\frac {w - z_0} {z - z_0} }^n \\rd w \n      | c = Sum of Infinite Geometric Sequence\n}}\n{{eqn | r = -\\sum_{n \\mathop = 0}^\\infty \\paren {\\frac 1 {2 \\pi i} \\int_{C_3} \\map f w \\paren {w - z_0}^n \\rd w} \\paren {z - z_0}^ {-n - 1} \n      | c = \n}}\n{{eqn | r = -\\sum_{k \\mathop = -1}^ {-\\infty} \\paren {\\frac 1 {2 \\pi i} \\int_{C_3} \\frac {\\map f w} {\\paren {w - z_0}^{k + 1} } \\rd w} \\paren {z - z_0}^k\n      | c = Re-indexing with $k = -n - 1$\n}}\n{{eqn | r = -\\sum_{k \\mathop = -1}^ {-\\infty} a_k \\paren {z - z_0}^k\n      | c = Letting $a_k$ equal the value in the large parentheses above, for all $k < 0$\n}}\n{{end-eqn}}\nCombining what has been shown above yields:\n{{begin-eqn}}\n{{eqn | l = \\map f z\n      | r = \\sum_{n \\mathop = 0}^\\infty a_n \\paren {z - z_0}^n + \\sum_{n \\mathop = -1}^ {-\\infty} a_n \\paren {z - z_0}^n\n      | c = \n}}\n{{eqn | r = \\sum_{n \\mathop = -\\infty}^\\infty a_n \\paren {z - z_0}^n\n      | c = \n}}\n{{end-eqn}}\n{{qed}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "17371": {"score": 0.9095297455787659, "content": {"text": "\\section{Argument Principle}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $\\gamma$ be a closed contour.\nLet $D$ be the region enclosed by $\\gamma$. \nLet $f$ be a function meromorphic on $D$. \nLet $f$ be holomorphic with no zeroes on $\\gamma$.\nLet $N$ denote the number of zeroes of $f$ in $D$, counted up to multiplicity.\nLet $P$ denote the number of poles of $f$ in $D$, counted up to order.\n{{explain|order}}\nThen:\n \n:$\\ds N - P = \\frac 1 {2\\pi i} \\oint_\\gamma \\frac {\\map {f'} z} {\\map f z} \\rd z$\n\\end{theorem}\n\n\\begin{proof}\nLet $n_1, n_2, n_3 \\ldots n_N$ denote the zeroes of $f$, and $p_1, p_2, p_3 \\ldots p_P$ denote its poles. \nThen, there exists a non-zero holomorphic function $g$ such that:\n:$\\ds \\map f x = \\frac {\\prod_{k \\mathop = 1}^N \\paren {z - n_k} } {\\prod_{j \\mathop = 1}^P \\paren {z - p_j} } \\map g z$\nTaking the logarithmic derivative:\n{{begin-eqn}}\n{{eqn\t| l = \\map \\LL {\\map f z}\n\t| r = \\frac {\\map {f'} z} {\\map f z}\n\t| c = {{Defof|Logarithmic Derivative of Meromorphic Function}}\n}}\n{{eqn\t| r = \\map \\LL {\\prod_{k \\mathop = 1}^N \\paren {z - n_k} } - \\map \\LL {\\prod_{j \\mathop = 1}^P \\paren {z - p_j} } + \\map \\LL {\\map g z}\n}}\n{{eqn\t| r = \\sum_{k \\mathop = 1}^N \\frac 1 {z - n_k} - \\sum_{j \\mathop = 1}^P \\frac 1 {z - p_j} + \\frac {\\map {g'} z} {\\map g z} \n}}\n{{end-eqn}}\nThen: \n{{begin-eqn}}\n{{eqn\t| l = \\oint_\\gamma \\frac {\\map {f'} z} {\\map f z} \\rd z\n\t| r = \\oint_\\gamma \\paren {\\sum_{k \\mathop = 1}^N \\frac 1 {z - n_k} - \\sum_{j \\mathop = 1}^P \\frac 1 {z - p_j} + \\frac {\\map {g'} z} {\\map g z} } \\rd z\n}}\n{{eqn\t| r = \\sum_{k \\mathop = 1}^N \\oint_\\gamma \\frac 1 {z - n_k} \\rd z - \\sum_{j \\mathop = 1}^P \\oint_\\gamma \\frac 1 {z - p_j} \\rd z + \\oint_\\gamma \\frac {\\map {g'} z} {\\map g z} \\rd z \n\t| c = Linear Combination of Contour Integrals\n}}\n{{eqn\t| r = \\sum_{k \\mathop = 1}^N 2 \\pi i - \\sum_{j \\mathop = 1}^P 2 \\pi i + \\oint_\\gamma \\frac {\\map {g'} z} {\\map g z} \\rd z\n}}\n{{eqn\t| r = 2\\pi i \\paren {N - P} + \\oint_\\gamma \\frac {\\map {g'} z} {\\map g z} \\rd z\n}}\n{{end-eqn}}\nBy our construction of $g$, $\\frac {g'} g$ is holomorphic on $D$ so, by the Cauchy Integral Theorem, the integral is equal to zero.\nWe therefore have:\n:$\\ds \\oint_\\gamma \\frac {\\map {f'} z} {\\map f z} \\rd z = 2\\pi i \\paren {N - P}$\nHence:\n:$\\ds N - P = \\frac 1 {2\\pi i} \\oint_\\gamma \\frac {\\map {f'} z} {\\map f z} \\rd z$\n{{qed}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "10586": {"score": 0.9336453080177307, "content": {"text": "\\section{Mittag-Leffler's Expansion Theorem}\nTags: Mittag-Leffler Expansions\n\n\\begin{theorem}\nLet $f$ be a meromorphic function that: \n:has only simple poles \n:is continuous, or has a removable singularity, at $0$. \nLet $X$ be the set of poles of $f$. \nFor $N \\in \\N$, let $C_N$ be a disk, centred at the origin, of radius $R_N$ where: \n:$R_N \\to \\infty$ as $N \\to \\infty$ \n:$\\partial C_N$ contains no poles of $f$ for any $N$. \n:there exists a real number $M > 0$ independent of $N$ such that for all $z \\in \\partial C_N$, $\\cmod {\\map f z} < M$ , for all $N \\in \\N$.\nThen:\n:$\\ds \\map f z = \\map f 0 + \\sum_{n \\mathop \\in X} \\Res f n \\paren {\\frac 1 {z - n} + \\frac 1 n}$ \nwhere:\n:$\\Res f n$ is the residue of $f$ at $n$\n:$z$ is not a pole of $f$\n:$\\ds \\map f 0 = \\lim_{z \\mathop \\to 0} \\map f z$ if $f$ has a removable singularity at $0$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\zeta \\in \\C \\setminus X$. \nThen: \n:$\\ds \\frac {\\map f z} {z - \\zeta}$ \nhas simple poles for $z \\in X \\cup \\set \\zeta$. \nLet $X_N$ be the set of poles contained within $C_N$. \nThen: \n{{begin-eqn}}\n{{eqn\t| l = \\frac 1 {2 \\pi i} \\oint_{\\partial C_N} \\frac {\\map f z} {z - \\zeta} \\rd z\n\t| r = \\Res {\\frac {\\map f z} {z - \\zeta} } \\zeta + \\sum_{n \\mathop \\in X_N} \\Res {\\frac {\\map f z} {z - \\zeta} } n\n\t| c = Residue Theorem\n}}\n{{eqn\t| r = \\lim_{z \\mathop \\to \\zeta} \\paren {\\frac {\\paren {z - \\zeta} \\map f z} {z - \\zeta} } + \\sum_{n \\mathop \\in X_N} \\paren {\\lim_{z \\mathop \\to n} \\frac {\\paren {z - n} \\map f z} {z - \\zeta} }\n\t| c = Residue at Simple Pole\n}}\n{{eqn\t| r = \\lim_{z \\mathop \\to \\zeta} \\map f z + \\sum_{n \\mathop \\in X_N} \\paren {\\lim_{z \\mathop \\to n} \\paren {\\paren {z - n} \\map f z } \\cdot \\lim_{z \\mathop \\to n} \\frac 1 {z - \\zeta} }\n\t| c = Product Rule for Limits of Real Functions \n}}\n{{eqn\t| r = \\map f \\zeta + \\sum_{n \\mathop \\in X_N} \\frac {\\Res f n} {n - \\zeta}\n\t| c = $f$ is continuous at $\\zeta$, Residue at Simple Pole\n}}\n{{end-eqn}}\nSetting $\\zeta = 0$, we obtain: \n:$\\ds \\frac 1 {2 \\pi i} \\oint_{\\partial C_N} \\frac {\\map f z} z \\rd z = \\map f 0 + \\sum_{n \\mathop \\in X_N} \\frac {\\Res f n} n$ \nSo: \n:$\\ds \\frac 1 {2 \\pi i} \\oint_{\\partial C_N} \\map f z \\paren {\\frac 1 {z - \\zeta} - \\frac 1 z} \\rd z = \\map f \\zeta + \\sum_{n \\mathop \\in X_N} {\\Res f n} \\paren {\\frac 1 {n - \\zeta} - \\frac 1 n} - \\map f 0$\nIt remains to show that the integral on the {{LHS}} vanishes as $N \\to \\infty$.\nWe have:\n{{begin-eqn}}\n{{eqn\t| l = \\cmod {\\frac 1 {2 \\pi i} \\oint_{\\partial C_N} \\map f z \\paren {\\frac 1 {z - \\zeta} - \\frac 1 z} \\rd z}\n\t| r = \\cmod {\\frac \\zeta {2 \\pi i} \\oint_{\\partial C_N} \\frac {\\map f z} {z \\paren {z - \\zeta} } \\rd z}\n}}\n{{eqn\t| o = \\le\n\t| r = \\frac {\\cmod \\zeta} {2 \\pi} \\cdot \\frac M {R_N \\paren {R_N - \\cmod \\zeta} } \\cdot 2 \\pi R_N\n\t| c = Triangle Inequality for Contour Integrals, Reverse Triangle Inequality, noting that $\\cmod z = R_N$ for $z \\in \\partial C_N$  \n}}\n{{eqn\t| o = \\le\n\t| r = \\frac {M \\size \\zeta} {R_N - \\size \\zeta}\n}}\n{{eqn\t| o = \\to\n\t| r = 0\n\t| c = as $N \\to \\infty$, $R_N \\to \\infty$\n}}\n{{end-eqn}}\nLetting $N \\to \\infty$ gives: \n:$\\ds 0 = \\map f \\zeta + \\sum_{n \\mathop \\in X} {\\Res f n} \\paren {\\frac 1 {n - \\zeta} - \\frac 1 n} - \\map f 0$\nGiving:\n:$\\ds \\map f \\zeta = \\map f 0 + \\sum_{n \\mathop \\in X} \\Res f n \\paren {\\frac 1 {\\zeta - n} + \\frac 1 n}$ \n{{qed}}\n{{Namedfor|Magnus Gustaf Mittag-Leffler|cat = Mittag-Leffler}}\n\\end{proof}\n\n"}}, "20559": {"score": 0.9131677746772766, "content": {"text": "\\section{Residue at Multiple Pole}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $f: \\C \\to \\C$ be a function meromorphic on some region, $D$, containing $a$. \nLet $f$ have a single pole in $D$, of order $N$, at $a$.\nThen the residue of $f$ at $a$ is given by:\n:$\\ds \\Res f a = \\frac 1 {\\paren {N - 1}!} \\lim_{z \\mathop \\to a} \\frac {\\d^{N - 1} } {\\d z^{N - 1} } \\paren {\\paren {z - a}^N \\map f z}$\n\\end{theorem}\n\n\\begin{proof}\nBy Existence of Laurent Series, there exists a Laurent series: \n:$\\ds \\map f z = \\sum_{n \\mathop = -\\infty}^\\infty c_n \\paren {z - a}^n$ \nconvergent on $D \\setminus \\set a$. \nAs $f$ has a pole of order $N$ at $a$, we have $c_n = 0$ for $n < -N$. \nSo: \n:$\\ds \\paren {z - a}^N \\map f z = \\sum_{n \\mathop = -N}^\\infty c_n \\paren {z - a}^{n + N}$ \nWhich can be rewritten: \n:$\\ds \\paren {z - a}^N \\map f z = \\sum_{n \\mathop = 0}^\\infty c_{n - N} \\paren {z - a}^n$\nNote that this is a Taylor series with centre $a$. \nBy the definition of a residue: \n:$\\ds \\Res f a = c_{-1}$\nThis corresponds to the $\\paren {N - 1}$th in the Taylor series of $\\paren {z - a}^N \\map f z$ about $a$. \nWe therefore have by Taylor Series of Holomorphic Function: \n:$\\ds c_{-1} = \\frac 1 {\\paren {N - 1}!} \\lim_{z \\mathop \\to a} \\frac {\\d^{N - 1} } {\\d z^{N - 1} } \\paren {\\paren {z - a}^N \\map f z}$\nHence the result. \n{{qed}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "21620": {"score": 0.9411336183547974, "content": {"text": "\\section{Sommerfeld-Watson Transform}\nTags: Complex Analysis\n\n\\begin{theorem}\nLet $\\map f z$ be a mapping with isolated poles.\n{{explain|Explain the context in which this theorem is placed. For example, what is the domain and range of $f$? One supposes $\\C$ but it needs to be made clear.}}\nLet $f$ go to zero faster than $\\dfrac 1 {\\size z}$ as $\\size z \\to \\infty$.\n{{explain|\"faster\"}}\nLet $C$ be a contour that is deformed such that all poles of $\\map f z$ are contained in $C$.\nThen:\n:$\\ds \\sum \\limits_{n \\mathop = -\\infty}^\\infty \\paren {-1}^n \\map f n = \\frac 1 {2 i} \\oint_C \\frac {\\map f z} {\\sin \\pi z} \\rd z$\n\\end{theorem}\n\n\\begin{proof}\nWe know from the Residue Theorem:\n{{begin-eqn}}\n{{eqn | l = \\oint_C \\map f z \\rd z\n      | r = 2 \\pi i \\, \\sum \\limits_{z_k} R_k(z_k)\n      | c = \n}}\n{{eqn | r = 2 \\pi i \\, \\sum_{z_k} \\lim_{z \\mathop \\to z_k} \\paren {\\paren {z - z_k} \\frac {\\map f z} {\\sin \\pi z} }\n      | c = \n}}\n{{end-eqn}}\nThis is for poles $z_k$ at order $N = 1$ because we say that simple poles exist for $\\dfrac {\\map f z} {\\sin \\pi z}$.\n{{explain|Rewrite the above comprehensibly}}\nUsing l'H\u00f4pital's rule:\n{{begin-eqn}}\n{{eqn | l = \\oint_C \\map f z \\rd z\n      | r = 2 \\pi i \\sum_{z_k} \\lim_{z \\mathop \\to z_k} \\paren {\\frac {\\partial_z \\paren {z - z_k} \\map f z} {\\partial_z \\sin \\pi z} }\n      | c = \n}}\n{{eqn | r = 2 \\pi i \\sum_{z_k} \\lim_{z \\mathop \\to z_k} \\paren {\\frac {\\map f z + \\paren {z - z_k} \\map {f'} z} {\\pi \\cos \\pi z} }\n      | c = \n}}\n{{end-eqn}}\n{{explain|$\\partial_z$}}\nBut $\\sin \\pi z$ has poles at $z_k = n$ for some $n \\in \\Z$ which implies:\n{{begin-eqn}}\n{{eqn | l = \\oint_C \\map f z \\rd z\n      | r = 2 \\pi i \\sum_{n \\mathop = -\\infty}^\\infty \\lim_{z \\mathop \\to n} \\paren {\\frac {\\map f z + \\paren {z - n} \\map {f'} z} {\\pi \\cos \\pi z} }\n      | c = \n}}\n{{eqn | r = 2 i \\sum_{n \\mathop = -\\infty}^\\infty \\paren {\\frac {\\map f n} {\\cos \\pi n} }\n      | c = \n}}\n{{end-eqn}}\nFinally:\n:$\\dfrac 1 {\\cos \\pi n} = \\cos \\pi n = \\paren {-1}^n$\nTherefore:\n:$\\ds \\frac 1 {2 i} \\oint_C \\map f z \\rd z = \\sum_{n \\mathop = -\\infty}^\\infty \\paren {-1}^n \\map f n$\n{{qed}}\n{{Namedfor|Arnold Johannes Wilhelm Sommerfeld|name2 = George Neville Watson|cat = Sommerfeld|cat2 = Watson}}\nCategory:Complex Analysis\n\\end{proof}\n\n"}}, "11616": {"score": 0.9185647964477539, "content": {"text": "\\section{Jensen's Formula}\nTags: Complex Analysis, Jensen's Formula\n\n\\begin{theorem}\nLet $S$ be an open subset of the complex plane containing the closed disk:\n:$D_r = \\set {z \\in \\C : \\cmod z \\le r}$\nof radius $r$ about $0$.\nLet $f: S \\to \\C$ be holomorphic on $S$. \nLet $f$ have no zeroes on the circle $\\cmod z = r$.\nLet $\\map f 0 \\ne 0$.\nLet $\\rho_1, \\ldots, \\rho_n$ be the zeroes of $f$ in $D_r$, counted with multiplicity.\nThen:\n:$(1): \\quad \\ds \\frac 1 {2 \\pi} \\int_0^{2 \\pi} \\ln \\cmod {\\map f {r e^{i \\theta} } } \\rd \\theta = \\ln \\cmod {\\map f 0} + \\sum_{k \\mathop = 1}^n \\paren {\\ln r - \\ln \\size {\\rho_k} }$\n\\end{theorem}\n\n\\begin{proof}\nWrite $f \\left({z}\\right) = \\left({z - \\rho_1}\\right) \\cdots \\left({z - \\rho_n}\\right) g \\left({z}\\right)$, so $g \\left({z}\\right) \\ne 0$ for $z \\in D_r$.\nIt is sufficient to check the equality for each factor of $f$ in this expansion.\nFirst let $h \\left({z}\\right) = z - \\rho_k$ for some $k \\in \\left\\{{1, \\ldots, n}\\right\\}$.\nMaking use of the substitution $u = r e^{i \\theta} - \\rho_k$ we find that:\n:$\\displaystyle \\frac 1 {2 \\pi} \\int_0^{2 \\pi} \\log \\left\\vert{h \\left({r e^{i \\theta} }\\right)}\\right\\vert \\ \\mathrm d \\theta = \\frac 1 {2 \\pi i} \\int_\\gamma \\frac {\\log \\left\\vert{u}\\right\\vert} {u + \\rho_k} \\ \\mathrm d u$\nwhere $\\gamma$ is a circle of radius $r$ centred at $-\\rho_k$, traversed anticlockwise.\n{{mistake|I think this last statement is False! The circle is not centered at 0 so $\\left\\vert{u}\\right\\vert$ is not constant.}}\nOn this circle, $\\log \\left\\vert{u}\\right\\vert = \\log r$ is constant, and we have that:\n:$\\displaystyle \\int_\\gamma \\frac 1 {u + \\rho_k} \\ \\mathrm d u = \\int_{\\left\\vert{z}\\right\\vert \\mathop = r} \\frac {\\mathrm d u} u = 2 \\pi i$\nTherefore the left hand side of $(1)$ is $\\log r$ as required.\nTo show equality for $g \\left({z}\\right)$, first observe that by the Residue Theorem:\n:$\\displaystyle \\int_{\\left\\vert{z}\\right\\vert = r} \\frac {\\log g \\left({z}\\right)} z \\ \\mathrm d z = 2 \\pi i \\log g \\left({0}\\right)$\nTherefore substituting $z = r e^{i \\theta}$ we have\n:$\\displaystyle 2 \\pi i \\log g \\left({0}\\right) = i \\int_0^{2 \\pi} \\log g \\left({r e^{i \\theta} }\\right) \\ \\mathrm d \\theta$\nComparing the imaginary parts of this equality we see that:\n:$\\displaystyle \\frac 1 {2 \\pi} \\int_0^{2 \\pi} \\log \\left\\vert{g \\left({r e^{i \\theta} }\\right)}\\right\\vert \\ \\mathrm d \\theta = \\log \\left\\vert{g \\left({0}\\right)}\\right\\vert$\nas required.\n{{qed}}\n{{namedfor|Johan Jensen|cat=Jensen}}\n{{stub|a little more detail needed perhaps}}\nCategory:Complex Analysis\n220277\n220206\n2015-06-07T18:27:29Z\nPrime.mover\n59\n220277\nwikitext\ntext/x-wiki\n{{MissingLinks}}\n\\end{proof}\n\n"}}, "7911": {"score": 0.9227436184883118, "content": {"text": "\\begin{definition}[Definition:Residue (Complex Analysis)]\nLet $f: \\C \\to \\C$ be a complex function.\nLet $z_0 \\in U \\subset \\C$ such that $f$ is analytic in $U \\setminus \\set {z_0}$.\nThen by Existence of Laurent Series, there is a Laurent series:\n:$\\ds \\sum_{j \\mathop = -\\infty}^\\infty a_j \\paren {z - z_0}^j$\nsuch that the sum converges to $f$ in $U - \\set {z_0}$.  \nThe '''residue''' at a point $z = z_0$ of a function $f: \\C \\to \\C$ is defined as $a_{-1}$ in that Laurent series.\nIt is denoted $\\Res f {z_0}$ or just $\\map {\\mathrm {Res} } {z_0}$ when $f$ is understood.\nCategory:Definitions/Complex Analysis\n\\end{definition}"}}}}, "TheoremQA_elainewan/math_calculus_5_2.json": {"gold": {"811": 1}, "retrieved": {"20121": {"score": 0.8366531133651733, "content": {"text": "\\section{Range of Common Logarithm of Number between 1 and 10}\nTags: Common Logarithms\n\n\\begin{theorem}\nLet $x \\in \\R$ be a real number such that:\n:$1 \\le x < 10$\nThen:\n:$0 \\le \\log_{10} x \\le 1$\nwhere $\\log_{10}$ denotes the common logarithm function.\n\\end{theorem}\n\n\\begin{proof}\nWe have:\n{{begin-eqn}}\n{{eqn | l = 1\n      | r = 10^0\n      | c = {{Defof|Integer Power}}\n}}\n{{eqn | l = 10\n      | r = 10^1\n      | c = {{Defof|Integer Power}}\n}}\n{{eqn | ll= \\leadsto\n      | l = \\log_{10} 1\n      | r = 0\n      | c = \n}}\n{{eqn | l = \\log_{10} 10\n      | r = 1\n      | c = \n}}\n{{end-eqn}}\nThe result follows from Logarithm is Strictly Increasing.\n{{explain|Strictly speaking we need another step in here, the above is just for the natural logarithm.}}\n\\end{proof}\n\n"}}, "12771": {"score": 0.8376501798629761, "content": {"text": "\\section{Hero's Method/Lemma 1}\nTags: Iterative Process for Estimating Square Roots, Hero's Method\n\n\\begin{theorem}\nLet $a \\in \\R$ be a real number such that $a > 0$.\nLet $x_1 \\in \\R$ be a real number such that $x_1 > 0$.\nLet $\\sequence {x_n}$ be the sequence in $\\R$ defined recursively by:\n:$\\forall n \\in \\N_{>0}: x_{n + 1} = \\dfrac {x_n + \\dfrac a {x_n} } 2$\nThen:\n:$\\forall n \\in \\N_{>0}: x_n > 0$\n\\end{theorem}\n\n\\begin{proof}\nThe proof proceeds by induction.\nFor all $n \\in \\Z_{>0}$, let $\\map P n$ be the proposition:\n:$x_n > 0$\n\\end{proof}\n\n"}}, "20285": {"score": 0.839859127998352, "content": {"text": "\\section{Real Number between Zero and One is Greater than Power/Natural Number}\nTags: Real Analysis, Powers, Inequalities, Real Number between Zero and One is Greater than Power, Real Numbers\n\n\\begin{theorem}\nLet $x \\in \\R$.\nLet $0 < x < 1$.\nLet $n$ be a natural number.\nThen:\n: $0 < x^n \\le x$\n\\end{theorem}\n\n\\begin{proof}\nFor all $n \\in \\N$, let $P \\left({n}\\right)$ be the proposition:\n: $\\displaystyle 0 < x < 1 \\implies 0 < x^n \\leq x$\n\\end{proof}\n\n"}}, "10384": {"score": 0.838428795337677, "content": {"text": "\\section{Multiplication using Parabola}\nTags: Multiplication, Parabolas, Algebra, Quadratics\n\n\\begin{theorem}\n:500pxrightthumb\nLet the parabola $P$ defined as $y = x^2$ be plotted on the Cartesian plane.\nLet $A = \\tuple {x_a, y_a}$ and $B = \\tuple {x_b, y_b}$ be points on the curve $\\map f x$ so that $x_a < x_b$.\nThen the line segment joining $A B$ will cross the $y$-axis at $-x_a x_b$.\nThus $P$ can be used as a nomogram to calculate the product of two numbers $x_a$ and $x_b$, as follows:\n:$(1) \\quad$ Find the points $-x_a$ and $x_b$ on the $x$-axis.\n:$(2) \\quad$ Find the points $A$ and $B$ where the lines $x = -x_a$ and $x = x_b$ cut $P$.\n:$(3) \\quad$ Lay a straightedge on the straight line joining $A$ and $B$ and locate its $y$-intercept $c$.\nThen $x_a x_b$ can be read off from the $y$-axis as the position of $c$.\n\\end{theorem}\n\n\\begin{proof}\nLet $\\map f x = x^2$.\nThen:\n:$\\map f {x_a} = x_a^2$\nand:\n:$\\map f {x_b} = x_b^2$\nThen the slope $m$ of the line segment joining $A B$ will be:\n{{begin-eqn}}\n{{eqn | l = m \n      | r = \\frac {x_b^2 - x_a^2} {x_b - x_a}\n      | c = Equation of Straight Line in Plane: Point-Slope Form\n}}\n{{eqn | r = \\frac {\\paren {x_b - x_a} \\paren {x_b + x_a} } {x_b - x_a}\n      | c = Difference of Two Squares\n}}\n{{eqn | r = x_b + x_a\n      | c = cancelling, $x_a \\ne x_b$\n}}\n{{end-eqn}}\nFrom Equation of Straight Line in Plane: Slope-Intercept Form:\n:$y = \\paren {x_b + x_a} x + c$\nwhere $c$ denotes the $y$-intercept.\nSubstituting the coordinates of point $A = \\tuple {x_a, x_a^2}$ for $\\tuple {x, y}$:\n{{begin-eqn}}\n{{eqn | l = x_a^2 \n      | r = \\paren {x_b + x_a} x_a + c\n}}\n{{eqn | ll= \\leadsto\n      | l = c\n      | r = x_a^2 - \\paren {x_a + x_b} x_a\n}}\n{{eqn | r = x_a^2 - x_a^2 - x_b x_a\n}}\n{{eqn | r = -x_b x_a\n}}\n{{end-eqn}}\n{{qed}}\n\\end{proof}\n\n"}}, "18861": {"score": 0.8377613425254822, "content": {"text": "\\section{Power Function on Base greater than One tends to One as Power tends to Zero/Rational Number/Lemma}\nTags:  Real Analysis, Powers\n\n\\begin{theorem}\nLet $a \\in \\R$ be a real number such that $a > 1$.\nLet $r \\in \\Q_{> 0}$ be a strictly positive rational number such that $r < 1$.\nThen:\n:$1 < a^r < 1 + a r$\n{{explain|See whether there is a proof of this based on Bernoulli's Inequality}}\n\\end{theorem}\n\n\\begin{proof}\nDefine a real function $g_r: \\R_{> 0} \\to \\R$ as:\n:$\\map {g_r} a = 1 + a r - a^r$\nThen differentiating {{WRT|Differentiation}} $a$ gives:\n:$D_a \\map {g_r} a = r \\paren {1 - a^{r - 1} }$\nWe show now that the derivative of $g_r$ is positive for all $a > 1$:\n{{begin-eqn}}\n{{eqn | l = r\n      | o = <\n      | r = 1\n}}\n{{eqn | ll= \\leadsto\n      | l = r - 1\n      | o = <\n      | r = 0\n      | c = Subtract $1$ from both sides\n}}\n{{eqn | ll= \\leadsto\n      | l = a^{r - 1}\n      | o = <\n      | r = a^0\n      | c = Power Function on Base Greater than One is Strictly Increasing: Rational Number\n}}\n{{eqn | ll= \\leadsto\n      | l = a^{r - 1}\n      | o = <\n      | r = 1\n      | c = {{Defof|Integer Power}}\n}}\n{{eqn | ll= \\leadsto\n      | l = -1 \n      | o = <\n      | r = -a^{r - 1}\n      | c = Order of Real Numbers is Dual of Order of their Negatives\n}}\n{{eqn | ll= \\leadsto\n      | l = 0 \n      | o = <\n      | r = 1 - a^{r - 1}\n      | c = adding $1$ to both sides\n}}\n{{eqn | ll= \\leadsto\n      | l = 0 \n      | o = <\n      | r = r \\left({1 - a^{r - 1} }\\right)\n      | c = multiplying both sides by $r > 0$\n}}\n{{eqn | ll= \\leadsto\n      | l = 0 \n      | o = <\n      | r = D_a \\map {g_r} a\n      | c = from the formula found for $D_a \\map {g_r} a$ above\n}}\n{{end-eqn}}\nSo $D_a \\map {g_r} a$ is positive for all $a > 1$.\nWhence, by Derivative of Monotone Function, $g_r$ is increasing for all $a > 1$.\nNow:\n:$\\map {g_r} 1 = r > 0$\nSo $\\map {g_r} a$ is positive for all $a > 1$.\nThat is:\n:$0 < 1 + a r - a^r$\nAdding $a^r$ to both sides of the above yields:\n:$a^r < 1 + a r$\nFinally:\n{{begin-eqn}}\n{{eqn | l = 0\n      | o = <\n      | r = r\n}}\n{{eqn | ll= \\leadsto\n      | l = a^0\n      | o = <\n      | r = a^r\n      | c = Power Function on Base Greater than One is Strictly Increasing: Rational Number\n}}\n{{eqn | ll= \\leadsto\n      | l = 1\n      | o = <\n      | r = a^r\n      | c = {{Defof|Integer Power}}\n}}\n{{end-eqn}}\nSo, for $0 < r < 1$:\n:$1 < a^r < 1 + a r$\n{{qed}}\nCategory:Powers\n\\end{proof}\n\n"}}, "20764": {"score": 0.8464750647544861, "content": {"text": "\\section{Root of Reciprocal is Reciprocal of Root}\nTags:  Real Analysis, Roots of Numbers\n\n\\begin{theorem}\nLet $x \\in \\R_{\\ge 0}$.\nLet $n \\in \\N$.\nLet $\\sqrt [n] x$ denote the $n$th root of $x$.\nThen:\n:$\\sqrt [n] {\\dfrac 1 x} = \\dfrac 1 {\\sqrt [n] x}$\n\\end{theorem}\n\n\\begin{proof}\nLet $y = \\sqrt [n] {\\dfrac 1 x}$.\nThen:\n{{begin-eqn}}\n{{eqn | l = \\sqrt [n] {\\dfrac 1 x}\n      | r = y\n      | c = \n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\dfrac 1 x\n      | r = y^n\n      | c = {{Defof|Root (Analysis)|$n$th root}}\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = x\n      | r = \\dfrac 1 {y^n}\n      | c = Reciprocal of Real Number is Unique\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = x\n      | r = \\paren {\\dfrac 1 y}^n\n      | c = Powers of Group Elements/Negative Index\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\sqrt [n] x\n      | r = \\dfrac 1 y\n      | c = {{Defof|Root (Analysis)|$n$th root}}\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\dfrac 1 {\\sqrt [n] x}\n      | r = y\n      | c = Reciprocal of Real Number is Unique\n}}\n{{eqn | ll= \\leadstoandfrom\n      | l = \\dfrac 1 {\\sqrt [n] x}\n      | r = \\sqrt [n] {\\dfrac 1 x}\n      | c = Definition of $y$\n}}\n{{end-eqn}}\nHence the result.\n{{qed}}\nCategory:Roots of Numbers\n\\end{proof}\n\n"}}, "19952": {"score": 0.8421171307563782, "content": {"text": "\\section{Proportion of Power}\nTags: Proportion\n\n\\begin{theorem}\nLet $x$ and $y$ be proportional.\n{{explain|Establish what types of object $x$ and $y$ are. As it stands here, they could be anything.}}\nLet $n \\in \\Z$. \nThen $x^n \\propto y^n$.\n\\end{theorem}\n\n\\begin{proof}\nLet $x \\propto y$.\nThen $\\exists k \\ne 0: x = k \\times y$ by the definition of proportion.\nRaising both sides of this equation to the $n$th power:\n{{begin-eqn}}\n{{eqn | l = x^n\n      | r = \\paren {k \\times y}^n\n}}\n{{eqn | r = k^n \\times y^n\n}}\n{{end-eqn}}\nso $k^n$ is the desired constant of proportion.\nThe result follows from the definition of proportion.\n{{qed}}\nCategory:Proportion\n\\end{proof}\n\n"}}, "18841": {"score": 0.8478752374649048, "content": {"text": "\\section{Positive Real has Real Square Root}\nTags: Real Numbers\n\n\\begin{theorem}\nLet $x \\in \\R_{>0}$ be a (strictly) positive real number.\nThen:\n:$\\exists y \\in \\R: x = y^2$\n\\end{theorem}\n\n\\begin{proof}\nLet $f: \\R \\to \\R$ be defined as:\n:$\\forall x \\in \\R: \\map f x = x^2$\nWe have that $f$ is the pointwise product of the identity mapping with itself.\nBy Product Rule for Continuous Real Functions and Identity Mapping is Continuous, $f$ is continuous.\nBy Power Function is Unbounded Above:\n:$\\exists q \\in \\R: \\map f q > x$\nThen:\n:$0^2 = 0 \\le x$\nBy the Intermediate Value Theorem:\n:$\\exists y \\in \\R: 0 < y < q: y^2 = x$\n{{qed}}\nCategory:Real Numbers\n\\end{proof}\n\n"}}, "18852": {"score": 0.8430902361869812, "content": {"text": "\\section{Power Function on Base Greater than One is Strictly Increasing/Positive Integer}\nTags:  Real Analysis, Power Function on Base Greater than One is Strictly Increasing, Powers\n\n\\begin{theorem}\nLet $a \\in \\R$ be a real number such that $a > 1$.\nLet $f: \\Z_{\\ge 0} \\to \\R$ be the real-valued function defined as:\n:$\\map f n = a^n$\nwhere $a^n$ denotes $a$ to the power of $n$.\nThen $f$ is strictly increasing.\n\\end{theorem}\n\n\\begin{proof}\nFix $n \\in \\Z_{\\ge 0}$.\nFrom Ordering of Reciprocals:\n:$0 < \\dfrac 1 a < 1$\nFrom Power Function on Base between Zero and One is Strictly Decreasing: Positive Integer:\n:$\\paren {\\dfrac 1 a}^{n + 1} < \\paren {\\dfrac 1 a}^n$\nFrom Real Number to Negative Power: Positive Integer:\n:$\\dfrac 1 {a^{n + 1} } < \\dfrac 1 {a^n}$\nFrom Ordering of Reciprocals:\n:$a^n < a^{n + 1}$\nHence the result.\n{{qed}}\nCategory:Power Function on Base Greater than One is Strictly Increasing\n\\end{proof}\n\n"}}, "8095": {"score": 0.8434809446334839, "content": {"text": "\\begin{definition}[Definition:Root (Analysis)]\nLet $x, y \\in \\R_{\\ge 0}$ be positive real numbers.\nLet $n \\in \\Z$ be an integer such that $n \\ne 0$.\nThen $y$ is the '''positive $n$th root of $x$''' {{iff}}:\n:$y^n = x$\nand we write:\n:$y = \\sqrt[n] x$\nUsing the power notation, this can also be written:\n:$y = x^{1/n}$\nWhen $n = 2$, we write $y = \\sqrt x$ and call $y$ the '''positive square root''' of $x$.\nWhen $n = 3$, we write $y = \\sqrt [3] x$ and call $y$ the '''cube root''' of $x$.\nNote the special case where $x = 0 = y$:\n:$0 = \\sqrt [n] 0$\n\\end{definition}"}}}}}